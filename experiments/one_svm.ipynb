{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODq/l6AV33b+AUoSBM+UTk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KapilM26/Insider-Threat-Detection/blob/main/one_svm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "EEDgbI0dbst1",
        "outputId": "6413e7e1-6a67-456d-887f-3b5b70cdd49b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/MIB0203.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0259549535f4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"MIB0203\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/MIB0203.csv\"\u001b[0m  \u001b[0;31m# Change to your actual file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m \u001b[0mfinal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;31m# final_df = combine_user_feature_data(user, dataset_path, insider_root)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/MIB0203.csv'"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "import csv\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from datetime import datetime, time, timedelta\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_user_usb_data(user_id, dataset_path):\n",
        "    usb_data = []\n",
        "    with open(os.path.join(dataset_path, \"/content/device.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[5] == \"Connect\":  # Check user and only \"Connect\" activity\n",
        "                usb_data.append(row)\n",
        "    return usb_data\n",
        "\n",
        "def get_num_usb_insertions_per_week(user, usb_data):\n",
        "    weekly_usb_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "    for row in usb_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_usb_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_usb_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_usb_insertions\"])\n",
        "\n",
        "\n",
        "def get_user_exe_data(user_id, dataset_path):\n",
        "    exe_data = []\n",
        "    with open(os.path.join(dataset_path, \"/content/file.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[4].endswith(\".exe\"):  # Check user and .exe files\n",
        "                exe_data.append(row)\n",
        "    return exe_data\n",
        "\n",
        "def get_num_exe_per_week(user, exe_data):\n",
        "    weekly_exe_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in exe_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_exe_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_exe_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_exe_files\"])\n",
        "\n",
        "# %%\n",
        "def get_user_logon_data(user_id, dataset_path):\n",
        "    logon_data = []\n",
        "    with open(os.path.join(dataset_path, \"/content/logon.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        for row in reader:\n",
        "            if row[2] == user_id:\n",
        "                logon_data.append(row)\n",
        "    return logon_data\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_user_pc(logon_data):\n",
        "    pc_dict = {}\n",
        "    for row in logon_data:\n",
        "        pc_dict[row[3]] = 1 + pc_dict.get(row[3], 0)\n",
        "    user_pc = max(pc_dict, key=pc_dict.get)\n",
        "    return user_pc\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_num_other_PC_per_week(user, user_pc, logon_data):\n",
        "    weekly_pc_counts = defaultdict(set)  # Dictionary to store unique PCs per week\n",
        "    all_weeks = set()  # Set to track all weeks where logons occurred\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")  # Adjusted format\n",
        "        week = logon_time.strftime(\"%Y-%W\")  # Year-Week format\n",
        "        all_weeks.add(week)  # Track all weeks\n",
        "\n",
        "        if row[3] != user_pc:  # Check if PC is different from user's primary PC\n",
        "            weekly_pc_counts[week].add(\n",
        "                row[3]\n",
        "            )  # Add PC to the week's set (unique values only)\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks)\n",
        "    max_week = max(all_weeks)\n",
        "\n",
        "    # Generate all weeks between min and max\n",
        "    start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "    end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "    current_date = start_date\n",
        "    complete_weeks = set()\n",
        "\n",
        "    while current_date <= end_date:\n",
        "        week_str = current_date.strftime(\"%Y-%W\")\n",
        "        complete_weeks.add(week_str)\n",
        "        current_date += timedelta(days=7)\n",
        "\n",
        "    # Ensure every week has a count (0 if no other PCs were accessed)\n",
        "    weekly_counts = {\n",
        "        week: len(weekly_pc_counts[week]) if week in weekly_pc_counts else 0\n",
        "        for week in complete_weeks\n",
        "    }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_other_pc\"])\n",
        "\n",
        "\n",
        "def get_after_hours_logons(\n",
        "    logon_data, user, business_start=time(9, 0, 0), business_end=time(17, 0, 0)\n",
        "):\n",
        "    \"\"\"\n",
        "    Aggregates after-hours logons per week for a specified user.\n",
        "\n",
        "    :param logon_data: List of logon events in the format [id, date, user, pc, activity]\n",
        "    :param user: The specific user to filter logon events for.\n",
        "    :param business_start: Datetime.time representing start of business hours.\n",
        "    :param business_end: Datetime.time representing end of business hours.\n",
        "    :return: DataFrame with ['user', 'week', 'after_hours_logons']\n",
        "    \"\"\"\n",
        "\n",
        "    after_hours_counts = defaultdict(int)\n",
        "\n",
        "    # Track all weeks for the user\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_id, timestamp, logon_user, pc, activity = row  # Unpack columns\n",
        "\n",
        "        if (\n",
        "            activity.lower() == \"logon\" and logon_user == user\n",
        "        ):  # Only process logons for the specified user\n",
        "            try:\n",
        "                logon_time = datetime.strptime(timestamp, \"%m/%d/%Y %H:%M:%S\")\n",
        "                logon_week = logon_time.strftime(\"%Y-%W\")  # Ensure same format\n",
        "\n",
        "                # Store this week to ensure it's included in results\n",
        "                all_weeks.add(logon_week)\n",
        "\n",
        "                # Extract only the time component\n",
        "                logon_hour = logon_time.time()\n",
        "\n",
        "                # Check if the logon occurred outside business hours\n",
        "                if logon_hour < business_start or logon_hour >= business_end:\n",
        "                    after_hours_counts[logon_week] += 1\n",
        "\n",
        "            except ValueError:\n",
        "                continue  # Skip invalid timestamps\n",
        "\n",
        "    # Ensure all weeks in range are included (like `get_num_other_PC_per_week`)\n",
        "    if all_weeks:\n",
        "        min_week = min(all_weeks)\n",
        "        max_week = max(all_weeks)\n",
        "\n",
        "        # Generate all weeks in range\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        # Fill in missing weeks with 0\n",
        "        after_hours_counts = {\n",
        "            week: after_hours_counts.get(week, 0) for week in complete_weeks\n",
        "        }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    result_data = [\n",
        "        (user, week, after_hours_counts[week])\n",
        "        for week in sorted(after_hours_counts.keys())\n",
        "    ]\n",
        "    after_hours_df = pd.DataFrame(\n",
        "        result_data, columns=[\"user\", \"week\", \"after_hours_logons\"]\n",
        "    )\n",
        "\n",
        "    return after_hours_df\n",
        "\n",
        "\n",
        "# %%\n",
        "def find_insider_answers_file(user, insider_root):\n",
        "    \"\"\"\n",
        "    Recursively searches for the insider CSV file for the given user in the `insider_root` directory.\n",
        "\n",
        "    :param user: The user ID (e.g., \"CWW1120\")\n",
        "    :param insider_root: The root folder containing multiple r5.2-* subfolders.\n",
        "    :return: The full path to the user's insider CSV file if found, else None.\n",
        "    \"\"\"\n",
        "    for root, _, files in os.walk(insider_root):\n",
        "        for file in files:\n",
        "            if file.startswith(f\"/content/r5.2-\") and file.endswith(\n",
        "                f\"-{user}.csv\"\n",
        "            ):  # Match user file format\n",
        "                return os.path.join(root, file)  # Return full file path if found\n",
        "    return None  # Return None if no file is found\n",
        "\n",
        "\n",
        "def extract_weeks_from_csv(file_path):\n",
        "    \"\"\"\n",
        "    Reads a CSV file using `csv.reader` and extracts unique weeks from the timestamps (3rd column).\n",
        "\n",
        "    :param file_path: Path to the insider CSV file.\n",
        "    :return: A set of detected `Year-Week` values.\n",
        "    \"\"\"\n",
        "    insider_weeks = set()\n",
        "\n",
        "    try:\n",
        "        with open(file_path, mode=\"r\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "            reader = csv.reader(file)\n",
        "            for row in reader:\n",
        "                if len(row) < 3:  # Ensure the timestamp column exists\n",
        "                    continue\n",
        "                try:\n",
        "                    logon_time = datetime.strptime(\n",
        "                        row[2], \"%m/%d/%Y %H:%M:%S\"\n",
        "                    )  # Parse timestamp\n",
        "                    week = logon_time.strftime(\"%Y-%W\")  # Convert to Year-Week format\n",
        "                    insider_weeks.add(week)\n",
        "                except ValueError:\n",
        "                    continue  # Skip rows with invalid timestamps\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "    return insider_weeks\n",
        "\n",
        "\n",
        "def label_insider_weeks(df, user, insider_root):\n",
        "    \"\"\"\n",
        "    Adds an 'insider' column to the DataFrame by checking if the user's week exists in their insider file.\n",
        "\n",
        "    :param df: DataFrame containing ['user', 'week', 'num_other_pc']\n",
        "    :param user: The user ID for whom the dataframe is filtered.\n",
        "    :param insider_root: Path to the folder containing multiple r5.2-* subfolders.\n",
        "    :return: DataFrame with an 'insider' column.\n",
        "    \"\"\"\n",
        "\n",
        "    # Locate the user's insider file\n",
        "    insider_file = find_insider_answers_file(user, insider_root)\n",
        "\n",
        "    # If no insider file exists for the user, mark all weeks as 0 (not insider)\n",
        "    if not insider_file:\n",
        "        df[\"insider\"] = 0\n",
        "        return df\n",
        "\n",
        "    # Extract weeks from the insider CSV file\n",
        "    insider_weeks = extract_weeks_from_csv(insider_file)\n",
        "\n",
        "    # Label insider weeks in the user's dataframe\n",
        "    df[\"insider\"] = df[\"week\"].apply(lambda w: 1 if w in insider_weeks else 0)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def combine_user_feature_data(user, dataset_path, insider_root):\n",
        "    # Get data from different feature functions\n",
        "    logon_data = get_user_logon_data(user, dataset_path)\n",
        "    user_pc = get_user_pc(logon_data)\n",
        "    num_other_pc = get_num_other_PC_per_week(user, user_pc, logon_data)\n",
        "    after_hours_logons = get_after_hours_logons(logon_data, user)\n",
        "\n",
        "    exe_data = get_user_exe_data(user, dataset_path)\n",
        "    num_exe_files = get_num_exe_per_week(user, exe_data)\n",
        "\n",
        "    usb_data = get_user_usb_data(user, dataset_path)\n",
        "    num_usb = get_num_usb_insertions_per_week(user, usb_data)\n",
        "\n",
        "    # Extract relevant columns\n",
        "    after_hours_df = after_hours_logons[[\"week\", \"after_hours_logons\"]]\n",
        "    exe_df         = num_exe_files[[\"week\", \"num_exe_files\"]]\n",
        "    usb_df         = num_usb[[\"week\", \"num_usb_insertions\"]]\n",
        "    other_pc_df    = num_other_pc[[\"week\", \"num_other_pc\"]]\n",
        "\n",
        "    # Merge all dataframes on \"week\" using an outer join\n",
        "    merged_df = after_hours_df.merge(exe_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(usb_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(other_pc_df, on=\"week\", how=\"outer\")\n",
        "\n",
        "    # Replace NaN with 0 in all feature columns\n",
        "    merged_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Add user column\n",
        "    merged_df.insert(0, \"user\", user)\n",
        "    labeled_df = label_insider_weeks(merged_df, user, insider_root)\n",
        "    return labeled_df\n",
        "\n",
        "# Example usage\n",
        "dataset_path = os.path.join(\"Insider threat dataset\", \"r5.2\")\n",
        "user = \"MIB0203\"\n",
        "file_path = \"/content/MIB0203.csv\"  # Change to your actual file path\n",
        "final_df = pd.read_csv(file_path)\n",
        "# final_df = combine_user_feature_data(user, dataset_path, insider_root)\n",
        "print(final_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Access the 'num_other_pc' data from the 'final_df' DataFrame\n",
        "# num_other_pc_data = final_df[['num_other_pc']]\n",
        "\n",
        "# # Initialize the Isolation Forest model\n",
        "# iso_forest = IsolationForest(contamination='auto', random_state=42)\n",
        "\n",
        "# # Fit the model\n",
        "# iso_forest.fit(num_other_pc_data)\n",
        "\n",
        "# # Predict anomalies and add results to the 'final_df' DataFrame\n",
        "# final_df['anomaly'] = iso_forest.predict(num_other_pc_data)\n",
        "# final_df['anomaly_score'] = iso_forest.decision_function(num_other_pc_data)\n",
        "\n",
        "# # Convert anomaly labels to boolean (1 for anomaly, -1 for normal)\n",
        "# final_df['anomaly'] = final_df['anomaly'].apply(lambda x: 1 if x == -1 else 0)\n",
        "\n",
        "# # Display the rows where anomalies were detected\n",
        "# print(final_df[final_df['anomaly'] == 1])"
      ],
      "metadata": {
        "id": "aHJ6fDm-erbz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_-VI7jyJtkEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pd.Series(y_pred).value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju74hOxQvFiC",
        "outputId": "bd463e21-01f9-4eef-8be8-f7152ef51a17"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    29\n",
            "1    15\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Select Features\n",
        "feature_cols = [\"after_hours_logons\", \"num_exe_files\", \"num_usb_insertions\", \"num_other_pc\"]\n",
        "X = final_df[feature_cols]\n",
        "y_true = final_df[\"insider\"]\n",
        "\n",
        "# Normalize Data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train One-Class SVM with a Higher `nu`\n",
        "oc_svm = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.4)  # Increased from 0.2 to 0.4\n",
        "oc_svm.fit(X_scaled)\n",
        "\n",
        "# Predict Anomalies\n",
        "decision_scores = oc_svm.decision_function(X_scaled)\n",
        "\n",
        "# Lower the Threshold to Detect More Anomalies\n",
        "threshold = np.percentile(decision_scores, 80)  # Changed from 90 to 80\n",
        "y_pred = [1 if score < threshold else 0 for score in decision_scores]\n",
        "\n",
        "# Calculate Metrics\n",
        "precision = precision_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "\n",
        "# Print Results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "# Debug: Print Prediction Counts\n",
        "print(\"Predicted Labels:\\n\", pd.Series(y_pred).value_counts())\n",
        "print(\"True Labels:\\n\", pd.Series(y_true).value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_3dG0QyAuES",
        "outputId": "fcbe2896-b4de-4925-bb3c-a7f3081aa427"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.0588\n",
            "Recall: 1.0000\n",
            "F1-score: 0.1111\n",
            "Predicted Labels:\n",
            " 1    34\n",
            "0    10\n",
            "Name: count, dtype: int64\n",
            "True Labels:\n",
            " insider\n",
            "0    42\n",
            "1     2\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "#  Select Features for Training\n",
        "feature_cols = [\"after_hours_logons\", \"num_exe_files\", \"num_usb_insertions\", \"num_other_pc\"]\n",
        "X = final_df[feature_cols]\n",
        "y_true = final_df[\"insider\"]  # Ground truth (0 = normal, 1 = anomaly)\n",
        "\n",
        "# Normalize the Features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "#  Train One-Class SVM\n",
        "oc_svm = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.9)  # Adjust nu for anomaly proportion\n",
        "oc_svm.fit(X_scaled)\n",
        "\n",
        "# Predict Anomalies\n",
        "decision_scores = oc_svm.decision_function(X_scaled)  # Get decision function scores\n",
        "threshold = np.percentile(decision_scores, 90)  # Detect top 10% as anomalies\n",
        "y_pred = [1 if score < threshold else 0 for score in decision_scores]\n",
        "\n",
        "#  Calculate Evaluation Metrics\n",
        "precision = precision_score(y_true, y_pred, pos_label=1)\n",
        "recall = recall_score(y_true, y_pred, pos_label=1)\n",
        "f1 = f1_score(y_true, y_pred, pos_label=1)\n",
        "\n",
        "#  Print Results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "N7GceFB1AKZM",
        "outputId": "a0a35bff-d11b-4427-a136-c9062dc93716"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'final_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-70260b674147>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#  Select Features for Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfeature_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"after_hours_logons\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_exe_files\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_usb_insertions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_other_pc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"insider\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Ground truth (0 = normal, 1 = anomaly)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'final_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define different threshold values for anomaly detection\n",
        "thresholds = np.percentile(decision_scores, [5, 10, 15, 20, 25])  # Try different percentiles\n",
        "\n",
        "# Loop through each threshold and compute precision, recall, F1-score\n",
        "for threshold in thresholds:\n",
        "  y_pred = np.where(decision_scores < threshold, 1, 0)  # Label as \"1\" if below threshold (anomaly)\n",
        "\n",
        "  precision = precision_score(y_true, y_pred)\n",
        "  recall = recall_score(y_true, y_pred)\n",
        "  f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "  print(f\"Threshold = {threshold:.4f}: Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "SkSZXtN0fVtb",
        "outputId": "6b86b669-3af1-404d-dac3-0f4f024f883e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c5d702ec4d65>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define different threshold values for anomaly detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecision_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Try different percentiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Loop through each threshold and compute precision, recall, F1-score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import LocalOutlierFactor  # Import LocalOutlierFactor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Define your possible contamination values (thresholds)\n",
        "contamination_values = [0.01, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15]\n",
        "\n",
        "# Loop through each contamination value and calculate performance metrics\n",
        "for contamination in contamination_values:\n",
        "    lof = LocalOutlierFactor(n_neighbors=20, contamination=contamination)\n",
        "\n",
        "    # Fit LOF model and get predictions\n",
        "    y_pred = lof.fit_predict(X)  # Assuming X_train is your feature matrix\n",
        "    y_pred = [1 if pred == -1 else 0 for pred in y_pred]  # Convert LOF output (-1 for outliers, 1 for inliers)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for insider=1 (malicious)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"Contamination = {contamination}: Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u0iyZUVGP22",
        "outputId": "15000a9f-dbd8-4e4e-b9ce-68162b048016"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contamination = 0.01: Precision = 1.0000, Recall = 0.5000, F1 Score = 0.6667\n",
            "Contamination = 0.025: Precision = 1.0000, Recall = 0.5000, F1 Score = 0.6667\n",
            "Contamination = 0.05: Precision = 1.0000, Recall = 1.0000, F1 Score = 1.0000\n",
            "Contamination = 0.075: Precision = 0.6667, Recall = 1.0000, F1 Score = 0.8000\n",
            "Contamination = 0.1: Precision = 0.5000, Recall = 1.0000, F1 Score = 0.6667\n",
            "Contamination = 0.125: Precision = 0.4000, Recall = 1.0000, F1 Score = 0.5714\n",
            "Contamination = 0.15: Precision = 0.3333, Recall = 1.0000, F1 Score = 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define different threshold values for anomaly detection\n",
        "    thresholds = np.percentile(decision_scores, [5, 10, 15, 20, 25])  # Try different percentiles\n",
        "\n",
        "    # Loop through each threshold and compute precision, recall, F1-score\n",
        "    for threshold in thresholds:\n",
        "        y_pred = np.where(decision_scores < threshold, 1, 0)  # Label as \"1\" if below threshold (anomaly)\n",
        "\n",
        "        precision = precision_score(y_true, y_pred)\n",
        "        recall = recall_score(y_true, y_pred)\n",
        "        f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "        print(f\"Threshold = {threshold:.4f}: Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "CGNOQftRe22n",
        "outputId": "9788310c-7cce-40e7-d8a4-49ccbe223a16"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-2-c0566b08f270>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-c0566b08f270>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    thresholds = np.percentile(decision_scores, [5, 10, 15, 20, 25])  # Try different percentiles\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df[final_df[\"num_usb_insertions\"]!=0].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "CzC-f6luwa_q",
        "outputId": "156bfcb4-904a-48e0-88c0-80dae91a173d"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       user     week  after_hours_logons  num_exe_files  num_usb_insertions  \\\n",
              "40  MIB0203  2010-41                   7              0                 1.0   \n",
              "41  MIB0203  2010-42                   7              0                 1.0   \n",
              "\n",
              "    num_other_pc  insider  \n",
              "40             0        1  \n",
              "41             1        1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f0e2b233-9f29-4d98-9ada-3b5f3b6e8eb1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>week</th>\n",
              "      <th>after_hours_logons</th>\n",
              "      <th>num_exe_files</th>\n",
              "      <th>num_usb_insertions</th>\n",
              "      <th>num_other_pc</th>\n",
              "      <th>insider</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>MIB0203</td>\n",
              "      <td>2010-41</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>MIB0203</td>\n",
              "      <td>2010-42</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f0e2b233-9f29-4d98-9ada-3b5f3b6e8eb1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f0e2b233-9f29-4d98-9ada-3b5f3b6e8eb1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f0e2b233-9f29-4d98-9ada-3b5f3b6e8eb1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-13b8750b-91c3-4ae2-a3ed-e5118bebb10a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-13b8750b-91c3-4ae2-a3ed-e5118bebb10a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-13b8750b-91c3-4ae2-a3ed-e5118bebb10a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"final_df[final_df[\\\"num_usb_insertions\\\"]!=0]\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"user\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"MIB0203\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"week\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2010-42\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"after_hours_logons\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 7,\n        \"max\": 7,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_exe_files\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_usb_insertions\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_other_pc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"insider\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "import csv\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from datetime import datetime, time, timedelta\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_user_usb_data(user_id, dataset_path):\n",
        "    usb_data = []\n",
        "    with open(os.path.join(dataset_path, \"/content/device.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[5] == \"Connect\":  # Check user and only \"Connect\" activity\n",
        "                usb_data.append(row)\n",
        "    return usb_data\n",
        "\n",
        "def get_num_usb_insertions_per_week(user, usb_data):\n",
        "    weekly_usb_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "    for row in usb_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_usb_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_usb_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_usb_insertions\"])\n",
        "\n",
        "\n",
        "def get_user_exe_data(user_id, dataset_path):\n",
        "    exe_data = []\n",
        "    with open(os.path.join(dataset_path, \"/content/file.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[4].endswith(\".exe\"):  # Check user and .exe files\n",
        "                exe_data.append(row)\n",
        "    return exe_data\n",
        "\n",
        "def get_num_exe_per_week(user, exe_data):\n",
        "    weekly_exe_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in exe_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_exe_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_exe_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_exe_files\"])\n",
        "\n",
        "# %%\n",
        "def get_user_logon_data(user_id, dataset_path):\n",
        "    logon_data = []\n",
        "    with open(os.path.join(dataset_path, \"/content/logon.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        for row in reader:\n",
        "            if row[2] == user_id:\n",
        "                logon_data.append(row)\n",
        "    return logon_data\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_user_pc(logon_data):\n",
        "    pc_dict = {}\n",
        "    for row in logon_data:\n",
        "        pc_dict[row[3]] = 1 + pc_dict.get(row[3], 0)\n",
        "    user_pc = max(pc_dict, key=pc_dict.get)\n",
        "    return user_pc\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_num_other_PC_per_week(user, user_pc, logon_data):\n",
        "    weekly_pc_counts = defaultdict(set)  # Dictionary to store unique PCs per week\n",
        "    all_weeks = set()  # Set to track all weeks where logons occurred\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")  # Adjusted format\n",
        "        week = logon_time.strftime(\"%Y-%W\")  # Year-Week format\n",
        "        all_weeks.add(week)  # Track all weeks\n",
        "\n",
        "        if row[3] != user_pc:  # Check if PC is different from user's primary PC\n",
        "            weekly_pc_counts[week].add(\n",
        "                row[3]\n",
        "            )  # Add PC to the week's set (unique values only)\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks)\n",
        "    max_week = max(all_weeks)\n",
        "\n",
        "    # Generate all weeks between min and max\n",
        "    start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "    end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "    current_date = start_date\n",
        "    complete_weeks = set()\n",
        "\n",
        "    while current_date <= end_date:\n",
        "        week_str = current_date.strftime(\"%Y-%W\")\n",
        "        complete_weeks.add(week_str)\n",
        "        current_date += timedelta(days=7)\n",
        "\n",
        "    # Ensure every week has a count (0 if no other PCs were accessed)\n",
        "    weekly_counts = {\n",
        "        week: len(weekly_pc_counts[week]) if week in weekly_pc_counts else 0\n",
        "        for week in complete_weeks\n",
        "    }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_other_pc\"])\n",
        "\n",
        "\n",
        "def get_after_hours_logons(\n",
        "    logon_data, user, business_start=time(9, 0, 0), business_end=time(17, 0, 0)\n",
        "):\n",
        "    \"\"\"\n",
        "    Aggregates after-hours logons per week for a specified user.\n",
        "\n",
        "    :param logon_data: List of logon events in the format [id, date, user, pc, activity]\n",
        "    :param user: The specific user to filter logon events for.\n",
        "    :param business_start: Datetime.time representing start of business hours.\n",
        "    :param business_end: Datetime.time representing end of business hours.\n",
        "    :return: DataFrame with ['user', 'week', 'after_hours_logons']\n",
        "    \"\"\"\n",
        "\n",
        "    after_hours_counts = defaultdict(int)\n",
        "\n",
        "    # Track all weeks for the user\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_id, timestamp, logon_user, pc, activity = row  # Unpack columns\n",
        "\n",
        "        if (\n",
        "            activity.lower() == \"logon\" and logon_user == user\n",
        "        ):  # Only process logons for the specified user\n",
        "            try:\n",
        "                logon_time = datetime.strptime(timestamp, \"%m/%d/%Y %H:%M:%S\")\n",
        "                logon_week = logon_time.strftime(\"%Y-%W\")  # Ensure same format\n",
        "\n",
        "                # Store this week to ensure it's included in results\n",
        "                all_weeks.add(logon_week)\n",
        "\n",
        "                # Extract only the time component\n",
        "                logon_hour = logon_time.time()\n",
        "\n",
        "                # Check if the logon occurred outside business hours\n",
        "                if logon_hour < business_start or logon_hour >= business_end:\n",
        "                    after_hours_counts[logon_week] += 1\n",
        "\n",
        "            except ValueError:\n",
        "                continue  # Skip invalid timestamps\n",
        "\n",
        "    # Ensure all weeks in range are included (like `get_num_other_PC_per_week`)\n",
        "    if all_weeks:\n",
        "        min_week = min(all_weeks)\n",
        "        max_week = max(all_weeks)\n",
        "\n",
        "        # Generate all weeks in range\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        # Fill in missing weeks with 0\n",
        "        after_hours_counts = {\n",
        "            week: after_hours_counts.get(week, 0) for week in complete_weeks\n",
        "        }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    result_data = [\n",
        "        (user, week, after_hours_counts[week])\n",
        "        for week in sorted(after_hours_counts.keys())\n",
        "    ]\n",
        "    after_hours_df = pd.DataFrame(\n",
        "        result_data, columns=[\"user\", \"week\", \"after_hours_logons\"]\n",
        "    )\n",
        "\n",
        "    return after_hours_df\n",
        "\n",
        "\n",
        "# %%\n",
        "def find_insider_answers_file(user, insider_root):\n",
        "    \"\"\"\n",
        "    Recursively searches for the insider CSV file for the given user in the `insider_root` directory.\n",
        "\n",
        "    :param user: The user ID (e.g., \"CWW1120\")\n",
        "    :param insider_root: The root folder containing multiple r5.2-* subfolders.\n",
        "    :return: The full path to the user's insider CSV file if found, else None.\n",
        "    \"\"\"\n",
        "    for root, _, files in os.walk(insider_root):\n",
        "        for file in files:\n",
        "            if file.startswith(f\"/content/r5.2-\") and file.endswith(\n",
        "                f\"-{user}.csv\"\n",
        "            ):  # Match user file format\n",
        "                return os.path.join(root, file)  # Return full file path if found\n",
        "    return None  # Return None if no file is found\n",
        "\n",
        "\n",
        "def extract_weeks_from_csv(file_path):\n",
        "    \"\"\"\n",
        "    Reads a CSV file using `csv.reader` and extracts unique weeks from the timestamps (3rd column).\n",
        "\n",
        "    :param file_path: Path to the insider CSV file.\n",
        "    :return: A set of detected `Year-Week` values.\n",
        "    \"\"\"\n",
        "    insider_weeks = set()\n",
        "\n",
        "    try:\n",
        "        with open(file_path, mode=\"r\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "            reader = csv.reader(file)\n",
        "            for row in reader:\n",
        "                if len(row) < 3:  # Ensure the timestamp column exists\n",
        "                    continue\n",
        "                try:\n",
        "                    logon_time = datetime.strptime(\n",
        "                        row[2], \"%m/%d/%Y %H:%M:%S\"\n",
        "                    )  # Parse timestamp\n",
        "                    week = logon_time.strftime(\"%Y-%W\")  # Convert to Year-Week format\n",
        "                    insider_weeks.add(week)\n",
        "                except ValueError:\n",
        "                    continue  # Skip rows with invalid timestamps\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "    return insider_weeks\n",
        "\n",
        "\n",
        "def label_insider_weeks(df, user, insider_root):\n",
        "    \"\"\"\n",
        "    Adds an 'insider' column to the DataFrame by checking if the user's week exists in their insider file.\n",
        "\n",
        "    :param df: DataFrame containing ['user', 'week', 'num_other_pc']\n",
        "    :param user: The user ID for whom the dataframe is filtered.\n",
        "    :param insider_root: Path to the folder containing multiple r5.2-* subfolders.\n",
        "    :return: DataFrame with an 'insider' column.\n",
        "    \"\"\"\n",
        "\n",
        "    # Locate the user's insider file\n",
        "    insider_file = find_insider_answers_file(user, insider_root)\n",
        "\n",
        "    # If no insider file exists for the user, mark all weeks as 0 (not insider)\n",
        "    if not insider_file:\n",
        "        df[\"insider\"] = 0\n",
        "        return df\n",
        "\n",
        "    # Extract weeks from the insider CSV file\n",
        "    insider_weeks = extract_weeks_from_csv(insider_file)\n",
        "\n",
        "    # Label insider weeks in the user's dataframe\n",
        "    df[\"insider\"] = df[\"week\"].apply(lambda w: 1 if w in insider_weeks else 0)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def combine_user_feature_data(user, dataset_path, insider_root):\n",
        "    # Get data from different feature functions\n",
        "    logon_data = get_user_logon_data(user, dataset_path)\n",
        "    user_pc = get_user_pc(logon_data)\n",
        "    num_other_pc = get_num_other_PC_per_week(user, user_pc, logon_data)\n",
        "    after_hours_logons = get_after_hours_logons(logon_data, user)\n",
        "\n",
        "    exe_data = get_user_exe_data(user, dataset_path)\n",
        "    num_exe_files = get_num_exe_per_week(user, exe_data)\n",
        "\n",
        "    usb_data = get_user_usb_data(user, dataset_path)\n",
        "    num_usb = get_num_usb_insertions_per_week(user, usb_data)\n",
        "\n",
        "    # Extract relevant columns\n",
        "    after_hours_df = after_hours_logons[[\"week\", \"after_hours_logons\"]]\n",
        "    exe_df         = num_exe_files[[\"week\", \"num_exe_files\"]]\n",
        "    usb_df         = num_usb[[\"week\", \"num_usb_insertions\"]]\n",
        "    other_pc_df    = num_other_pc[[\"week\", \"num_other_pc\"]]\n",
        "\n",
        "    # Merge all dataframes on \"week\" using an outer join\n",
        "    merged_df = after_hours_df.merge(exe_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(usb_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(other_pc_df, on=\"week\", how=\"outer\")\n",
        "\n",
        "    # Replace NaN with 0 in all feature columns\n",
        "    merged_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Add user column\n",
        "    merged_df.insert(0, \"user\", user)\n",
        "    labeled_df = label_insider_weeks(merged_df, user, insider_root)\n",
        "    return labeled_df\n",
        "\n",
        "# Example usage\n",
        "dataset_path = os.path.join(\"Insider threat dataset\", \"r5.2\")\n",
        "user = \"ALT1465\"\n",
        "file_path = \"/content/ALT1465.csv\"  # Change to your actual file path\n",
        "final_df = pd.read_csv(file_path)\n",
        "# final_df = combine_user_feature_data(user, data\n",
        "print(final_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUs-cGSR2yTb",
        "outputId": "bd2174c3-52fc-4ac8-e0fe-b5c886188429"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       user     week  after_hours_logons  num_exe_files  num_usb_insertions  \\\n",
            "0   ALT1465  2010-01                   0              0                 0.0   \n",
            "1   ALT1465  2010-02                   4              0                 0.0   \n",
            "2   ALT1465  2010-03                   4              0                 0.0   \n",
            "3   ALT1465  2010-04                   4              0                 0.0   \n",
            "4   ALT1465  2010-05                   1              0                 0.0   \n",
            "5   ALT1465  2010-06                   0              0                 0.0   \n",
            "6   ALT1465  2010-07                   5              0                 0.0   \n",
            "7   ALT1465  2010-08                   2              0                 0.0   \n",
            "8   ALT1465  2010-09                   2              0                 0.0   \n",
            "9   ALT1465  2010-10                   5              0                 0.0   \n",
            "10  ALT1465  2010-11                   2              0                 0.0   \n",
            "11  ALT1465  2010-12                   3              0                 0.0   \n",
            "12  ALT1465  2010-13                   4              0                 0.0   \n",
            "13  ALT1465  2010-14                   1              0                 0.0   \n",
            "14  ALT1465  2010-15                   4              0                 0.0   \n",
            "15  ALT1465  2010-16                   4              0                 0.0   \n",
            "16  ALT1465  2010-17                   5              0                 0.0   \n",
            "17  ALT1465  2010-18                   0              0                 0.0   \n",
            "18  ALT1465  2010-19                   3              0                 0.0   \n",
            "19  ALT1465  2010-20                   3              0                 0.0   \n",
            "20  ALT1465  2010-21                   1              0                 0.0   \n",
            "21  ALT1465  2010-22                   3              0                 0.0   \n",
            "22  ALT1465  2010-23                   3              0                 0.0   \n",
            "23  ALT1465  2010-24                   1              0                 0.0   \n",
            "24  ALT1465  2010-25                   3              0                 0.0   \n",
            "25  ALT1465  2010-26                   3              0                 0.0   \n",
            "26  ALT1465  2010-27                   2              0                 0.0   \n",
            "27  ALT1465  2010-28                   3              0                 0.0   \n",
            "28  ALT1465  2010-29                   1              0                 0.0   \n",
            "29  ALT1465  2010-30                   2              0                 0.0   \n",
            "30  ALT1465  2010-31                   1              0                 0.0   \n",
            "31  ALT1465  2010-32                   3              0                 3.0   \n",
            "32  ALT1465  2010-33                   6              0                 3.0   \n",
            "33  ALT1465  2010-34                   2              0                 3.0   \n",
            "34  ALT1465  2010-35                   2              0                 0.0   \n",
            "35  ALT1465  2010-36                   1              0                 0.0   \n",
            "\n",
            "    num_other_pc  insider  \n",
            "0              2        0  \n",
            "1              2        0  \n",
            "2              2        0  \n",
            "3              2        0  \n",
            "4              0        0  \n",
            "5              1        0  \n",
            "6              2        0  \n",
            "7              1        0  \n",
            "8              1        0  \n",
            "9              2        0  \n",
            "10             2        0  \n",
            "11             2        0  \n",
            "12             1        0  \n",
            "13             1        0  \n",
            "14             2        0  \n",
            "15             1        0  \n",
            "16             2        0  \n",
            "17             0        0  \n",
            "18             1        0  \n",
            "19             2        0  \n",
            "20             2        0  \n",
            "21             2        0  \n",
            "22             2        0  \n",
            "23             1        0  \n",
            "24             1        0  \n",
            "25             1        0  \n",
            "26             0        0  \n",
            "27             1        0  \n",
            "28             1        0  \n",
            "29             1        0  \n",
            "30             1        0  \n",
            "31             1        1  \n",
            "32             2        1  \n",
            "33             1        0  \n",
            "34             0        0  \n",
            "35             1        0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "#  Select Features for Training\n",
        "feature_cols = [\"after_hours_logons\", \"num_exe_files\", \"num_usb_insertions\", \"num_other_pc\"]\n",
        "X = final_df[feature_cols]\n",
        "y_true = final_df[\"insider\"]  # Ground truth (0 = normal, 1 = anomaly)\n",
        "\n",
        "# Normalize the Features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "#  Train One-Class SVM\n",
        "oc_svm = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.9)  # Adjust nu for anomaly proportion\n",
        "oc_svm.fit(X_scaled)\n",
        "\n",
        "# Predict Anomalies\n",
        "decision_scores = oc_svm.decision_function(X_scaled)  # Get decision function scores\n",
        "threshold = np.percentile(decision_scores, 90)  # Detect top 10% as anomalies\n",
        "y_pred = [1 if score < threshold else 0 for score in decision_scores]\n",
        "\n",
        "#  Calculate Evaluation Metrics\n",
        "precision = precision_score(y_true, y_pred, pos_label=1)\n",
        "recall = recall_score(y_true, y_pred, pos_label=1)\n",
        "f1 = f1_score(y_true, y_pred, pos_label=1)\n",
        "\n",
        "#  Print Results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbYVq29ZEOyq",
        "outputId": "02f6901f-b72c-4534-a9a2-3969ea4f65cb"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.0690\n",
            "Recall: 1.0000\n",
            "F1-score: 0.1290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import LocalOutlierFactor  # Import LocalOutlierFactor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Define your possible contamination values (thresholds)\n",
        "contamination_values = [0.01, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15]\n",
        "\n",
        "# Loop through each contamination value and calculate performance metrics\n",
        "for contamination in contamination_values:\n",
        "    lof = LocalOutlierFactor(n_neighbors=20, contamination=contamination)\n",
        "\n",
        "    # Fit LOF model and get predictions\n",
        "    y_pred = lof.fit_predict(X)  # Assuming X_train is your feature matrix\n",
        "    y_pred = [1 if pred == -1 else 0 for pred in y_pred]  # Convert LOF output (-1 for outliers, 1 for inliers)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for insider=1 (malicious)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"Contamination = {contamination}: Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QB4V-6SwFaGV",
        "outputId": "5abe9e8a-9891-47cb-de57-492c6c81ebe5"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contamination = 0.01: Precision = 1.0000, Recall = 0.5000, F1 Score = 0.6667\n",
            "Contamination = 0.025: Precision = 1.0000, Recall = 0.5000, F1 Score = 0.6667\n",
            "Contamination = 0.05: Precision = 1.0000, Recall = 1.0000, F1 Score = 1.0000\n",
            "Contamination = 0.075: Precision = 0.6667, Recall = 1.0000, F1 Score = 0.8000\n",
            "Contamination = 0.1: Precision = 0.5000, Recall = 1.0000, F1 Score = 0.6667\n",
            "Contamination = 0.125: Precision = 0.4000, Recall = 1.0000, F1 Score = 0.5714\n",
            "Contamination = 0.15: Precision = 0.3333, Recall = 1.0000, F1 Score = 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df[final_df[\"num_usb_insertions\"]!=0].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "gzgoEVyQExoA",
        "outputId": "f079748a-81b3-44f8-b0d1-a9f6bd004ab0"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       user     week  after_hours_logons  num_exe_files  num_usb_insertions  \\\n",
              "31  ALT1465  2010-32                   3              0                 3.0   \n",
              "32  ALT1465  2010-33                   6              0                 3.0   \n",
              "33  ALT1465  2010-34                   2              0                 3.0   \n",
              "\n",
              "    num_other_pc  insider  \n",
              "31             1        1  \n",
              "32             2        1  \n",
              "33             1        0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-753b6b79-8082-4c22-b6f3-9f423daad83c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>week</th>\n",
              "      <th>after_hours_logons</th>\n",
              "      <th>num_exe_files</th>\n",
              "      <th>num_usb_insertions</th>\n",
              "      <th>num_other_pc</th>\n",
              "      <th>insider</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>ALT1465</td>\n",
              "      <td>2010-32</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>ALT1465</td>\n",
              "      <td>2010-33</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>ALT1465</td>\n",
              "      <td>2010-34</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-753b6b79-8082-4c22-b6f3-9f423daad83c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-753b6b79-8082-4c22-b6f3-9f423daad83c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-753b6b79-8082-4c22-b6f3-9f423daad83c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-674cb885-e50e-4f5c-85b0-b24eb2469be3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-674cb885-e50e-4f5c-85b0-b24eb2469be3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-674cb885-e50e-4f5c-85b0-b24eb2469be3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"final_df[final_df[\\\"num_usb_insertions\\\"]!=0]\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"user\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"ALT1465\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"week\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2010-32\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"after_hours_logons\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 2,\n        \"max\": 6,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_exe_files\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_usb_insertions\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 3.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_other_pc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"insider\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "import csv\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from datetime import datetime, time, timedelta\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_user_usb_data(user_id, dataset_path):\n",
        "    usb_data = []\n",
        "    with open(os.path.join(dataset_path, \"/content/device.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[5] == \"Connect\":  # Check user and only \"Connect\" activity\n",
        "                usb_data.append(row)\n",
        "    return usb_data\n",
        "\n",
        "def get_num_usb_insertions_per_week(user, usb_data):\n",
        "    weekly_usb_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "    for row in usb_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_usb_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_usb_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_usb_insertions\"])\n",
        "\n",
        "\n",
        "def get_user_exe_data(user_id, dataset_path):\n",
        "    exe_data = []\n",
        "    with open(os.path.join(dataset_path, \"/content/file.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[4].endswith(\".exe\"):  # Check user and .exe files\n",
        "                exe_data.append(row)\n",
        "    return exe_data\n",
        "\n",
        "def get_num_exe_per_week(user, exe_data):\n",
        "    weekly_exe_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in exe_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_exe_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_exe_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_exe_files\"])\n",
        "\n",
        "# %%\n",
        "def get_user_logon_data(user_id, dataset_path):\n",
        "    logon_data = []\n",
        "    with open(os.path.join(dataset_path, \"/content/logon.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        for row in reader:\n",
        "            if row[2] == user_id:\n",
        "                logon_data.append(row)\n",
        "    return logon_data\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_user_pc(logon_data):\n",
        "    pc_dict = {}\n",
        "    for row in logon_data:\n",
        "        pc_dict[row[3]] = 1 + pc_dict.get(row[3], 0)\n",
        "    user_pc = max(pc_dict, key=pc_dict.get)\n",
        "    return user_pc\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_num_other_PC_per_week(user, user_pc, logon_data):\n",
        "    weekly_pc_counts = defaultdict(set)  # Dictionary to store unique PCs per week\n",
        "    all_weeks = set()  # Set to track all weeks where logons occurred\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")  # Adjusted format\n",
        "        week = logon_time.strftime(\"%Y-%W\")  # Year-Week format\n",
        "        all_weeks.add(week)  # Track all weeks\n",
        "\n",
        "        if row[3] != user_pc:  # Check if PC is different from user's primary PC\n",
        "            weekly_pc_counts[week].add(\n",
        "                row[3]\n",
        "            )  # Add PC to the week's set (unique values only)\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks)\n",
        "    max_week = max(all_weeks)\n",
        "\n",
        "    # Generate all weeks between min and max\n",
        "    start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "    end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "    current_date = start_date\n",
        "    complete_weeks = set()\n",
        "\n",
        "    while current_date <= end_date:\n",
        "        week_str = current_date.strftime(\"%Y-%W\")\n",
        "        complete_weeks.add(week_str)\n",
        "        current_date += timedelta(days=7)\n",
        "\n",
        "    # Ensure every week has a count (0 if no other PCs were accessed)\n",
        "    weekly_counts = {\n",
        "        week: len(weekly_pc_counts[week]) if week in weekly_pc_counts else 0\n",
        "        for week in complete_weeks\n",
        "    }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_other_pc\"])\n",
        "\n",
        "\n",
        "def get_after_hours_logons(\n",
        "    logon_data, user, business_start=time(9, 0, 0), business_end=time(17, 0, 0)\n",
        "):\n",
        "    \"\"\"\n",
        "    Aggregates after-hours logons per week for a specified user.\n",
        "\n",
        "    :param logon_data: List of logon events in the format [id, date, user, pc, activity]\n",
        "    :param user: The specific user to filter logon events for.\n",
        "    :param business_start: Datetime.time representing start of business hours.\n",
        "    :param business_end: Datetime.time representing end of business hours.\n",
        "    :return: DataFrame with ['user', 'week', 'after_hours_logons']\n",
        "    \"\"\"\n",
        "\n",
        "    after_hours_counts = defaultdict(int)\n",
        "\n",
        "    # Track all weeks for the user\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_id, timestamp, logon_user, pc, activity = row  # Unpack columns\n",
        "\n",
        "        if (\n",
        "            activity.lower() == \"logon\" and logon_user == user\n",
        "        ):  # Only process logons for the specified user\n",
        "            try:\n",
        "                logon_time = datetime.strptime(timestamp, \"%m/%d/%Y %H:%M:%S\")\n",
        "                logon_week = logon_time.strftime(\"%Y-%W\")  # Ensure same format\n",
        "\n",
        "                # Store this week to ensure it's included in results\n",
        "                all_weeks.add(logon_week)\n",
        "\n",
        "                # Extract only the time component\n",
        "                logon_hour = logon_time.time()\n",
        "\n",
        "                # Check if the logon occurred outside business hours\n",
        "                if logon_hour < business_start or logon_hour >= business_end:\n",
        "                    after_hours_counts[logon_week] += 1\n",
        "\n",
        "            except ValueError:\n",
        "                continue  # Skip invalid timestamps\n",
        "\n",
        "    # Ensure all weeks in range are included (like `get_num_other_PC_per_week`)\n",
        "    if all_weeks:\n",
        "        min_week = min(all_weeks)\n",
        "        max_week = max(all_weeks)\n",
        "\n",
        "        # Generate all weeks in range\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        # Fill in missing weeks with 0\n",
        "        after_hours_counts = {\n",
        "            week: after_hours_counts.get(week, 0) for week in complete_weeks\n",
        "        }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    result_data = [\n",
        "        (user, week, after_hours_counts[week])\n",
        "        for week in sorted(after_hours_counts.keys())\n",
        "    ]\n",
        "    after_hours_df = pd.DataFrame(\n",
        "        result_data, columns=[\"user\", \"week\", \"after_hours_logons\"]\n",
        "    )\n",
        "\n",
        "    return after_hours_df\n",
        "\n",
        "\n",
        "# %%\n",
        "def find_insider_answers_file(user, insider_root):\n",
        "    \"\"\"\n",
        "    Recursively searches for the insider CSV file for the given user in the `insider_root` directory.\n",
        "\n",
        "    :param user: The user ID (e.g., \"CWW1120\")\n",
        "    :param insider_root: The root folder containing multiple r5.2-* subfolders.\n",
        "    :return: The full path to the user's insider CSV file if found, else None.\n",
        "    \"\"\"\n",
        "    for root, _, files in os.walk(insider_root):\n",
        "        for file in files:\n",
        "            if file.startswith(f\"/content/r5.2-\") and file.endswith(\n",
        "                f\"-{user}.csv\"\n",
        "            ):  # Match user file format\n",
        "                return os.path.join(root, file)  # Return full file path if found\n",
        "    return None  # Return None if no file is found\n",
        "\n",
        "\n",
        "def extract_weeks_from_csv(file_path):\n",
        "    \"\"\"\n",
        "    Reads a CSV file using `csv.reader` and extracts unique weeks from the timestamps (3rd column).\n",
        "\n",
        "    :param file_path: Path to the insider CSV file.\n",
        "    :return: A set of detected `Year-Week` values.\n",
        "    \"\"\"\n",
        "    insider_weeks = set()\n",
        "\n",
        "    try:\n",
        "        with open(file_path, mode=\"r\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "            reader = csv.reader(file)\n",
        "            for row in reader:\n",
        "                if len(row) < 3:  # Ensure the timestamp column exists\n",
        "                    continue\n",
        "                try:\n",
        "                    logon_time = datetime.strptime(\n",
        "                        row[2], \"%m/%d/%Y %H:%M:%S\"\n",
        "                    )  # Parse timestamp\n",
        "                    week = logon_time.strftime(\"%Y-%W\")  # Convert to Year-Week format\n",
        "                    insider_weeks.add(week)\n",
        "                except ValueError:\n",
        "                    continue  # Skip rows with invalid timestamps\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "    return insider_weeks\n",
        "\n",
        "\n",
        "def label_insider_weeks(df, user, insider_root):\n",
        "    \"\"\"\n",
        "    Adds an 'insider' column to the DataFrame by checking if the user's week exists in their insider file.\n",
        "\n",
        "    :param df: DataFrame containing ['user', 'week', 'num_other_pc']\n",
        "    :param user: The user ID for whom the dataframe is filtered.\n",
        "    :param insider_root: Path to the folder containing multiple r5.2-* subfolders.\n",
        "    :return: DataFrame with an 'insider' column.\n",
        "    \"\"\"\n",
        "\n",
        "    # Locate the user's insider file\n",
        "    insider_file = find_insider_answers_file(user, insider_root)\n",
        "\n",
        "    # If no insider file exists for the user, mark all weeks as 0 (not insider)\n",
        "    if not insider_file:\n",
        "        df[\"insider\"] = 0\n",
        "        return df\n",
        "\n",
        "    # Extract weeks from the insider CSV file\n",
        "    insider_weeks = extract_weeks_from_csv(insider_file)\n",
        "\n",
        "    # Label insider weeks in the user's dataframe\n",
        "    df[\"insider\"] = df[\"week\"].apply(lambda w: 1 if w in insider_weeks else 0)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def combine_user_feature_data(user, dataset_path, insider_root):\n",
        "    # Get data from different feature functions\n",
        "    logon_data = get_user_logon_data(user, dataset_path)\n",
        "    user_pc = get_user_pc(logon_data)\n",
        "    num_other_pc = get_num_other_PC_per_week(user, user_pc, logon_data)\n",
        "    after_hours_logons = get_after_hours_logons(logon_data, user)\n",
        "\n",
        "    exe_data = get_user_exe_data(user, dataset_path)\n",
        "    num_exe_files = get_num_exe_per_week(user, exe_data)\n",
        "\n",
        "    usb_data = get_user_usb_data(user, dataset_path)\n",
        "    num_usb = get_num_usb_insertions_per_week(user, usb_data)\n",
        "\n",
        "    # Extract relevant columns\n",
        "    after_hours_df = after_hours_logons[[\"week\", \"after_hours_logons\"]]\n",
        "    exe_df         = num_exe_files[[\"week\", \"num_exe_files\"]]\n",
        "    usb_df         = num_usb[[\"week\", \"num_usb_insertions\"]]\n",
        "    other_pc_df    = num_other_pc[[\"week\", \"num_other_pc\"]]\n",
        "\n",
        "    # Merge all dataframes on \"week\" using an outer join\n",
        "    merged_df = after_hours_df.merge(exe_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(usb_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(other_pc_df, on=\"week\", how=\"outer\")\n",
        "\n",
        "    # Replace NaN with 0 in all feature columns\n",
        "    merged_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Add user column\n",
        "    merged_df.insert(0, \"user\", user)\n",
        "    labeled_df = label_insider_weeks(merged_df, user, insider_root)\n",
        "    return labeled_df\n",
        "\n",
        "# Example usage\n",
        "dataset_path = os.path.join(\"Insider threat dataset\", \"r5.2\")\n",
        "user = \"NIV1608\"\n",
        "file_path = \"/content/NIV1608.csv\"  # Change to your actual file path\n",
        "final_df = pd.read_csv(file_path)\n",
        "# final_df = combine_user_feature_data(user, data\n",
        "print(final_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMs4yp8OGZ7G",
        "outputId": "9c9d379d-58be-40a4-c1c7-2c87620b23fb"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       user     week  after_hours_logons  num_exe_files  num_usb_insertions  \\\n",
            "0   NIV1608  2010-01                   3              0                 0.0   \n",
            "1   NIV1608  2010-02                   2              0                 0.0   \n",
            "2   NIV1608  2010-03                   4              0                 0.0   \n",
            "3   NIV1608  2010-04                   1              0                 0.0   \n",
            "4   NIV1608  2010-05                   2              0                 0.0   \n",
            "..      ...      ...                 ...            ...                 ...   \n",
            "56  NIV1608  2011-05                   5              0                 7.0   \n",
            "57  NIV1608  2011-06                   7              0                 7.0   \n",
            "58  NIV1608  2011-07                   3              0                 4.0   \n",
            "59  NIV1608  2011-08                   1              0                 0.0   \n",
            "60  NIV1608  2011-09                   0              0                 0.0   \n",
            "\n",
            "    num_other_pc  insider  \n",
            "0              0        0  \n",
            "1              0        0  \n",
            "2              0        0  \n",
            "3              0        0  \n",
            "4              0        0  \n",
            "..           ...      ...  \n",
            "56             0        1  \n",
            "57             0        1  \n",
            "58             0        0  \n",
            "59             0        0  \n",
            "60             0        0  \n",
            "\n",
            "[61 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "#  Select Features for Training\n",
        "feature_cols = [\"after_hours_logons\", \"num_exe_files\", \"num_usb_insertions\", \"num_other_pc\"]\n",
        "X = final_df[feature_cols]\n",
        "y_true = final_df[\"insider\"]  # Ground truth (0 = normal, 1 = anomaly)\n",
        "\n",
        "# Normalize the Features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "#  Train One-Class SVM\n",
        "oc_svm = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.9)  # Adjust nu for anomaly proportion\n",
        "oc_svm.fit(X_scaled)\n",
        "\n",
        "# Predict Anomalies\n",
        "decision_scores = oc_svm.decision_function(X_scaled)  # Get decision function scores\n",
        "threshold = np.percentile(decision_scores, 90)  # Detect top 10% as anomalies\n",
        "y_pred = [1 if score < threshold else 0 for score in decision_scores]\n",
        "\n",
        "#  Calculate Evaluation Metrics\n",
        "precision = precision_score(y_true, y_pred, pos_label=1)\n",
        "recall = recall_score(y_true, y_pred, pos_label=1)\n",
        "f1 = f1_score(y_true, y_pred, pos_label=1)\n",
        "\n",
        "#  Print Results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGdwffCzGxUS",
        "outputId": "c2b0ebdb-c272-4754-c155-a3e10775e6c5"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.0488\n",
            "Recall: 1.0000\n",
            "F1-score: 0.0930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import LocalOutlierFactor  # Import LocalOutlierFactor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Define your possible contamination values (thresholds)\n",
        "contamination_values = [0.01, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15]\n",
        "\n",
        "# Loop through each contamination value and calculate performance metrics\n",
        "for contamination in contamination_values:\n",
        "    lof = LocalOutlierFactor(n_neighbors=20, contamination=contamination)\n",
        "\n",
        "    # Fit LOF model and get predictions\n",
        "    y_pred = lof.fit_predict(X)  # Assuming X_train is your feature matrix\n",
        "    y_pred = [1 if pred == -1 else 0 for pred in y_pred]  # Convert LOF output (-1 for outliers, 1 for inliers)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for insider=1 (malicious)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"Contamination = {contamination}: Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWS7YIShG8za",
        "outputId": "09612889-a9f7-4ed4-cd47-33e1381f9bb7"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contamination = 0.01: Precision = 1.0000, Recall = 0.5000, F1 Score = 0.6667\n",
            "Contamination = 0.025: Precision = 1.0000, Recall = 1.0000, F1 Score = 1.0000\n",
            "Contamination = 0.05: Precision = 0.6667, Recall = 1.0000, F1 Score = 0.8000\n",
            "Contamination = 0.075: Precision = 0.6667, Recall = 1.0000, F1 Score = 0.8000\n",
            "Contamination = 0.1: Precision = 0.6667, Recall = 1.0000, F1 Score = 0.8000\n",
            "Contamination = 0.125: Precision = 0.6667, Recall = 1.0000, F1 Score = 0.8000\n",
            "Contamination = 0.15: Precision = 0.2222, Recall = 1.0000, F1 Score = 0.3636\n"
          ]
        }
      ]
    }
  ]
}