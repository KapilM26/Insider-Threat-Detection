{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN79iHr7Iw7+u3kbtseBl9e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KapilM26/Insider-Threat-Detection/blob/main/one_svm2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEDgbI0dbst1",
        "outputId": "b6991c67-0178-4880-f7ef-22361ad5a204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       user     week  after_hours_logons  num_exe_files  num_usb_insertions  \\\n",
            "0   MIB0203  2010-01                   5              0                 0.0   \n",
            "1   MIB0203  2010-02                   7              0                 0.0   \n",
            "2   MIB0203  2010-03                   6              0                 0.0   \n",
            "3   MIB0203  2010-04                   5              0                 0.0   \n",
            "4   MIB0203  2010-05                   5              0                 0.0   \n",
            "5   MIB0203  2010-06                   6              0                 0.0   \n",
            "6   MIB0203  2010-07                   5              0                 0.0   \n",
            "7   MIB0203  2010-08                   5              0                 0.0   \n",
            "8   MIB0203  2010-09                   6              0                 0.0   \n",
            "9   MIB0203  2010-10                   5              0                 0.0   \n",
            "10  MIB0203  2010-11                   5              0                 0.0   \n",
            "11  MIB0203  2010-12                   5              0                 0.0   \n",
            "12  MIB0203  2010-13                   4              0                 0.0   \n",
            "13  MIB0203  2010-14                   5              0                 0.0   \n",
            "14  MIB0203  2010-15                   5              0                 0.0   \n",
            "15  MIB0203  2010-16                   5              0                 0.0   \n",
            "16  MIB0203  2010-17                   5              0                 0.0   \n",
            "17  MIB0203  2010-18                   5              0                 0.0   \n",
            "18  MIB0203  2010-19                   5              0                 0.0   \n",
            "19  MIB0203  2010-20                   5              0                 0.0   \n",
            "20  MIB0203  2010-21                   5              0                 0.0   \n",
            "21  MIB0203  2010-22                   4              0                 0.0   \n",
            "22  MIB0203  2010-23                   5              0                 0.0   \n",
            "23  MIB0203  2010-24                   5              0                 0.0   \n",
            "24  MIB0203  2010-25                   5              0                 0.0   \n",
            "25  MIB0203  2010-26                   5              0                 0.0   \n",
            "26  MIB0203  2010-27                   4              0                 0.0   \n",
            "27  MIB0203  2010-28                   5              0                 0.0   \n",
            "28  MIB0203  2010-29                   5              0                 0.0   \n",
            "29  MIB0203  2010-30                   6              0                 0.0   \n",
            "30  MIB0203  2010-31                   5              0                 0.0   \n",
            "31  MIB0203  2010-32                   5              0                 0.0   \n",
            "32  MIB0203  2010-33                   5              0                 0.0   \n",
            "33  MIB0203  2010-34                   6              0                 0.0   \n",
            "34  MIB0203  2010-35                   5              0                 0.0   \n",
            "35  MIB0203  2010-36                   4              0                 0.0   \n",
            "36  MIB0203  2010-37                   5              0                 0.0   \n",
            "37  MIB0203  2010-38                   5              0                 0.0   \n",
            "38  MIB0203  2010-39                   5              0                 0.0   \n",
            "39  MIB0203  2010-40                   6              0                 0.0   \n",
            "40  MIB0203  2010-41                   7              0                 1.0   \n",
            "41  MIB0203  2010-42                   7              0                 1.0   \n",
            "42  MIB0203  2010-43                   5              0                 0.0   \n",
            "43  MIB0203  2010-44                   4              0                 0.0   \n",
            "\n",
            "    num_other_pc  insider  \n",
            "0              0        0  \n",
            "1              1        0  \n",
            "2              1        0  \n",
            "3              0        0  \n",
            "4              0        0  \n",
            "5              1        0  \n",
            "6              0        0  \n",
            "7              0        0  \n",
            "8              1        0  \n",
            "9              1        0  \n",
            "10             0        0  \n",
            "11             0        0  \n",
            "12             0        0  \n",
            "13             0        0  \n",
            "14             0        0  \n",
            "15             0        0  \n",
            "16             0        0  \n",
            "17             0        0  \n",
            "18             0        0  \n",
            "19             0        0  \n",
            "20             0        0  \n",
            "21             0        0  \n",
            "22             0        0  \n",
            "23             0        0  \n",
            "24             0        0  \n",
            "25             0        0  \n",
            "26             0        0  \n",
            "27             0        0  \n",
            "28             0        0  \n",
            "29             1        0  \n",
            "30             0        0  \n",
            "31             0        0  \n",
            "32             0        0  \n",
            "33             1        0  \n",
            "34             0        0  \n",
            "35             0        0  \n",
            "36             0        0  \n",
            "37             0        0  \n",
            "38             0        0  \n",
            "39             0        0  \n",
            "40             0        1  \n",
            "41             1        1  \n",
            "42             0        0  \n",
            "43             0        0  \n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "import csv\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from datetime import datetime, time, timedelta\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_user_usb_data(user_id, dataset_path):\n",
        "    usb_data = []\n",
        "    with open(os.path.join(dataset_path, \"/device.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[5] == \"Connect\":  # Check user and only \"Connect\" activity\n",
        "                usb_data.append(row)\n",
        "    return usb_data\n",
        "\n",
        "def get_num_usb_insertions_per_week(user, usb_data):\n",
        "    weekly_usb_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "    for row in usb_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_usb_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_usb_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_usb_insertions\"])\n",
        "\n",
        "\n",
        "def get_user_exe_data(user_id, dataset_path):\n",
        "    exe_data = []\n",
        "    with open(os.path.join(dataset_path, \"/file.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[4].endswith(\".exe\"):  # Check user and .exe files\n",
        "                exe_data.append(row)\n",
        "    return exe_data\n",
        "\n",
        "def get_num_exe_per_week(user, exe_data):\n",
        "    weekly_exe_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in exe_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_exe_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_exe_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_exe_files\"])\n",
        "\n",
        "# %%\n",
        "def get_user_logon_data(user_id, dataset_path):\n",
        "    logon_data = []\n",
        "    with open(os.path.join(dataset_path, \"/logon.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        for row in reader:\n",
        "            if row[2] == user_id:\n",
        "                logon_data.append(row)\n",
        "    return logon_data\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_user_pc(logon_data):\n",
        "    pc_dict = {}\n",
        "    for row in logon_data:\n",
        "        pc_dict[row[3]] = 1 + pc_dict.get(row[3], 0)\n",
        "    user_pc = max(pc_dict, key=pc_dict.get)\n",
        "    return user_pc\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_num_other_PC_per_week(user, user_pc, logon_data):\n",
        "    weekly_pc_counts = defaultdict(set)  # Dictionary to store unique PCs per week\n",
        "    all_weeks = set()  # Set to track all weeks where logons occurred\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")  # Adjusted format\n",
        "        week = logon_time.strftime(\"%Y-%W\")  # Year-Week format\n",
        "        all_weeks.add(week)  # Track all weeks\n",
        "\n",
        "        if row[3] != user_pc:  # Check if PC is different from user's primary PC\n",
        "            weekly_pc_counts[week].add(\n",
        "                row[3]\n",
        "            )  # Add PC to the week's set (unique values only)\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks)\n",
        "    max_week = max(all_weeks)\n",
        "\n",
        "    # Generate all weeks between min and max\n",
        "    start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "    end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "    current_date = start_date\n",
        "    complete_weeks = set()\n",
        "\n",
        "    while current_date <= end_date:\n",
        "        week_str = current_date.strftime(\"%Y-%W\")\n",
        "        complete_weeks.add(week_str)\n",
        "        current_date += timedelta(days=7)\n",
        "\n",
        "    # Ensure every week has a count (0 if no other PCs were accessed)\n",
        "    weekly_counts = {\n",
        "        week: len(weekly_pc_counts[week]) if week in weekly_pc_counts else 0\n",
        "        for week in complete_weeks\n",
        "    }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_other_pc\"])\n",
        "\n",
        "\n",
        "def get_after_hours_logons(\n",
        "    logon_data, user, business_start=time(9, 0, 0), business_end=time(17, 0, 0)\n",
        "):\n",
        "    \"\"\"\n",
        "    Aggregates after-hours logons per week for a specified user.\n",
        "\n",
        "    :param logon_data: List of logon events in the format [id, date, user, pc, activity]\n",
        "    :param user: The specific user to filter logon events for.\n",
        "    :param business_start: Datetime.time representing start of business hours.\n",
        "    :param business_end: Datetime.time representing end of business hours.\n",
        "    :return: DataFrame with ['user', 'week', 'after_hours_logons']\n",
        "    \"\"\"\n",
        "\n",
        "    after_hours_counts = defaultdict(int)\n",
        "\n",
        "    # Track all weeks for the user\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_id, timestamp, logon_user, pc, activity = row  # Unpack columns\n",
        "\n",
        "        if (\n",
        "            activity.lower() == \"logon\" and logon_user == user\n",
        "        ):  # Only process logons for the specified user\n",
        "            try:\n",
        "                logon_time = datetime.strptime(timestamp, \"%m/%d/%Y %H:%M:%S\")\n",
        "                logon_week = logon_time.strftime(\"%Y-%W\")  # Ensure same format\n",
        "\n",
        "                # Store this week to ensure it's included in results\n",
        "                all_weeks.add(logon_week)\n",
        "\n",
        "                # Extract only the time component\n",
        "                logon_hour = logon_time.time()\n",
        "\n",
        "                # Check if the logon occurred outside business hours\n",
        "                if logon_hour < business_start or logon_hour >= business_end:\n",
        "                    after_hours_counts[logon_week] += 1\n",
        "\n",
        "            except ValueError:\n",
        "                continue  # Skip invalid timestamps\n",
        "\n",
        "    # Ensure all weeks in range are included (like `get_num_other_PC_per_week`)\n",
        "    if all_weeks:\n",
        "        min_week = min(all_weeks)\n",
        "        max_week = max(all_weeks)\n",
        "\n",
        "        # Generate all weeks in range\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        # Fill in missing weeks with 0\n",
        "        after_hours_counts = {\n",
        "            week: after_hours_counts.get(week, 0) for week in complete_weeks\n",
        "        }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    result_data = [\n",
        "        (user, week, after_hours_counts[week])\n",
        "        for week in sorted(after_hours_counts.keys())\n",
        "    ]\n",
        "    after_hours_df = pd.DataFrame(\n",
        "        result_data, columns=[\"user\", \"week\", \"after_hours_logons\"]\n",
        "    )\n",
        "\n",
        "    return after_hours_df\n",
        "\n",
        "\n",
        "# %%\n",
        "def find_insider_answers_file(user, insider_root):\n",
        "    \"\"\"\n",
        "    Recursively searches for the insider CSV file for the given user in the `insider_root` directory.\n",
        "\n",
        "    :param user: The user ID (e.g., \"CWW1120\")\n",
        "    :param insider_root: The root folder containing multiple r5.2-* subfolders.\n",
        "    :return: The full path to the user's insider CSV file if found, else None.\n",
        "    \"\"\"\n",
        "    for root, _, files in os.walk(insider_root):\n",
        "        for file in files:\n",
        "            if file.startswith(f\"/content/r5.2-\") and file.endswith(\n",
        "                f\"-{user}.csv\"\n",
        "            ):  # Match user file format\n",
        "                return os.path.join(root, file)  # Return full file path if found\n",
        "    return None  # Return None if no file is found\n",
        "\n",
        "\n",
        "def extract_weeks_from_csv(file_path):\n",
        "    \"\"\"\n",
        "    Reads a CSV file using `csv.reader` and extracts unique weeks from the timestamps (3rd column).\n",
        "\n",
        "    :param file_path: Path to the insider CSV file.\n",
        "    :return: A set of detected `Year-Week` values.\n",
        "    \"\"\"\n",
        "    insider_weeks = set()\n",
        "\n",
        "    try:\n",
        "        with open(file_path, mode=\"r\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "            reader = csv.reader(file)\n",
        "            for row in reader:\n",
        "                if len(row) < 3:  # Ensure the timestamp column exists\n",
        "                    continue\n",
        "                try:\n",
        "                    logon_time = datetime.strptime(\n",
        "                        row[2], \"%m/%d/%Y %H:%M:%S\"\n",
        "                    )  # Parse timestamp\n",
        "                    week = logon_time.strftime(\"%Y-%W\")  # Convert to Year-Week format\n",
        "                    insider_weeks.add(week)\n",
        "                except ValueError:\n",
        "                    continue  # Skip rows with invalid timestamps\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "    return insider_weeks\n",
        "\n",
        "\n",
        "def label_insider_weeks(df, user, insider_root):\n",
        "    \"\"\"\n",
        "    Adds an 'insider' column to the DataFrame by checking if the user's week exists in their insider file.\n",
        "\n",
        "    :param df: DataFrame containing ['user', 'week', 'num_other_pc']\n",
        "    :param user: The user ID for whom the dataframe is filtered.\n",
        "    :param insider_root: Path to the folder containing multiple r5.2-* subfolders.\n",
        "    :return: DataFrame with an 'insider' column.\n",
        "    \"\"\"\n",
        "\n",
        "    # Locate the user's insider file\n",
        "    insider_file = find_insider_answers_file(user, insider_root)\n",
        "\n",
        "    # If no insider file exists for the user, mark all weeks as 0 (not insider)\n",
        "    if not insider_file:\n",
        "        df[\"insider\"] = 0\n",
        "        return df\n",
        "\n",
        "    # Extract weeks from the insider CSV file\n",
        "    insider_weeks = extract_weeks_from_csv(insider_file)\n",
        "\n",
        "    # Label insider weeks in the user's dataframe\n",
        "    df[\"insider\"] = df[\"week\"].apply(lambda w: 1 if w in insider_weeks else 0)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def combine_user_feature_data(user, dataset_path, insider_root):\n",
        "    # Get data from different feature functions\n",
        "    logon_data = get_user_logon_data(user, dataset_path)\n",
        "    user_pc = get_user_pc(logon_data)\n",
        "    num_other_pc = get_num_other_PC_per_week(user, user_pc, logon_data)\n",
        "    after_hours_logons = get_after_hours_logons(logon_data, user)\n",
        "\n",
        "    exe_data = get_user_exe_data(user, dataset_path)\n",
        "    num_exe_files = get_num_exe_per_week(user, exe_data)\n",
        "\n",
        "    usb_data = get_user_usb_data(user, dataset_path)\n",
        "    num_usb = get_num_usb_insertions_per_week(user, usb_data)\n",
        "\n",
        "    # Extract relevant columns\n",
        "    after_hours_df = after_hours_logons[[\"week\", \"after_hours_logons\"]]\n",
        "    exe_df         = num_exe_files[[\"week\", \"num_exe_files\"]]\n",
        "    usb_df         = num_usb[[\"week\", \"num_usb_insertions\"]]\n",
        "    other_pc_df    = num_other_pc[[\"week\", \"num_other_pc\"]]\n",
        "\n",
        "    # Merge all dataframes on \"week\" using an outer join\n",
        "    merged_df = after_hours_df.merge(exe_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(usb_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(other_pc_df, on=\"week\", how=\"outer\")\n",
        "\n",
        "    # Replace NaN with 0 in all feature columns\n",
        "    merged_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Add user column\n",
        "    merged_df.insert(0, \"user\", user)\n",
        "    labeled_df = label_insider_weeks(merged_df, user, insider_root)\n",
        "    return labeled_df\n",
        "\n",
        "# Example usage\n",
        "dataset_path = os.path.join(\"Insider threat dataset\", \"r5.2\")\n",
        "user = \"MIB0203\"\n",
        "file_path = \"/MIB0203.csv\"  # Change to your actual file path\n",
        "final_df = pd.read_csv(file_path)\n",
        "# final_df = combine_user_feature_data(user, dataset_path, insider_root)\n",
        "print(final_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Access the 'num_other_pc' data from the 'final_df' DataFrame\n",
        "# num_other_pc_data = final_df[['num_other_pc']]\n",
        "\n",
        "# # Initialize the Isolation Forest model\n",
        "# iso_forest = IsolationForest(contamination='auto', random_state=42)\n",
        "\n",
        "# # Fit the model\n",
        "# iso_forest.fit(num_other_pc_data)\n",
        "\n",
        "# # Predict anomalies and add results to the 'final_df' DataFrame\n",
        "# final_df['anomaly'] = iso_forest.predict(num_other_pc_data)\n",
        "# final_df['anomaly_score'] = iso_forest.decision_function(num_other_pc_data)\n",
        "\n",
        "# # Convert anomaly labels to boolean (1 for anomaly, -1 for normal)\n",
        "# final_df['anomaly'] = final_df['anomaly'].apply(lambda x: 1 if x == -1 else 0)\n",
        "\n",
        "# # Display the rows where anomalies were detected\n",
        "# print(final_df[final_df['anomaly'] == 1])"
      ],
      "metadata": {
        "id": "aHJ6fDm-erbz"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_-VI7jyJtkEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pd.Series(y_pred).value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju74hOxQvFiC",
        "outputId": "d1916ff3-c5ec-4ee2-ec1c-7f96c265a2a2"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1    16\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load your dataset (assuming final_df is already loaded from the uploaded data)\n",
        "# If final_df is missing, make sure you load it using the proper dataset\n",
        "df = pd.read_csv('/MIB0203.csv')  # Make sure the path is correct for your environment\n",
        "\n",
        "# Select Features\n",
        "feature_cols = [\"after_hours_logons\", \"num_exe_files\", \"num_usb_insertions\", \"num_other_pc\"]\n",
        "X = df[feature_cols]  # Using df instead of final_df\n",
        "y_true = df[\"insider\"]  # Ground truth for evaluation\n",
        "\n",
        "# Normalize Data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train One-Class SVM with a Higher `nu`\n",
        "oc_svm = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.4)  # Increased nu from 0.2 to 0.4\n",
        "oc_svm.fit(X_scaled)\n",
        "\n",
        "# Predict Anomalies\n",
        "decision_scores = oc_svm.decision_function(X_scaled)\n",
        "\n",
        "# Set a Fixed Threshold (e.g., 0.2 instead of percentile)\n",
        "threshold = 0.2  # Change this to any value you prefer\n",
        "y_pred = [1 if score < threshold else 0 for score in decision_scores]\n",
        "\n",
        "# Calculate Metrics\n",
        "precision = precision_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "\n",
        "# Print Results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "# Debug: Print Prediction Counts\n",
        "print(\"Predicted Labels:\\n\", pd.Series(y_pred).value_counts())\n",
        "print(\"True Labels:\\n\", pd.Series(y_true).value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32bI3RB3YVoj",
        "outputId": "6b933df0-5eba-4580-b7c5-d916a9ce47ae"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.0455\n",
            "Recall: 1.0000\n",
            "F1-score: 0.0870\n",
            "Predicted Labels:\n",
            " 1    44\n",
            "Name: count, dtype: int64\n",
            "True Labels:\n",
            " insider\n",
            "0    42\n",
            "1     2\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Select Features\n",
        "feature_cols = [\"after_hours_logons\", \"num_exe_files\", \"num_usb_insertions\", \"num_other_pc\"]\n",
        "X = final_df[feature_cols]\n",
        "y_true = final_df[\"insider\"]\n",
        "\n",
        "# Normalize Data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train One-Class SVM with a Higher `nu`\n",
        "oc_svm = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.4)  # Increased from 0.2 to 0.4\n",
        "oc_svm.fit(X_scaled)\n",
        "\n",
        "# Predict Anomalies\n",
        "decision_scores = oc_svm.decision_function(X_scaled)\n",
        "\n",
        "\n",
        "# Set a Fixed Threshold (e.g., 0.2 instead of percentile)\n",
        "threshold = 0.5  # Change this to any value you prefer\n",
        "y_pred = [1 if score < threshold else 0 for score in decision_scores]\n",
        "\n",
        "# Calculate Metrics\n",
        "precision = precision_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "\n",
        "# Print Results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "# Debug: Print Prediction Counts\n",
        "print(\"Predicted Labels:\\n\", pd.Series(y_pred).value_counts())\n",
        "print(\"True Labels:\\n\", pd.Series(y_true).value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_3dG0QyAuES",
        "outputId": "7280555d-43ad-4550-e258-13b6cfb89d5b"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.0455\n",
            "Recall: 1.0000\n",
            "F1-score: 0.0870\n",
            "Predicted Labels:\n",
            " 1    44\n",
            "Name: count, dtype: int64\n",
            "True Labels:\n",
            " insider\n",
            "0    42\n",
            "1     2\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "#  Select Features for Training\n",
        "feature_cols = [\"after_hours_logons\", \"num_exe_files\", \"num_usb_insertions\", \"num_other_pc\"]\n",
        "X = final_df[feature_cols]\n",
        "y_true = final_df[\"insider\"]  # Ground truth (0 = normal, 1 = anomaly)\n",
        "\n",
        "# Normalize the Features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "#  Train One-Class SVM\n",
        "oc_svm = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.9)  # Adjust nu for anomaly proportion\n",
        "oc_svm.fit(X_scaled)\n",
        "\n",
        "# Predict Anomalies\n",
        "decision_scores = oc_svm.decision_function(X_scaled)  # Get decision function scores\n",
        "threshold = np.percentile(decision_scores, 90)  # Detect top 10% as anomalies\n",
        "y_pred = [1 if score < threshold else 0 for score in decision_scores]\n",
        "\n",
        "#  Calculate Evaluation Metrics\n",
        "precision = precision_score(y_true, y_pred, pos_label=1)\n",
        "recall = recall_score(y_true, y_pred, pos_label=1)\n",
        "f1 = f1_score(y_true, y_pred, pos_label=1)\n",
        "\n",
        "#  Print Results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7GceFB1AKZM",
        "outputId": "e812b2bf-ffb7-4860-ff2d-72b0fb7c2162"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.1333\n",
            "Recall: 1.0000\n",
            "F1-score: 0.2353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define fixed threshold values for anomaly detection\n",
        "thresholds = [-0.2, -0.1, 0, 0.1, 0.2]  # Try different fixed thresholds\n",
        "\n",
        "# Loop through each threshold and compute precision, recall, F1-score\n",
        "for threshold in thresholds:\n",
        "    y_pred = np.where(decision_scores < threshold, 1, 0)  # Label as \"1\" if below threshold (anomaly)\n",
        "\n",
        "    precision = precision_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "\n",
        "    print(f\"Threshold = {threshold:.4f}: Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkSZXtN0fVtb",
        "outputId": "3270caf1-3b9a-40d9-af8e-c2b4835485a1"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold = -0.2000: Precision = 0.1333, Recall = 1.0000, F1 Score = 0.2353\n",
            "Threshold = -0.1000: Precision = 0.1333, Recall = 1.0000, F1 Score = 0.2353\n",
            "Threshold = 0.0000: Precision = 0.0455, Recall = 1.0000, F1 Score = 0.0870\n",
            "Threshold = 0.1000: Precision = 0.0455, Recall = 1.0000, F1 Score = 0.0870\n",
            "Threshold = 0.2000: Precision = 0.0455, Recall = 1.0000, F1 Score = 0.0870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df[final_df[\"num_usb_insertions\"]!=0].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "CzC-f6luwa_q",
        "outputId": "df0765d3-0ada-48ad-e927-2e6aba54b5dc"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       user     week  after_hours_logons  num_exe_files  num_usb_insertions  \\\n",
              "40  MIB0203  2010-41                   7              0                 1.0   \n",
              "41  MIB0203  2010-42                   7              0                 1.0   \n",
              "\n",
              "    num_other_pc  insider  \n",
              "40             0        1  \n",
              "41             1        1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-64801caf-3d89-4d53-a51c-beb1bab6455b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>week</th>\n",
              "      <th>after_hours_logons</th>\n",
              "      <th>num_exe_files</th>\n",
              "      <th>num_usb_insertions</th>\n",
              "      <th>num_other_pc</th>\n",
              "      <th>insider</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>MIB0203</td>\n",
              "      <td>2010-41</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>MIB0203</td>\n",
              "      <td>2010-42</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-64801caf-3d89-4d53-a51c-beb1bab6455b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-64801caf-3d89-4d53-a51c-beb1bab6455b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-64801caf-3d89-4d53-a51c-beb1bab6455b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-da3f8dd1-86a9-4804-bf6a-7dbfc7fbbc44\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-da3f8dd1-86a9-4804-bf6a-7dbfc7fbbc44')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-da3f8dd1-86a9-4804-bf6a-7dbfc7fbbc44 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"final_df[final_df[\\\"num_usb_insertions\\\"]!=0]\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"user\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"MIB0203\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"week\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2010-42\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"after_hours_logons\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 7,\n        \"max\": 7,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_exe_files\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_usb_insertions\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_other_pc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"insider\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "import csv\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from datetime import datetime, time, timedelta\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_user_usb_data(user_id, dataset_path):\n",
        "    usb_data = []\n",
        "    with open(os.path.join(dataset_path, \"/device.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[5] == \"Connect\":  # Check user and only \"Connect\" activity\n",
        "                usb_data.append(row)\n",
        "    return usb_data\n",
        "\n",
        "def get_num_usb_insertions_per_week(user, usb_data):\n",
        "    weekly_usb_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "    for row in usb_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_usb_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_usb_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_usb_insertions\"])\n",
        "\n",
        "\n",
        "def get_user_exe_data(user_id, dataset_path):\n",
        "    exe_data = []\n",
        "    with open(os.path.join(dataset_path, \"/file.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[4].endswith(\".exe\"):  # Check user and .exe files\n",
        "                exe_data.append(row)\n",
        "    return exe_data\n",
        "\n",
        "def get_num_exe_per_week(user, exe_data):\n",
        "    weekly_exe_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in exe_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_exe_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_exe_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_exe_files\"])\n",
        "\n",
        "# %%\n",
        "def get_user_logon_data(user_id, dataset_path):\n",
        "    logon_data = []\n",
        "    with open(os.path.join(dataset_path, \"/logon.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        for row in reader:\n",
        "            if row[2] == user_id:\n",
        "                logon_data.append(row)\n",
        "    return logon_data\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_user_pc(logon_data):\n",
        "    pc_dict = {}\n",
        "    for row in logon_data:\n",
        "        pc_dict[row[3]] = 1 + pc_dict.get(row[3], 0)\n",
        "    user_pc = max(pc_dict, key=pc_dict.get)\n",
        "    return user_pc\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_num_other_PC_per_week(user, user_pc, logon_data):\n",
        "    weekly_pc_counts = defaultdict(set)  # Dictionary to store unique PCs per week\n",
        "    all_weeks = set()  # Set to track all weeks where logons occurred\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")  # Adjusted format\n",
        "        week = logon_time.strftime(\"%Y-%W\")  # Year-Week format\n",
        "        all_weeks.add(week)  # Track all weeks\n",
        "\n",
        "        if row[3] != user_pc:  # Check if PC is different from user's primary PC\n",
        "            weekly_pc_counts[week].add(\n",
        "                row[3]\n",
        "            )  # Add PC to the week's set (unique values only)\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks)\n",
        "    max_week = max(all_weeks)\n",
        "\n",
        "    # Generate all weeks between min and max\n",
        "    start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "    end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "    current_date = start_date\n",
        "    complete_weeks = set()\n",
        "\n",
        "    while current_date <= end_date:\n",
        "        week_str = current_date.strftime(\"%Y-%W\")\n",
        "        complete_weeks.add(week_str)\n",
        "        current_date += timedelta(days=7)\n",
        "\n",
        "    # Ensure every week has a count (0 if no other PCs were accessed)\n",
        "    weekly_counts = {\n",
        "        week: len(weekly_pc_counts[week]) if week in weekly_pc_counts else 0\n",
        "        for week in complete_weeks\n",
        "    }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_other_pc\"])\n",
        "\n",
        "\n",
        "def get_after_hours_logons(\n",
        "    logon_data, user, business_start=time(9, 0, 0), business_end=time(17, 0, 0)\n",
        "):\n",
        "    \"\"\"\n",
        "    Aggregates after-hours logons per week for a specified user.\n",
        "\n",
        "    :param logon_data: List of logon events in the format [id, date, user, pc, activity]\n",
        "    :param user: The specific user to filter logon events for.\n",
        "    :param business_start: Datetime.time representing start of business hours.\n",
        "    :param business_end: Datetime.time representing end of business hours.\n",
        "    :return: DataFrame with ['user', 'week', 'after_hours_logons']\n",
        "    \"\"\"\n",
        "\n",
        "    after_hours_counts = defaultdict(int)\n",
        "\n",
        "    # Track all weeks for the user\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_id, timestamp, logon_user, pc, activity = row  # Unpack columns\n",
        "\n",
        "        if (\n",
        "            activity.lower() == \"logon\" and logon_user == user\n",
        "        ):  # Only process logons for the specified user\n",
        "            try:\n",
        "                logon_time = datetime.strptime(timestamp, \"%m/%d/%Y %H:%M:%S\")\n",
        "                logon_week = logon_time.strftime(\"%Y-%W\")  # Ensure same format\n",
        "\n",
        "                # Store this week to ensure it's included in results\n",
        "                all_weeks.add(logon_week)\n",
        "\n",
        "                # Extract only the time component\n",
        "                logon_hour = logon_time.time()\n",
        "\n",
        "                # Check if the logon occurred outside business hours\n",
        "                if logon_hour < business_start or logon_hour >= business_end:\n",
        "                    after_hours_counts[logon_week] += 1\n",
        "\n",
        "            except ValueError:\n",
        "                continue  # Skip invalid timestamps\n",
        "\n",
        "    # Ensure all weeks in range are included (like `get_num_other_PC_per_week`)\n",
        "    if all_weeks:\n",
        "        min_week = min(all_weeks)\n",
        "        max_week = max(all_weeks)\n",
        "\n",
        "        # Generate all weeks in range\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        # Fill in missing weeks with 0\n",
        "        after_hours_counts = {\n",
        "            week: after_hours_counts.get(week, 0) for week in complete_weeks\n",
        "        }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    result_data = [\n",
        "        (user, week, after_hours_counts[week])\n",
        "        for week in sorted(after_hours_counts.keys())\n",
        "    ]\n",
        "    after_hours_df = pd.DataFrame(\n",
        "        result_data, columns=[\"user\", \"week\", \"after_hours_logons\"]\n",
        "    )\n",
        "\n",
        "    return after_hours_df\n",
        "\n",
        "\n",
        "# %%\n",
        "def find_insider_answers_file(user, insider_root):\n",
        "    \"\"\"\n",
        "    Recursively searches for the insider CSV file for the given user in the `insider_root` directory.\n",
        "\n",
        "    :param user: The user ID (e.g., \"CWW1120\")\n",
        "    :param insider_root: The root folder containing multiple r5.2-* subfolders.\n",
        "    :return: The full path to the user's insider CSV file if found, else None.\n",
        "    \"\"\"\n",
        "    for root, _, files in os.walk(insider_root):\n",
        "        for file in files:\n",
        "            if file.startswith(f\"/content/r5.2-\") and file.endswith(\n",
        "                f\"-{user}.csv\"\n",
        "            ):  # Match user file format\n",
        "                return os.path.join(root, file)  # Return full file path if found\n",
        "    return None  # Return None if no file is found\n",
        "\n",
        "\n",
        "def extract_weeks_from_csv(file_path):\n",
        "    \"\"\"\n",
        "    Reads a CSV file using `csv.reader` and extracts unique weeks from the timestamps (3rd column).\n",
        "\n",
        "    :param file_path: Path to the insider CSV file.\n",
        "    :return: A set of detected `Year-Week` values.\n",
        "    \"\"\"\n",
        "    insider_weeks = set()\n",
        "\n",
        "    try:\n",
        "        with open(file_path, mode=\"r\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "            reader = csv.reader(file)\n",
        "            for row in reader:\n",
        "                if len(row) < 3:  # Ensure the timestamp column exists\n",
        "                    continue\n",
        "                try:\n",
        "                    logon_time = datetime.strptime(\n",
        "                        row[2], \"%m/%d/%Y %H:%M:%S\"\n",
        "                    )  # Parse timestamp\n",
        "                    week = logon_time.strftime(\"%Y-%W\")  # Convert to Year-Week format\n",
        "                    insider_weeks.add(week)\n",
        "                except ValueError:\n",
        "                    continue  # Skip rows with invalid timestamps\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "    return insider_weeks\n",
        "\n",
        "\n",
        "def label_insider_weeks(df, user, insider_root):\n",
        "    \"\"\"\n",
        "    Adds an 'insider' column to the DataFrame by checking if the user's week exists in their insider file.\n",
        "\n",
        "    :param df: DataFrame containing ['user', 'week', 'num_other_pc']\n",
        "    :param user: The user ID for whom the dataframe is filtered.\n",
        "    :param insider_root: Path to the folder containing multiple r5.2-* subfolders.\n",
        "    :return: DataFrame with an 'insider' column.\n",
        "    \"\"\"\n",
        "\n",
        "    # Locate the user's insider file\n",
        "    insider_file = find_insider_answers_file(user, insider_root)\n",
        "\n",
        "    # If no insider file exists for the user, mark all weeks as 0 (not insider)\n",
        "    if not insider_file:\n",
        "        df[\"insider\"] = 0\n",
        "        return df\n",
        "\n",
        "    # Extract weeks from the insider CSV file\n",
        "    insider_weeks = extract_weeks_from_csv(insider_file)\n",
        "\n",
        "    # Label insider weeks in the user's dataframe\n",
        "    df[\"insider\"] = df[\"week\"].apply(lambda w: 1 if w in insider_weeks else 0)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def combine_user_feature_data(user, dataset_path, insider_root):\n",
        "    # Get data from different feature functions\n",
        "    logon_data = get_user_logon_data(user, dataset_path)\n",
        "    user_pc = get_user_pc(logon_data)\n",
        "    num_other_pc = get_num_other_PC_per_week(user, user_pc, logon_data)\n",
        "    after_hours_logons = get_after_hours_logons(logon_data, user)\n",
        "\n",
        "    exe_data = get_user_exe_data(user, dataset_path)\n",
        "    num_exe_files = get_num_exe_per_week(user, exe_data)\n",
        "\n",
        "    usb_data = get_user_usb_data(user, dataset_path)\n",
        "    num_usb = get_num_usb_insertions_per_week(user, usb_data)\n",
        "\n",
        "    # Extract relevant columns\n",
        "    after_hours_df = after_hours_logons[[\"week\", \"after_hours_logons\"]]\n",
        "    exe_df         = num_exe_files[[\"week\", \"num_exe_files\"]]\n",
        "    usb_df         = num_usb[[\"week\", \"num_usb_insertions\"]]\n",
        "    other_pc_df    = num_other_pc[[\"week\", \"num_other_pc\"]]\n",
        "\n",
        "    # Merge all dataframes on \"week\" using an outer join\n",
        "    merged_df = after_hours_df.merge(exe_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(usb_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(other_pc_df, on=\"week\", how=\"outer\")\n",
        "\n",
        "    # Replace NaN with 0 in all feature columns\n",
        "    merged_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Add user column\n",
        "    merged_df.insert(0, \"user\", user)\n",
        "    labeled_df = label_insider_weeks(merged_df, user, insider_root)\n",
        "    return labeled_df\n",
        "\n",
        "# Example usage\n",
        "dataset_path = os.path.join(\"Insider threat dataset\", \"r5.2\")\n",
        "user = \"ALT1465\"\n",
        "file_path = \"/ALT1465.csv\"  # Change to your actual file path\n",
        "final_df = pd.read_csv(file_path)\n",
        "# final_df = combine_user_feature_data(user, data\n",
        "print(final_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUs-cGSR2yTb",
        "outputId": "2d285d8a-a563-4cce-ecf4-a40f81548eef"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       user     week  after_hours_logons  num_exe_files  num_usb_insertions  \\\n",
            "0   ALT1465  2010-01                   0              0                 0.0   \n",
            "1   ALT1465  2010-02                   4              0                 0.0   \n",
            "2   ALT1465  2010-03                   4              0                 0.0   \n",
            "3   ALT1465  2010-04                   4              0                 0.0   \n",
            "4   ALT1465  2010-05                   1              0                 0.0   \n",
            "5   ALT1465  2010-06                   0              0                 0.0   \n",
            "6   ALT1465  2010-07                   5              0                 0.0   \n",
            "7   ALT1465  2010-08                   2              0                 0.0   \n",
            "8   ALT1465  2010-09                   2              0                 0.0   \n",
            "9   ALT1465  2010-10                   5              0                 0.0   \n",
            "10  ALT1465  2010-11                   2              0                 0.0   \n",
            "11  ALT1465  2010-12                   3              0                 0.0   \n",
            "12  ALT1465  2010-13                   4              0                 0.0   \n",
            "13  ALT1465  2010-14                   1              0                 0.0   \n",
            "14  ALT1465  2010-15                   4              0                 0.0   \n",
            "15  ALT1465  2010-16                   4              0                 0.0   \n",
            "16  ALT1465  2010-17                   5              0                 0.0   \n",
            "17  ALT1465  2010-18                   0              0                 0.0   \n",
            "18  ALT1465  2010-19                   3              0                 0.0   \n",
            "19  ALT1465  2010-20                   3              0                 0.0   \n",
            "20  ALT1465  2010-21                   1              0                 0.0   \n",
            "21  ALT1465  2010-22                   3              0                 0.0   \n",
            "22  ALT1465  2010-23                   3              0                 0.0   \n",
            "23  ALT1465  2010-24                   1              0                 0.0   \n",
            "24  ALT1465  2010-25                   3              0                 0.0   \n",
            "25  ALT1465  2010-26                   3              0                 0.0   \n",
            "26  ALT1465  2010-27                   2              0                 0.0   \n",
            "27  ALT1465  2010-28                   3              0                 0.0   \n",
            "28  ALT1465  2010-29                   1              0                 0.0   \n",
            "29  ALT1465  2010-30                   2              0                 0.0   \n",
            "30  ALT1465  2010-31                   1              0                 0.0   \n",
            "31  ALT1465  2010-32                   3              0                 3.0   \n",
            "32  ALT1465  2010-33                   6              0                 3.0   \n",
            "33  ALT1465  2010-34                   2              0                 3.0   \n",
            "34  ALT1465  2010-35                   2              0                 0.0   \n",
            "35  ALT1465  2010-36                   1              0                 0.0   \n",
            "\n",
            "    num_other_pc  insider  \n",
            "0              2        0  \n",
            "1              2        0  \n",
            "2              2        0  \n",
            "3              2        0  \n",
            "4              0        0  \n",
            "5              1        0  \n",
            "6              2        0  \n",
            "7              1        0  \n",
            "8              1        0  \n",
            "9              2        0  \n",
            "10             2        0  \n",
            "11             2        0  \n",
            "12             1        0  \n",
            "13             1        0  \n",
            "14             2        0  \n",
            "15             1        0  \n",
            "16             2        0  \n",
            "17             0        0  \n",
            "18             1        0  \n",
            "19             2        0  \n",
            "20             2        0  \n",
            "21             2        0  \n",
            "22             2        0  \n",
            "23             1        0  \n",
            "24             1        0  \n",
            "25             1        0  \n",
            "26             0        0  \n",
            "27             1        0  \n",
            "28             1        0  \n",
            "29             1        0  \n",
            "30             1        0  \n",
            "31             1        1  \n",
            "32             2        1  \n",
            "33             1        0  \n",
            "34             0        0  \n",
            "35             1        0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load your dataset (assuming final_df is already loaded from the uploaded data)\n",
        "# If final_df is missing, make sure you load it using the proper dataset\n",
        "df = pd.read_csv('/ALT1465.csv')  # Make sure the path is correct for your environment\n",
        "\n",
        "# Select Features\n",
        "feature_cols = [\"after_hours_logons\", \"num_exe_files\", \"num_usb_insertions\", \"num_other_pc\"]\n",
        "X = df[feature_cols]  # Using df instead of final_df\n",
        "y_true = df[\"insider\"]  # Ground truth for evaluation\n",
        "\n",
        "# Normalize Data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train One-Class SVM with a Higher `nu`\n",
        "oc_svm = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.4)  # Increased nu from 0.2 to 0.4\n",
        "oc_svm.fit(X_scaled)\n",
        "\n",
        "# Predict Anomalies\n",
        "decision_scores = oc_svm.decision_function(X_scaled)\n",
        "\n",
        "# Set a Fixed Threshold (e.g., 0.2 instead of percentile)\n",
        "threshold = 0.2  # Change this to any value you prefer\n",
        "y_pred = [1 if score < threshold else 0 for score in decision_scores]\n",
        "\n",
        "# Calculate Metrics\n",
        "precision = precision_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "\n",
        "# Print Results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "# Debug: Print Prediction Counts\n",
        "print(\"Predicted Labels:\\n\", pd.Series(y_pred).value_counts())\n",
        "print(\"True Labels:\\n\", pd.Series(y_true).value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv6iZIHB_M0K",
        "outputId": "70b0c83c-e9e4-42ce-ba2b-0f5dac98ae5e"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.1000\n",
            "Recall: 1.0000\n",
            "F1-score: 0.1818\n",
            "Predicted Labels:\n",
            " 1    20\n",
            "0    16\n",
            "Name: count, dtype: int64\n",
            "True Labels:\n",
            " insider\n",
            "0    34\n",
            "1     2\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load your dataset (assuming final_df is already loaded from the uploaded data)\n",
        "# If final_df is missing, make sure you load it using the proper dataset\n",
        "df = pd.read_csv('/ALT1465.csv')  # Make sure the path is correct for your environment\n",
        "\n",
        "# Select Features\n",
        "feature_cols = [\"after_hours_logons\", \"num_exe_files\", \"num_usb_insertions\", \"num_other_pc\"]\n",
        "X = df[feature_cols]  # Using df instead of final_df\n",
        "y_true = df[\"insider\"]  # Ground truth for evaluation\n",
        "\n",
        "# Normalize Data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train One-Class SVM with a Higher `nu`\n",
        "oc_svm = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.4)  # Increased nu from 0.2 to 0.4\n",
        "oc_svm.fit(X_scaled)\n",
        "\n",
        "# Predict Anomalies\n",
        "decision_scores = oc_svm.decision_function(X_scaled)\n",
        "\n",
        "# Set a Fixed Threshold\n",
        "threshold = 0.6  # Change this to any value you prefer\n",
        "y_pred = [1 if score < threshold else 0 for score in decision_scores]\n",
        "\n",
        "# Calculate Metrics\n",
        "precision = precision_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "\n",
        "# Print Results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "# Debug: Print Prediction Counts\n",
        "print(\"Predicted Labels:\\n\", pd.Series(y_pred).value_counts())\n",
        "print(\"True Labels:\\n\", pd.Series(y_true).value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbYVq29ZEOyq",
        "outputId": "8bb98453-e502-451a-a78d-278dcbbf2349"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.0556\n",
            "Recall: 1.0000\n",
            "F1-score: 0.1053\n",
            "Predicted Labels:\n",
            " 1    36\n",
            "Name: count, dtype: int64\n",
            "True Labels:\n",
            " insider\n",
            "0    34\n",
            "1     2\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define fixed threshold values for anomaly detection\n",
        "thresholds = [-0.2, -0.1, 0, 0.1, 0.2]  # Try different fixed thresholds\n",
        "\n",
        "# Loop through each threshold and compute precision, recall, F1-score\n",
        "for threshold in thresholds:\n",
        "    y_pred = np.where(decision_scores < threshold, 1, 0)  # Label as \"1\" if below threshold (anomaly)\n",
        "\n",
        "    precision = precision_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "\n",
        "    print(f\"Threshold = {threshold:.4f}: Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QB4V-6SwFaGV",
        "outputId": "97026cab-61ec-4cce-d657-742846a8e1de"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold = -0.2000: Precision = 0.3333, Recall = 1.0000, F1 Score = 0.5000\n",
            "Threshold = -0.1000: Precision = 0.2857, Recall = 1.0000, F1 Score = 0.4444\n",
            "Threshold = 0.0000: Precision = 0.2000, Recall = 1.0000, F1 Score = 0.3333\n",
            "Threshold = 0.1000: Precision = 0.1250, Recall = 1.0000, F1 Score = 0.2222\n",
            "Threshold = 0.2000: Precision = 0.1000, Recall = 1.0000, F1 Score = 0.1818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "import csv\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from datetime import datetime, time, timedelta\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_user_usb_data(user_id, dataset_path):\n",
        "    usb_data = []\n",
        "    with open(os.path.join(dataset_path, \"/device.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[5] == \"Connect\":  # Check user and only \"Connect\" activity\n",
        "                usb_data.append(row)\n",
        "    return usb_data\n",
        "\n",
        "def get_num_usb_insertions_per_week(user, usb_data):\n",
        "    weekly_usb_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "    for row in usb_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_usb_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_usb_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_usb_insertions\"])\n",
        "\n",
        "\n",
        "def get_user_exe_data(user_id, dataset_path):\n",
        "    exe_data = []\n",
        "    with open(os.path.join(dataset_path, \"/file.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[4].endswith(\".exe\"):  # Check user and .exe files\n",
        "                exe_data.append(row)\n",
        "    return exe_data\n",
        "\n",
        "def get_num_exe_per_week(user, exe_data):\n",
        "    weekly_exe_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in exe_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_exe_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_exe_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_exe_files\"])\n",
        "\n",
        "# %%\n",
        "def get_user_logon_data(user_id, dataset_path):\n",
        "    logon_data = []\n",
        "    with open(os.path.join(dataset_path, \"/logon.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        for row in reader:\n",
        "            if row[2] == user_id:\n",
        "                logon_data.append(row)\n",
        "    return logon_data\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_user_pc(logon_data):\n",
        "    pc_dict = {}\n",
        "    for row in logon_data:\n",
        "        pc_dict[row[3]] = 1 + pc_dict.get(row[3], 0)\n",
        "    user_pc = max(pc_dict, key=pc_dict.get)\n",
        "    return user_pc\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_num_other_PC_per_week(user, user_pc, logon_data):\n",
        "    weekly_pc_counts = defaultdict(set)  # Dictionary to store unique PCs per week\n",
        "    all_weeks = set()  # Set to track all weeks where logons occurred\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")  # Adjusted format\n",
        "        week = logon_time.strftime(\"%Y-%W\")  # Year-Week format\n",
        "        all_weeks.add(week)  # Track all weeks\n",
        "\n",
        "        if row[3] != user_pc:  # Check if PC is different from user's primary PC\n",
        "            weekly_pc_counts[week].add(\n",
        "                row[3]\n",
        "            )  # Add PC to the week's set (unique values only)\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks)\n",
        "    max_week = max(all_weeks)\n",
        "\n",
        "    # Generate all weeks between min and max\n",
        "    start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "    end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "    current_date = start_date\n",
        "    complete_weeks = set()\n",
        "\n",
        "    while current_date <= end_date:\n",
        "        week_str = current_date.strftime(\"%Y-%W\")\n",
        "        complete_weeks.add(week_str)\n",
        "        current_date += timedelta(days=7)\n",
        "\n",
        "    # Ensure every week has a count (0 if no other PCs were accessed)\n",
        "    weekly_counts = {\n",
        "        week: len(weekly_pc_counts[week]) if week in weekly_pc_counts else 0\n",
        "        for week in complete_weeks\n",
        "    }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_other_pc\"])\n",
        "\n",
        "\n",
        "def get_after_hours_logons(\n",
        "    logon_data, user, business_start=time(9, 0, 0), business_end=time(17, 0, 0)\n",
        "):\n",
        "    \"\"\"\n",
        "    Aggregates after-hours logons per week for a specified user.\n",
        "\n",
        "    :param logon_data: List of logon events in the format [id, date, user, pc, activity]\n",
        "    :param user: The specific user to filter logon events for.\n",
        "    :param business_start: Datetime.time representing start of business hours.\n",
        "    :param business_end: Datetime.time representing end of business hours.\n",
        "    :return: DataFrame with ['user', 'week', 'after_hours_logons']\n",
        "    \"\"\"\n",
        "\n",
        "    after_hours_counts = defaultdict(int)\n",
        "\n",
        "    # Track all weeks for the user\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_id, timestamp, logon_user, pc, activity = row  # Unpack columns\n",
        "\n",
        "        if (\n",
        "            activity.lower() == \"logon\" and logon_user == user\n",
        "        ):  # Only process logons for the specified user\n",
        "            try:\n",
        "                logon_time = datetime.strptime(timestamp, \"%m/%d/%Y %H:%M:%S\")\n",
        "                logon_week = logon_time.strftime(\"%Y-%W\")  # Ensure same format\n",
        "\n",
        "                # Store this week to ensure it's included in results\n",
        "                all_weeks.add(logon_week)\n",
        "\n",
        "                # Extract only the time component\n",
        "                logon_hour = logon_time.time()\n",
        "\n",
        "                # Check if the logon occurred outside business hours\n",
        "                if logon_hour < business_start or logon_hour >= business_end:\n",
        "                    after_hours_counts[logon_week] += 1\n",
        "\n",
        "            except ValueError:\n",
        "                continue  # Skip invalid timestamps\n",
        "\n",
        "    # Ensure all weeks in range are included (like `get_num_other_PC_per_week`)\n",
        "    if all_weeks:\n",
        "        min_week = min(all_weeks)\n",
        "        max_week = max(all_weeks)\n",
        "\n",
        "        # Generate all weeks in range\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        # Fill in missing weeks with 0\n",
        "        after_hours_counts = {\n",
        "            week: after_hours_counts.get(week, 0) for week in complete_weeks\n",
        "        }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    result_data = [\n",
        "        (user, week, after_hours_counts[week])\n",
        "        for week in sorted(after_hours_counts.keys())\n",
        "    ]\n",
        "    after_hours_df = pd.DataFrame(\n",
        "        result_data, columns=[\"user\", \"week\", \"after_hours_logons\"]\n",
        "    )\n",
        "\n",
        "    return after_hours_df\n",
        "\n",
        "\n",
        "# %%\n",
        "def find_insider_answers_file(user, insider_root):\n",
        "    \"\"\"\n",
        "    Recursively searches for the insider CSV file for the given user in the `insider_root` directory.\n",
        "\n",
        "    :param user: The user ID (e.g., \"CWW1120\")\n",
        "    :param insider_root: The root folder containing multiple r5.2-* subfolders.\n",
        "    :return: The full path to the user's insider CSV file if found, else None.\n",
        "    \"\"\"\n",
        "    for root, _, files in os.walk(insider_root):\n",
        "        for file in files:\n",
        "            if file.startswith(f\"/content/r5.2-\") and file.endswith(\n",
        "                f\"-{user}.csv\"\n",
        "            ):  # Match user file format\n",
        "                return os.path.join(root, file)  # Return full file path if found\n",
        "    return None  # Return None if no file is found\n",
        "\n",
        "\n",
        "def extract_weeks_from_csv(file_path):\n",
        "    \"\"\"\n",
        "    Reads a CSV file using `csv.reader` and extracts unique weeks from the timestamps (3rd column).\n",
        "\n",
        "    :param file_path: Path to the insider CSV file.\n",
        "    :return: A set of detected `Year-Week` values.\n",
        "    \"\"\"\n",
        "    insider_weeks = set()\n",
        "\n",
        "    try:\n",
        "        with open(file_path, mode=\"r\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "            reader = csv.reader(file)\n",
        "            for row in reader:\n",
        "                if len(row) < 3:  # Ensure the timestamp column exists\n",
        "                    continue\n",
        "                try:\n",
        "                    logon_time = datetime.strptime(\n",
        "                        row[2], \"%m/%d/%Y %H:%M:%S\"\n",
        "                    )  # Parse timestamp\n",
        "                    week = logon_time.strftime(\"%Y-%W\")  # Convert to Year-Week format\n",
        "                    insider_weeks.add(week)\n",
        "                except ValueError:\n",
        "                    continue  # Skip rows with invalid timestamps\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "    return insider_weeks\n",
        "\n",
        "\n",
        "def label_insider_weeks(df, user, insider_root):\n",
        "    \"\"\"\n",
        "    Adds an 'insider' column to the DataFrame by checking if the user's week exists in their insider file.\n",
        "\n",
        "    :param df: DataFrame containing ['user', 'week', 'num_other_pc']\n",
        "    :param user: The user ID for whom the dataframe is filtered.\n",
        "    :param insider_root: Path to the folder containing multiple r5.2-* subfolders.\n",
        "    :return: DataFrame with an 'insider' column.\n",
        "    \"\"\"\n",
        "\n",
        "    # Locate the user's insider file\n",
        "    insider_file = find_insider_answers_file(user, insider_root)\n",
        "\n",
        "    # If no insider file exists for the user, mark all weeks as 0 (not insider)\n",
        "    if not insider_file:\n",
        "        df[\"insider\"] = 0\n",
        "        return df\n",
        "\n",
        "    # Extract weeks from the insider CSV file\n",
        "    insider_weeks = extract_weeks_from_csv(insider_file)\n",
        "\n",
        "    # Label insider weeks in the user's dataframe\n",
        "    df[\"insider\"] = df[\"week\"].apply(lambda w: 1 if w in insider_weeks else 0)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def combine_user_feature_data(user, dataset_path, insider_root):\n",
        "    # Get data from different feature functions\n",
        "    logon_data = get_user_logon_data(user, dataset_path)\n",
        "    user_pc = get_user_pc(logon_data)\n",
        "    num_other_pc = get_num_other_PC_per_week(user, user_pc, logon_data)\n",
        "    after_hours_logons = get_after_hours_logons(logon_data, user)\n",
        "\n",
        "    exe_data = get_user_exe_data(user, dataset_path)\n",
        "    num_exe_files = get_num_exe_per_week(user, exe_data)\n",
        "\n",
        "    usb_data = get_user_usb_data(user, dataset_path)\n",
        "    num_usb = get_num_usb_insertions_per_week(user, usb_data)\n",
        "\n",
        "    # Extract relevant columns\n",
        "    after_hours_df = after_hours_logons[[\"week\", \"after_hours_logons\"]]\n",
        "    exe_df         = num_exe_files[[\"week\", \"num_exe_files\"]]\n",
        "    usb_df         = num_usb[[\"week\", \"num_usb_insertions\"]]\n",
        "    other_pc_df    = num_other_pc[[\"week\", \"num_other_pc\"]]\n",
        "\n",
        "    # Merge all dataframes on \"week\" using an outer join\n",
        "    merged_df = after_hours_df.merge(exe_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(usb_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(other_pc_df, on=\"week\", how=\"outer\")\n",
        "\n",
        "    # Replace NaN with 0 in all feature columns\n",
        "    merged_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Add user column\n",
        "    merged_df.insert(0, \"user\", user)\n",
        "    labeled_df = label_insider_weeks(merged_df, user, insider_root)\n",
        "    return labeled_df\n",
        "\n",
        "# Example usage\n",
        "dataset_path = os.path.join(\"Insider threat dataset\", \"r5.2\")\n",
        "user = \"NIV1608\"\n",
        "file_path = \"/NIV1608.csv\"  # Change to your actual file path\n",
        "final_df = pd.read_csv(file_path)\n",
        "# final_df = combine_user_feature_data(user, data\n",
        "print(final_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMs4yp8OGZ7G",
        "outputId": "ac737c00-b8c5-4a2a-f142-7c882b2fa820"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       user     week  after_hours_logons  num_exe_files  num_usb_insertions  \\\n",
            "0   NIV1608  2010-01                   3              0                 0.0   \n",
            "1   NIV1608  2010-02                   2              0                 0.0   \n",
            "2   NIV1608  2010-03                   4              0                 0.0   \n",
            "3   NIV1608  2010-04                   1              0                 0.0   \n",
            "4   NIV1608  2010-05                   2              0                 0.0   \n",
            "..      ...      ...                 ...            ...                 ...   \n",
            "56  NIV1608  2011-05                   5              0                 7.0   \n",
            "57  NIV1608  2011-06                   7              0                 7.0   \n",
            "58  NIV1608  2011-07                   3              0                 4.0   \n",
            "59  NIV1608  2011-08                   1              0                 0.0   \n",
            "60  NIV1608  2011-09                   0              0                 0.0   \n",
            "\n",
            "    num_other_pc  insider  \n",
            "0              0        0  \n",
            "1              0        0  \n",
            "2              0        0  \n",
            "3              0        0  \n",
            "4              0        0  \n",
            "..           ...      ...  \n",
            "56             0        1  \n",
            "57             0        1  \n",
            "58             0        0  \n",
            "59             0        0  \n",
            "60             0        0  \n",
            "\n",
            "[61 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load your dataset (assuming final_df is already loaded from the uploaded data)\n",
        "# If final_df is missing, make sure you load it using the proper dataset\n",
        "df = pd.read_csv('/NIV1608.csv')  # Make sure the path is correct for your environment\n",
        "\n",
        "# Select Features\n",
        "feature_cols = [\"after_hours_logons\", \"num_exe_files\", \"num_usb_insertions\", \"num_other_pc\"]\n",
        "X = df[feature_cols]  # Using df instead of final_df\n",
        "y_true = df[\"insider\"]  # Ground truth for evaluation\n",
        "\n",
        "# Normalize Data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train One-Class SVM with a Higher `nu`\n",
        "oc_svm = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.4)  # Increased nu from 0.2 to 0.4\n",
        "oc_svm.fit(X_scaled)\n",
        "\n",
        "# Predict Anomalies\n",
        "decision_scores = oc_svm.decision_function(X_scaled)\n",
        "\n",
        "# Set a Fixed Threshold (e.g., 0.2 instead of percentile)\n",
        "threshold = 0.2  # Change this to any value you prefer\n",
        "y_pred = [1 if score < threshold else 0 for score in decision_scores]\n",
        "\n",
        "# Calculate Metrics\n",
        "precision = precision_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "\n",
        "# Print Results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "# Debug: Print Prediction Counts\n",
        "print(\"Predicted Labels:\\n\", pd.Series(y_pred).value_counts())\n",
        "print(\"True Labels:\\n\", pd.Series(y_true).value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGdwffCzGxUS",
        "outputId": "9db23696-2b8e-478d-ec34-3b4fb4cff4fb"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.0328\n",
            "Recall: 1.0000\n",
            "F1-score: 0.0635\n",
            "Predicted Labels:\n",
            " 1    61\n",
            "Name: count, dtype: int64\n",
            "True Labels:\n",
            " insider\n",
            "0    59\n",
            "1     2\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define fixed threshold values for anomaly detection\n",
        "thresholds = [-0.2, -0.1, 0, 0.1, 0.2]  # Try different fixed thresholds\n",
        "\n",
        "# Loop through each threshold and compute precision, recall, F1-score\n",
        "for threshold in thresholds:\n",
        "    y_pred = np.where(decision_scores < threshold, 1, 0)  # Label as \"1\" if below threshold (anomaly)\n",
        "\n",
        "    precision = precision_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "\n",
        "    print(f\"Threshold = {threshold:.4f}: Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWS7YIShG8za",
        "outputId": "b8b59f49-e188-42af-ddea-2051e7783fb3"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold = -0.2000: Precision = 0.1250, Recall = 1.0000, F1 Score = 0.2222\n",
            "Threshold = -0.1000: Precision = 0.1250, Recall = 1.0000, F1 Score = 0.2222\n",
            "Threshold = 0.0000: Precision = 0.0645, Recall = 1.0000, F1 Score = 0.1212\n",
            "Threshold = 0.1000: Precision = 0.0328, Recall = 1.0000, F1 Score = 0.0635\n",
            "Threshold = 0.2000: Precision = 0.0328, Recall = 1.0000, F1 Score = 0.0635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "import csv\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from datetime import datetime, time, timedelta\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_user_usb_data(user_id, dataset_path):\n",
        "    usb_data = []\n",
        "    with open(os.path.join(dataset_path, \"/device.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[5] == \"Connect\":  # Check user and only \"Connect\" activity\n",
        "                usb_data.append(row)\n",
        "    return usb_data\n",
        "\n",
        "def get_num_usb_insertions_per_week(user, usb_data):\n",
        "    weekly_usb_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "    for row in usb_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_usb_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_usb_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_usb_insertions\"])\n",
        "\n",
        "\n",
        "def get_user_exe_data(user_id, dataset_path):\n",
        "    exe_data = []\n",
        "    with open(os.path.join(dataset_path, \"/file.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[4].endswith(\".exe\"):  # Check user and .exe files\n",
        "                exe_data.append(row)\n",
        "    return exe_data\n",
        "\n",
        "def get_num_exe_per_week(user, exe_data):\n",
        "    weekly_exe_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in exe_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_exe_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_exe_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_exe_files\"])\n",
        "\n",
        "# %%\n",
        "def get_user_logon_data(user_id, dataset_path):\n",
        "    logon_data = []\n",
        "    with open(os.path.join(dataset_path, \"/logon.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        for row in reader:\n",
        "            if row[2] == user_id:\n",
        "                logon_data.append(row)\n",
        "    return logon_data\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_user_pc(logon_data):\n",
        "    pc_dict = {}\n",
        "    for row in logon_data:\n",
        "        pc_dict[row[3]] = 1 + pc_dict.get(row[3], 0)\n",
        "    user_pc = max(pc_dict, key=pc_dict.get)\n",
        "    return user_pc\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_num_other_PC_per_week(user, user_pc, logon_data):\n",
        "    weekly_pc_counts = defaultdict(set)  # Dictionary to store unique PCs per week\n",
        "    all_weeks = set()  # Set to track all weeks where logons occurred\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")  # Adjusted format\n",
        "        week = logon_time.strftime(\"%Y-%W\")  # Year-Week format\n",
        "        all_weeks.add(week)  # Track all weeks\n",
        "\n",
        "        if row[3] != user_pc:  # Check if PC is different from user's primary PC\n",
        "            weekly_pc_counts[week].add(\n",
        "                row[3]\n",
        "            )  # Add PC to the week's set (unique values only)\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks)\n",
        "    max_week = max(all_weeks)\n",
        "\n",
        "    # Generate all weeks between min and max\n",
        "    start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "    end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "    current_date = start_date\n",
        "    complete_weeks = set()\n",
        "\n",
        "    while current_date <= end_date:\n",
        "        week_str = current_date.strftime(\"%Y-%W\")\n",
        "        complete_weeks.add(week_str)\n",
        "        current_date += timedelta(days=7)\n",
        "\n",
        "    # Ensure every week has a count (0 if no other PCs were accessed)\n",
        "    weekly_counts = {\n",
        "        week: len(weekly_pc_counts[week]) if week in weekly_pc_counts else 0\n",
        "        for week in complete_weeks\n",
        "    }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_other_pc\"])\n",
        "\n",
        "\n",
        "def get_after_hours_logons(\n",
        "    logon_data, user, business_start=time(9, 0, 0), business_end=time(17, 0, 0)\n",
        "):\n",
        "    \"\"\"\n",
        "    Aggregates after-hours logons per week for a specified user.\n",
        "\n",
        "    :param logon_data: List of logon events in the format [id, date, user, pc, activity]\n",
        "    :param user: The specific user to filter logon events for.\n",
        "    :param business_start: Datetime.time representing start of business hours.\n",
        "    :param business_end: Datetime.time representing end of business hours.\n",
        "    :return: DataFrame with ['user', 'week', 'after_hours_logons']\n",
        "    \"\"\"\n",
        "\n",
        "    after_hours_counts = defaultdict(int)\n",
        "\n",
        "    # Track all weeks for the user\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_id, timestamp, logon_user, pc, activity = row  # Unpack columns\n",
        "\n",
        "        if (\n",
        "            activity.lower() == \"logon\" and logon_user == user\n",
        "        ):  # Only process logons for the specified user\n",
        "            try:\n",
        "                logon_time = datetime.strptime(timestamp, \"%m/%d/%Y %H:%M:%S\")\n",
        "                logon_week = logon_time.strftime(\"%Y-%W\")  # Ensure same format\n",
        "\n",
        "                # Store this week to ensure it's included in results\n",
        "                all_weeks.add(logon_week)\n",
        "\n",
        "                # Extract only the time component\n",
        "                logon_hour = logon_time.time()\n",
        "\n",
        "                # Check if the logon occurred outside business hours\n",
        "                if logon_hour < business_start or logon_hour >= business_end:\n",
        "                    after_hours_counts[logon_week] += 1\n",
        "\n",
        "            except ValueError:\n",
        "                continue  # Skip invalid timestamps\n",
        "\n",
        "    # Ensure all weeks in range are included (like `get_num_other_PC_per_week`)\n",
        "    if all_weeks:\n",
        "        min_week = min(all_weeks)\n",
        "        max_week = max(all_weeks)\n",
        "\n",
        "        # Generate all weeks in range\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        # Fill in missing weeks with 0\n",
        "        after_hours_counts = {\n",
        "            week: after_hours_counts.get(week, 0) for week in complete_weeks\n",
        "        }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    result_data = [\n",
        "        (user, week, after_hours_counts[week])\n",
        "        for week in sorted(after_hours_counts.keys())\n",
        "    ]\n",
        "    after_hours_df = pd.DataFrame(\n",
        "        result_data, columns=[\"user\", \"week\", \"after_hours_logons\"]\n",
        "    )\n",
        "\n",
        "    return after_hours_df\n",
        "\n",
        "\n",
        "# %%\n",
        "def find_insider_answers_file(user, insider_root):\n",
        "    \"\"\"\n",
        "    Recursively searches for the insider CSV file for the given user in the `insider_root` directory.\n",
        "\n",
        "    :param user: The user ID (e.g., \"CWW1120\")\n",
        "    :param insider_root: The root folder containing multiple r5.2-* subfolders.\n",
        "    :return: The full path to the user's insider CSV file if found, else None.\n",
        "    \"\"\"\n",
        "    for root, _, files in os.walk(insider_root):\n",
        "        for file in files:\n",
        "            if file.startswith(f\"/content/r5.2-\") and file.endswith(\n",
        "                f\"-{user}.csv\"\n",
        "            ):  # Match user file format\n",
        "                return os.path.join(root, file)  # Return full file path if found\n",
        "    return None  # Return None if no file is found\n",
        "\n",
        "\n",
        "def extract_weeks_from_csv(file_path):\n",
        "    \"\"\"\n",
        "    Reads a CSV file using `csv.reader` and extracts unique weeks from the timestamps (3rd column).\n",
        "\n",
        "    :param file_path: Path to the insider CSV file.\n",
        "    :return: A set of detected `Year-Week` values.\n",
        "    \"\"\"\n",
        "    insider_weeks = set()\n",
        "\n",
        "    try:\n",
        "        with open(file_path, mode=\"r\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "            reader = csv.reader(file)\n",
        "            for row in reader:\n",
        "                if len(row) < 3:  # Ensure the timestamp column exists\n",
        "                    continue\n",
        "                try:\n",
        "                    logon_time = datetime.strptime(\n",
        "                        row[2], \"%m/%d/%Y %H:%M:%S\"\n",
        "                    )  # Parse timestamp\n",
        "                    week = logon_time.strftime(\"%Y-%W\")  # Convert to Year-Week format\n",
        "                    insider_weeks.add(week)\n",
        "                except ValueError:\n",
        "                    continue  # Skip rows with invalid timestamps\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "    return insider_weeks\n",
        "\n",
        "\n",
        "def label_insider_weeks(df, user, insider_root):\n",
        "    \"\"\"\n",
        "    Adds an 'insider' column to the DataFrame by checking if the user's week exists in their insider file.\n",
        "\n",
        "    :param df: DataFrame containing ['user', 'week', 'num_other_pc']\n",
        "    :param user: The user ID for whom the dataframe is filtered.\n",
        "    :param insider_root: Path to the folder containing multiple r5.2-* subfolders.\n",
        "    :return: DataFrame with an 'insider' column.\n",
        "    \"\"\"\n",
        "\n",
        "    # Locate the user's insider file\n",
        "    insider_file = find_insider_answers_file(user, insider_root)\n",
        "\n",
        "    # If no insider file exists for the user, mark all weeks as 0 (not insider)\n",
        "    if not insider_file:\n",
        "        df[\"insider\"] = 0\n",
        "        return df\n",
        "\n",
        "    # Extract weeks from the insider CSV file\n",
        "    insider_weeks = extract_weeks_from_csv(insider_file)\n",
        "\n",
        "    # Label insider weeks in the user's dataframe\n",
        "    df[\"insider\"] = df[\"week\"].apply(lambda w: 1 if w in insider_weeks else 0)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def combine_user_feature_data(user, dataset_path, insider_root):\n",
        "    # Get data from different feature functions\n",
        "    logon_data = get_user_logon_data(user, dataset_path)\n",
        "    user_pc = get_user_pc(logon_data)\n",
        "    num_other_pc = get_num_other_PC_per_week(user, user_pc, logon_data)\n",
        "    after_hours_logons = get_after_hours_logons(logon_data, user)\n",
        "\n",
        "    exe_data = get_user_exe_data(user, dataset_path)\n",
        "    num_exe_files = get_num_exe_per_week(user, exe_data)\n",
        "\n",
        "    usb_data = get_user_usb_data(user, dataset_path)\n",
        "    num_usb = get_num_usb_insertions_per_week(user, usb_data)\n",
        "\n",
        "    # Extract relevant columns\n",
        "    after_hours_df = after_hours_logons[[\"week\", \"after_hours_logons\"]]\n",
        "    exe_df         = num_exe_files[[\"week\", \"num_exe_files\"]]\n",
        "    usb_df         = num_usb[[\"week\", \"num_usb_insertions\"]]\n",
        "    other_pc_df    = num_other_pc[[\"week\", \"num_other_pc\"]]\n",
        "\n",
        "    # Merge all dataframes on \"week\" using an outer join\n",
        "    merged_df = after_hours_df.merge(exe_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(usb_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(other_pc_df, on=\"week\", how=\"outer\")\n",
        "\n",
        "    # Replace NaN with 0 in all feature columns\n",
        "    merged_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Add user column\n",
        "    merged_df.insert(0, \"user\", user)\n",
        "    labeled_df = label_insider_weeks(merged_df, user, insider_root)\n",
        "    return labeled_df\n",
        "\n",
        "# Example usage\n",
        "dataset_path = os.path.join(\"Insider threat dataset\", \"r5.2\")\n",
        "user = \"HMS1658\"\n",
        "file_path = \"/HMS1658.csv\"  # Change to your actual file path\n",
        "df = pd.read_csv(file_path, on_bad_lines='skip')  # This will skip lines with errors\n",
        "\n",
        "# final_df = combine_user_feature_data(user, data\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8pTKwpmMPE5",
        "outputId": "7aa58a5e-cf34-4aec-f113-3cf4e8cd2a04"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           user     week  after_hours_logons  num_exe_files  \\\n",
            "0   HMS1658.csv  2010-01                   0              0   \n",
            "1   HMS1658.csv  2010-02                   4              0   \n",
            "2   HMS1658.csv  2010-03                   4              0   \n",
            "3   HMS1658.csv  2010-04                   4              0   \n",
            "4   HMS1658.csv  2010-05                   1              0   \n",
            "5   HMS1658.csv  2010-06                   0              0   \n",
            "6   HMS1658.csv  2010-07                   5              0   \n",
            "7   HMS1658.csv  2010-08                   2              0   \n",
            "8   HMS1658.csv  2010-09                   2              0   \n",
            "9   HMS1658.csv  2010-10                   5              0   \n",
            "10  HMS1658.csv  2010-11                   2              0   \n",
            "11  HMS1658.csv  2010-12                   3              0   \n",
            "12  HMS1658.csv  2010-13                   4              0   \n",
            "13  HMS1658.csv  2010-14                   1              0   \n",
            "14  HMS1658.csv  2010-15                   4              0   \n",
            "15  HMS1658.csv  2010-16                   4              0   \n",
            "\n",
            "    num_usb_insertions  num_other_pc  insider  \n",
            "0                  0.0             2        0  \n",
            "1                  0.0             2        0  \n",
            "2                  0.0             2        0  \n",
            "3                  0.0             2        0  \n",
            "4                  0.0             0        0  \n",
            "5                  0.0             1        0  \n",
            "6                  0.0             2        0  \n",
            "7                  0.0             1        0  \n",
            "8                  0.0             1        0  \n",
            "9                  0.0             2        0  \n",
            "10                 0.0             2        0  \n",
            "11                 0.0             2        0  \n",
            "12                 0.0             1        0  \n",
            "13                 0.0             1        0  \n",
            "14                 0.0             2        0  \n",
            "15                 0.0             1        0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset again if necessary\n",
        "file_path = \"/HMS1658.csv\"  # Adjust to your file path if needed\n",
        "df = pd.read_csv(file_path, on_bad_lines='skip')\n",
        "\n",
        "# Check if the dataset is loaded correctly\n",
        "print(\"Dataset Loaded:\\n\", df.head())\n",
        "\n",
        "# Select Features\n",
        "feature_cols = [\"after_hours_logons\", \"num_exe_files\", \"num_usb_insertions\", \"num_other_pc\"]\n",
        "X = df[feature_cols]  # Using df instead of final_df\n",
        "y_true = df[\"insider\"]  # Ground truth for evaluation\n",
        "\n",
        "# Normalize Data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train One-Class SVM with a Higher `nu`\n",
        "oc_svm = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.4)  # Change gamma to 'scale'\n",
        "oc_svm.fit(X_scaled)\n",
        "\n",
        "# Predict Anomalies\n",
        "decision_scores = oc_svm.decision_function(X_scaled)\n",
        "\n",
        "# Check decision scores\n",
        "print(\"Decision Scores:\\n\", decision_scores[:10])\n",
        "\n",
        "# Set a Fixed Threshold\n",
        "threshold = 0.01  # Change this to any value you prefer\n",
        "y_pred = [1 if score < threshold else 0 for score in decision_scores]\n",
        "\n",
        "# Check Predictions\n",
        "print(\"Predictions:\\n\", y_pred[:10])\n",
        "\n",
        "# Calculate Metrics\n",
        "precision = precision_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "\n",
        "# Print Results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JNWcinKS2-R",
        "outputId": "994bf6c8-7689-43cd-84ea-61e721a6e49e"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded:\n",
            "           user     week  after_hours_logons  num_exe_files  \\\n",
            "0  HMS1658.csv  2010-01                   0              0   \n",
            "1  HMS1658.csv  2010-02                   4              0   \n",
            "2  HMS1658.csv  2010-03                   4              0   \n",
            "3  HMS1658.csv  2010-04                   4              0   \n",
            "4  HMS1658.csv  2010-05                   1              0   \n",
            "\n",
            "   num_usb_insertions  num_other_pc  insider  \n",
            "0                 0.0             2        0  \n",
            "1                 0.0             2        0  \n",
            "2                 0.0             2        0  \n",
            "3                 0.0             2        0  \n",
            "4                 0.0             0        0  \n",
            "Decision Scores:\n",
            " [-1.03034908e-01  9.44578350e-02  9.44578350e-02  9.44578350e-02\n",
            " -4.47549744e-01 -6.66545136e-02  5.14413227e-05 -3.39440310e-04\n",
            " -3.39440310e-04  5.14413227e-05]\n",
            "Predictions:\n",
            " [1, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
            "Precision: 0.0000\n",
            "Recall: 1.0000\n",
            "F1-score: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset again if necessary\n",
        "file_path = \"/HMS1658.csv\"  # Adjust to your file path if needed\n",
        "df = pd.read_csv(file_path, on_bad_lines='skip')\n",
        "\n",
        "# Check if the dataset is loaded correctly\n",
        "print(\"Dataset Loaded:\\n\", df.head())\n",
        "\n",
        "# Select Features\n",
        "feature_cols = [\"after_hours_logons\", \"num_exe_files\", \"num_usb_insertions\", \"num_other_pc\"]\n",
        "X = df[feature_cols]  # Using df instead of final_df\n",
        "y_true = df[\"insider\"]  # Ground truth for evaluation\n",
        "\n",
        "# Introduce anomalies for testing (optional, if you don't already have anomalies)\n",
        "# For testing purposes, you can modify a few values in the 'insider' column\n",
        "df.loc[0, 'insider'] = 1  # Set the first entry as an anomaly\n",
        "df.loc[5, 'insider'] = 1  # Set the sixth entry as an anomaly\n",
        "y_true = df[\"insider\"]\n",
        "\n",
        "# Normalize Data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train One-Class SVM with a Higher `nu`\n",
        "oc_svm = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.4)  # Adjust nu and gamma\n",
        "oc_svm.fit(X_scaled)\n",
        "\n",
        "# Predict Anomalies\n",
        "decision_scores = oc_svm.decision_function(X_scaled)\n",
        "\n",
        "# Check decision scores\n",
        "print(\"Decision Scores:\\n\", decision_scores[:10])\n",
        "\n",
        "# Set a Fixed Threshold (Adjust the threshold to see different results)\n",
        "threshold = 0.2  # You can change this value to something lower to capture anomalies\n",
        "y_pred = [1 if score < threshold else 0 for score in decision_scores]\n",
        "\n",
        "# Check Predictions\n",
        "print(\"Predictions:\\n\", y_pred[:10])\n",
        "\n",
        "# Calculate Metrics\n",
        "precision = precision_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "\n",
        "# Print Results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCRNMtUKUa-b",
        "outputId": "bf94da5c-b51c-4ceb-af6e-b724cac2df50"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded:\n",
            "           user     week  after_hours_logons  num_exe_files  \\\n",
            "0  HMS1658.csv  2010-01                   0              0   \n",
            "1  HMS1658.csv  2010-02                   4              0   \n",
            "2  HMS1658.csv  2010-03                   4              0   \n",
            "3  HMS1658.csv  2010-04                   4              0   \n",
            "4  HMS1658.csv  2010-05                   1              0   \n",
            "\n",
            "   num_usb_insertions  num_other_pc  insider  \n",
            "0                 0.0             2        0  \n",
            "1                 0.0             2        0  \n",
            "2                 0.0             2        0  \n",
            "3                 0.0             2        0  \n",
            "4                 0.0             0        0  \n",
            "Decision Scores:\n",
            " [-1.03034908e-01  9.44578350e-02  9.44578350e-02  9.44578350e-02\n",
            " -4.47549744e-01 -6.66545136e-02  5.14413227e-05 -3.39440310e-04\n",
            " -3.39440310e-04  5.14413227e-05]\n",
            "Predictions:\n",
            " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Precision: 0.1250\n",
            "Recall: 1.0000\n",
            "F1-score: 0.2222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define fixed threshold values for anomaly detection\n",
        "thresholds = [-0.2, -0.1, 0, 0.1, 0.2]  # Try different fixed thresholds\n",
        "\n",
        "# Loop through each threshold and compute precision, recall, F1-score\n",
        "for threshold in thresholds:\n",
        "    y_pred = np.where(decision_scores < threshold, 1, 0)  # Label as \"1\" if below threshold (anomaly)\n",
        "\n",
        "    precision = precision_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "\n",
        "    print(f\"Threshold = {threshold:.4f}: Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C_tTEugO_DD",
        "outputId": "6ca7c211-49a5-4199-bd1b-611eee0252ff"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold = -0.2000: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
            "Threshold = -0.1000: Precision = 0.5000, Recall = 0.5000, F1 Score = 0.5000\n",
            "Threshold = 0.0000: Precision = 0.4000, Recall = 1.0000, F1 Score = 0.5714\n",
            "Threshold = 0.1000: Precision = 0.1250, Recall = 1.0000, F1 Score = 0.2222\n",
            "Threshold = 0.2000: Precision = 0.1250, Recall = 1.0000, F1 Score = 0.2222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-create the DataFrame with the provided data after system reset\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Data for the CSV file\n",
        "data = {\n",
        "    \"user\": [\"CGF1056\"] * 16,\n",
        "    \"week\": [\n",
        "        \"2010-01\", \"2010-02\", \"2010-03\", \"2010-04\", \"2010-05\", \"2010-06\", \"2010-07\", \"2010-08\", \"2010-09\", \"2010-10\",\n",
        "        \"2010-11\", \"2010-12\", \"2010-13\", \"2010-14\", \"2010-15\", \"2010-16\"\n",
        "    ],\n",
        "    \"after_hours_logons\": [0, 4, 4, 4, 1, 0, 5, 2, 2, 5, 2, 3, 4, 1, 4, 4],\n",
        "    \"num_exe_files\": [0] * 16,\n",
        "    \"num_usb_insertions\": [0.0] * 16,\n",
        "    \"num_other_pc\": [2, 2, 2, 2, 0, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1],\n",
        "    \"insider\": [0] * 16\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "df_new = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "csv_file_path = '/r5.2-4-CGF1056.csv'\n",
        "df_new.to_csv(csv_file_path, index=False)\n",
        "\n",
        "csv_file_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Jy_Y-QYga5jz",
        "outputId": "663c75e4-8df3-454f-83e0-e37b3e927824"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/r5.2-4-CGF1056.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "import csv\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from datetime import datetime, time, timedelta\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_user_usb_data(user_id, dataset_path):\n",
        "    usb_data = []\n",
        "    with open(os.path.join(dataset_path, \"/device.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[5] == \"Connect\":  # Check user and only \"Connect\" activity\n",
        "                usb_data.append(row)\n",
        "    return usb_data\n",
        "\n",
        "def get_num_usb_insertions_per_week(user, usb_data):\n",
        "    weekly_usb_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "    for row in usb_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_usb_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_usb_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_usb_insertions\"])\n",
        "\n",
        "\n",
        "def get_user_exe_data(user_id, dataset_path):\n",
        "    exe_data = []\n",
        "    with open(os.path.join(dataset_path, \"/file.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[4].endswith(\".exe\"):  # Check user and .exe files\n",
        "                exe_data.append(row)\n",
        "    return exe_data\n",
        "\n",
        "def get_num_exe_per_week(user, exe_data):\n",
        "    weekly_exe_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in exe_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_exe_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_exe_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_exe_files\"])\n",
        "\n",
        "# %%\n",
        "def get_user_logon_data(user_id, dataset_path):\n",
        "    logon_data = []\n",
        "    with open(os.path.join(dataset_path, \"/logon.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        for row in reader:\n",
        "            if row[2] == user_id:\n",
        "                logon_data.append(row)\n",
        "    return logon_data\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_user_pc(logon_data):\n",
        "    pc_dict = {}\n",
        "    for row in logon_data:\n",
        "        pc_dict[row[3]] = 1 + pc_dict.get(row[3], 0)\n",
        "    user_pc = max(pc_dict, key=pc_dict.get)\n",
        "    return user_pc\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_num_other_PC_per_week(user, user_pc, logon_data):\n",
        "    weekly_pc_counts = defaultdict(set)  # Dictionary to store unique PCs per week\n",
        "    all_weeks = set()  # Set to track all weeks where logons occurred\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")  # Adjusted format\n",
        "        week = logon_time.strftime(\"%Y-%W\")  # Year-Week format\n",
        "        all_weeks.add(week)  # Track all weeks\n",
        "\n",
        "        if row[3] != user_pc:  # Check if PC is different from user's primary PC\n",
        "            weekly_pc_counts[week].add(\n",
        "                row[3]\n",
        "            )  # Add PC to the week's set (unique values only)\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks)\n",
        "    max_week = max(all_weeks)\n",
        "\n",
        "    # Generate all weeks between min and max\n",
        "    start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "    end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "    current_date = start_date\n",
        "    complete_weeks = set()\n",
        "\n",
        "    while current_date <= end_date:\n",
        "        week_str = current_date.strftime(\"%Y-%W\")\n",
        "        complete_weeks.add(week_str)\n",
        "        current_date += timedelta(days=7)\n",
        "\n",
        "    # Ensure every week has a count (0 if no other PCs were accessed)\n",
        "    weekly_counts = {\n",
        "        week: len(weekly_pc_counts[week]) if week in weekly_pc_counts else 0\n",
        "        for week in complete_weeks\n",
        "    }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_other_pc\"])\n",
        "\n",
        "\n",
        "def get_after_hours_logons(\n",
        "    logon_data, user, business_start=time(9, 0, 0), business_end=time(17, 0, 0)\n",
        "):\n",
        "    \"\"\"\n",
        "    Aggregates after-hours logons per week for a specified user.\n",
        "\n",
        "    :param logon_data: List of logon events in the format [id, date, user, pc, activity]\n",
        "    :param user: The specific user to filter logon events for.\n",
        "    :param business_start: Datetime.time representing start of business hours.\n",
        "    :param business_end: Datetime.time representing end of business hours.\n",
        "    :return: DataFrame with ['user', 'week', 'after_hours_logons']\n",
        "    \"\"\"\n",
        "\n",
        "    after_hours_counts = defaultdict(int)\n",
        "\n",
        "    # Track all weeks for the user\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_id, timestamp, logon_user, pc, activity = row  # Unpack columns\n",
        "\n",
        "        if (\n",
        "            activity.lower() == \"logon\" and logon_user == user\n",
        "        ):  # Only process logons for the specified user\n",
        "            try:\n",
        "                logon_time = datetime.strptime(timestamp, \"%m/%d/%Y %H:%M:%S\")\n",
        "                logon_week = logon_time.strftime(\"%Y-%W\")  # Ensure same format\n",
        "\n",
        "                # Store this week to ensure it's included in results\n",
        "                all_weeks.add(logon_week)\n",
        "\n",
        "                # Extract only the time component\n",
        "                logon_hour = logon_time.time()\n",
        "\n",
        "                # Check if the logon occurred outside business hours\n",
        "                if logon_hour < business_start or logon_hour >= business_end:\n",
        "                    after_hours_counts[logon_week] += 1\n",
        "\n",
        "            except ValueError:\n",
        "                continue  # Skip invalid timestamps\n",
        "\n",
        "    # Ensure all weeks in range are included (like `get_num_other_PC_per_week`)\n",
        "    if all_weeks:\n",
        "        min_week = min(all_weeks)\n",
        "        max_week = max(all_weeks)\n",
        "\n",
        "        # Generate all weeks in range\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        # Fill in missing weeks with 0\n",
        "        after_hours_counts = {\n",
        "            week: after_hours_counts.get(week, 0) for week in complete_weeks\n",
        "        }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    result_data = [\n",
        "        (user, week, after_hours_counts[week])\n",
        "        for week in sorted(after_hours_counts.keys())\n",
        "    ]\n",
        "    after_hours_df = pd.DataFrame(\n",
        "        result_data, columns=[\"user\", \"week\", \"after_hours_logons\"]\n",
        "    )\n",
        "\n",
        "    return after_hours_df\n",
        "\n",
        "\n",
        "# %%\n",
        "def find_insider_answers_file(user, insider_root):\n",
        "    \"\"\"\n",
        "    Recursively searches for the insider CSV file for the given user in the `insider_root` directory.\n",
        "\n",
        "    :param user: The user ID (e.g., \"CWW1120\")\n",
        "    :param insider_root: The root folder containing multiple r5.2-* subfolders.\n",
        "    :return: The full path to the user's insider CSV file if found, else None.\n",
        "    \"\"\"\n",
        "    for root, _, files in os.walk(insider_root):\n",
        "        for file in files:\n",
        "            if file.startswith(f\"/content/r5.2-\") and file.endswith(\n",
        "                f\"-{user}.csv\"\n",
        "            ):  # Match user file format\n",
        "                return os.path.join(root, file)  # Return full file path if found\n",
        "    return None  # Return None if no file is found\n",
        "\n",
        "\n",
        "def extract_weeks_from_csv(file_path):\n",
        "    \"\"\"\n",
        "    Reads a CSV file using `csv.reader` and extracts unique weeks from the timestamps (3rd column).\n",
        "\n",
        "    :param file_path: Path to the insider CSV file.\n",
        "    :return: A set of detected `Year-Week` values.\n",
        "    \"\"\"\n",
        "    insider_weeks = set()\n",
        "\n",
        "    try:\n",
        "        with open(file_path, mode=\"r\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "            reader = csv.reader(file)\n",
        "            for row in reader:\n",
        "                if len(row) < 3:  # Ensure the timestamp column exists\n",
        "                    continue\n",
        "                try:\n",
        "                    logon_time = datetime.strptime(\n",
        "                        row[2], \"%m/%d/%Y %H:%M:%S\"\n",
        "                    )  # Parse timestamp\n",
        "                    week = logon_time.strftime(\"%Y-%W\")  # Convert to Year-Week format\n",
        "                    insider_weeks.add(week)\n",
        "                except ValueError:\n",
        "                    continue  # Skip rows with invalid timestamps\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "    return insider_weeks\n",
        "\n",
        "\n",
        "def label_insider_weeks(df, user, insider_root):\n",
        "    \"\"\"\n",
        "    Adds an 'insider' column to the DataFrame by checking if the user's week exists in their insider file.\n",
        "\n",
        "    :param df: DataFrame containing ['user', 'week', 'num_other_pc']\n",
        "    :param user: The user ID for whom the dataframe is filtered.\n",
        "    :param insider_root: Path to the folder containing multiple r5.2-* subfolders.\n",
        "    :return: DataFrame with an 'insider' column.\n",
        "    \"\"\"\n",
        "\n",
        "    # Locate the user's insider file\n",
        "    insider_file = find_insider_answers_file(user, insider_root)\n",
        "\n",
        "    # If no insider file exists for the user, mark all weeks as 0 (not insider)\n",
        "    if not insider_file:\n",
        "        df[\"insider\"] = 0\n",
        "        return df\n",
        "\n",
        "    # Extract weeks from the insider CSV file\n",
        "    insider_weeks = extract_weeks_from_csv(insider_file)\n",
        "\n",
        "    # Label insider weeks in the user's dataframe\n",
        "    df[\"insider\"] = df[\"week\"].apply(lambda w: 1 if w in insider_weeks else 0)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def combine_user_feature_data(user, dataset_path, insider_root):\n",
        "    # Get data from different feature functions\n",
        "    logon_data = get_user_logon_data(user, dataset_path)\n",
        "    user_pc = get_user_pc(logon_data)\n",
        "    num_other_pc = get_num_other_PC_per_week(user, user_pc, logon_data)\n",
        "    after_hours_logons = get_after_hours_logons(logon_data, user)\n",
        "\n",
        "    exe_data = get_user_exe_data(user, dataset_path)\n",
        "    num_exe_files = get_num_exe_per_week(user, exe_data)\n",
        "\n",
        "    usb_data = get_user_usb_data(user, dataset_path)\n",
        "    num_usb = get_num_usb_insertions_per_week(user, usb_data)\n",
        "\n",
        "    # Extract relevant columns\n",
        "    after_hours_df = after_hours_logons[[\"week\", \"after_hours_logons\"]]\n",
        "    exe_df         = num_exe_files[[\"week\", \"num_exe_files\"]]\n",
        "    usb_df         = num_usb[[\"week\", \"num_usb_insertions\"]]\n",
        "    other_pc_df    = num_other_pc[[\"week\", \"num_other_pc\"]]\n",
        "\n",
        "    # Merge all dataframes on \"week\" using an outer join\n",
        "    merged_df = after_hours_df.merge(exe_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(usb_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(other_pc_df, on=\"week\", how=\"outer\")\n",
        "\n",
        "    # Replace NaN with 0 in all feature columns\n",
        "    merged_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Add user column\n",
        "    merged_df.insert(0, \"user\", user)\n",
        "    labeled_df = label_insider_weeks(merged_df, user, insider_root)\n",
        "    return labeled_df\n",
        "\n",
        "# Example usage\n",
        "dataset_path = os.path.join(\"Insider threat dataset\", \"r5.2\")\n",
        "user = \"CGF1056\"\n",
        "file_path = \"/r5.2-4-CGF1056.csv\"  # Change to your actual file path\n",
        "df = pd.read_csv(file_path, on_bad_lines='skip')  # This will skip lines with errors\n",
        "\n",
        "# final_df = combine_user_feature_data(user, data\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_W_506zbGCg",
        "outputId": "deceb216-3cbd-4607-94e7-c85eeffd9868"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       user     week  after_hours_logons  num_exe_files  num_usb_insertions  \\\n",
            "0   CGF1056  2010-01                   0              0                 0.0   \n",
            "1   CGF1056  2010-02                   4              0                 0.0   \n",
            "2   CGF1056  2010-03                   4              0                 0.0   \n",
            "3   CGF1056  2010-04                   4              0                 0.0   \n",
            "4   CGF1056  2010-05                   1              0                 0.0   \n",
            "5   CGF1056  2010-06                   0              0                 0.0   \n",
            "6   CGF1056  2010-07                   5              0                 0.0   \n",
            "7   CGF1056  2010-08                   2              0                 0.0   \n",
            "8   CGF1056  2010-09                   2              0                 0.0   \n",
            "9   CGF1056  2010-10                   5              0                 0.0   \n",
            "10  CGF1056  2010-11                   2              0                 0.0   \n",
            "11  CGF1056  2010-12                   3              0                 0.0   \n",
            "12  CGF1056  2010-13                   4              0                 0.0   \n",
            "13  CGF1056  2010-14                   1              0                 0.0   \n",
            "14  CGF1056  2010-15                   4              0                 0.0   \n",
            "15  CGF1056  2010-16                   4              0                 0.0   \n",
            "\n",
            "    num_other_pc  insider  \n",
            "0              2        0  \n",
            "1              2        0  \n",
            "2              2        0  \n",
            "3              2        0  \n",
            "4              0        0  \n",
            "5              1        0  \n",
            "6              2        0  \n",
            "7              1        0  \n",
            "8              1        0  \n",
            "9              2        0  \n",
            "10             2        0  \n",
            "11             2        0  \n",
            "12             1        0  \n",
            "13             1        0  \n",
            "14             2        0  \n",
            "15             1        0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset again if necessary\n",
        "file_path = \"/r5.2-4-CGF1056.csv\"  # Adjust to your file path if needed\n",
        "df = pd.read_csv(file_path, on_bad_lines='skip')\n",
        "\n",
        "# Check if the dataset is loaded correctly\n",
        "print(\"Dataset Loaded:\\n\", df.head())\n",
        "\n",
        "# Select Features\n",
        "feature_cols = [\"after_hours_logons\", \"num_exe_files\", \"num_usb_insertions\", \"num_other_pc\"]\n",
        "X = df[feature_cols]  # Using df instead of final_df\n",
        "y_true = df[\"insider\"]  # Ground truth for evaluation\n",
        "\n",
        "# Introduce anomalies for testing (optional, if you don't already have anomalies)\n",
        "# For testing purposes, you can modify a few values in the 'insider' column\n",
        "df.loc[0, 'insider'] = 1  # Set the first entry as an anomaly\n",
        "df.loc[5, 'insider'] = 1  # Set the sixth entry as an anomaly\n",
        "y_true = df[\"insider\"]\n",
        "df.loc[0, 'insider'] = 1  # Set the first entry as an anomaly\n",
        "df.loc[5, 'insider'] = 1  # Set the sixth entry as an anomaly\n",
        "# Normalize Data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train One-Class SVM with a Higher `nu`\n",
        "oc_svm = OneClassSVM(kernel=\"rbf\", gamma=\"auto\", nu=0.05)  # Adjust nu and gamma\n",
        "oc_svm.fit(X_scaled)\n",
        "\n",
        "# Predict Anomalies\n",
        "decision_scores = oc_svm.decision_function(X_scaled)\n",
        "\n",
        "# Check decision scores\n",
        "print(\"Decision Scores:\\n\", decision_scores[:10])\n",
        "\n",
        "# Set a Fixed Threshold (Adjust the threshold to see different results)\n",
        "threshold = 0.1  # You can change this value to something lower to capture anomalies\n",
        "y_pred = [1 if score < threshold else 0 for score in decision_scores]\n",
        "\n",
        "# Check Predictions\n",
        "print(\"Predictions:\\n\", y_pred[:10])\n",
        "\n",
        "# Calculate Metrics\n",
        "precision = precision_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "\n",
        "# Print Results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APoHselQbVwB",
        "outputId": "f5c4c24e-4850-42cd-9f7f-4f27c508a300"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded:\n",
            "       user     week  after_hours_logons  num_exe_files  num_usb_insertions  \\\n",
            "0  CGF1056  2010-01                   0              0                 0.0   \n",
            "1  CGF1056  2010-02                   4              0                 0.0   \n",
            "2  CGF1056  2010-03                   4              0                 0.0   \n",
            "3  CGF1056  2010-04                   4              0                 0.0   \n",
            "4  CGF1056  2010-05                   1              0                 0.0   \n",
            "\n",
            "   num_other_pc  insider  \n",
            "0             2        0  \n",
            "1             2        0  \n",
            "2             2        0  \n",
            "3             2        0  \n",
            "4             0        0  \n",
            "Decision Scores:\n",
            " [-7.65529258e-05  1.93726596e-02  1.93726596e-02  1.93726596e-02\n",
            "  1.15565618e-04  1.15565532e-04 -1.00646649e-04  3.93789874e-02\n",
            "  3.93789874e-02 -1.00646649e-04]\n",
            "Predictions:\n",
            " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Precision: 0.1250\n",
            "Recall: 1.0000\n",
            "F1-score: 0.2222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define fixed threshold values for anomaly detection\n",
        "thresholds = [-3.0, -2.0, -1.5, -1.0, -0.5]  # Try different fixed thresholds\n",
        "\n",
        "# Loop through each threshold and compute precision, recall, F1-score\n",
        "for threshold in thresholds:\n",
        "    y_pred = np.where(decision_scores < threshold, 1, 0)  # Label as \"1\" if below threshold (anomaly)\n",
        "\n",
        "    precision = precision_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "\n",
        "    print(f\"Threshold = {threshold:.4f}: Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAWdAGtkbqqz",
        "outputId": "0770c00d-7302-4ce0-e183-a71e5dfc9802"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold = -3.0000: Precision = 1.0000, Recall = 0.0000, F1 Score = 0.0000\n",
            "Threshold = -2.0000: Precision = 1.0000, Recall = 0.0000, F1 Score = 0.0000\n",
            "Threshold = -1.5000: Precision = 1.0000, Recall = 0.0000, F1 Score = 0.0000\n",
            "Threshold = -1.0000: Precision = 1.0000, Recall = 0.0000, F1 Score = 0.0000\n",
            "Threshold = -0.5000: Precision = 1.0000, Recall = 0.0000, F1 Score = 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "\n",
        "# Define the dataset paths\n",
        "device_file_path = \"/device.csv\"\n",
        "logon_file_path = \"/logon.csv\"\n",
        "files_file_path = \"/file.csv\"\n",
        "\n",
        "user_id = \"ATO0307\"  # The user you're interested in\n",
        "\n",
        "# 1. Load Data\n",
        "device_data = pd.read_csv(device_file_path)\n",
        "logon_data = pd.read_csv(logon_file_path)\n",
        "files_data = pd.read_csv(files_file_path)\n",
        "\n",
        "# 2. Filter Data for the User\n",
        "device_data_user = device_data[device_data['user'] == user]\n",
        "logon_data_user = logon_data[logon_data['user'] == user]\n",
        "files_data_user = files_data[files_data['user'] == user]\n",
        "\n",
        "# 3. Process USB Insertions (from device.csv)\n",
        "def get_num_usb_insertions_per_week(user_data):\n",
        "    usb_counts = defaultdict(int)\n",
        "    for row in user_data.itertuples():\n",
        "        # Assuming timestamp is in 'datetime' column and format is \"%m/%d/%Y %H:%M:%S\"\n",
        "        timestamp = datetime.strptime(row.timestamp, \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = timestamp.strftime(\"%Y-%W\")\n",
        "        usb_counts[week] += 1\n",
        "    return usb_counts\n",
        "\n",
        "usb_insertions = get_num_usb_insertions_per_week(device_data_user)\n",
        "def get_after_hours_logons_per_week(user_data, business_start=\"09:00\", business_end=\"17:00\"):\n",
        "    logon_counts = defaultdict(int)\n",
        "    business_start_time = datetime.strptime(business_start, \"%H:%M\").time()\n",
        "    business_end_time = datetime.strptime(business_end, \"%H:%M\").time()\n",
        "\n",
        "    for row in user_data.itertuples(index=False): # Set index=False to get row values as a tuple\n",
        "        timestamp = datetime.strptime(row.date, \"%m/%d/%Y %H:%M:%S\") # Access the date column\n",
        "        logon_week = timestamp.strftime(\"%Y-%W\")\n",
        "        logon_time = timestamp.time()\n",
        "\n",
        "        # Check if the logon occurred outside business hours\n",
        "        if logon_time < business_start_time or logon_time >= business_end_time:\n",
        "            logon_counts[logon_week] += 1\n",
        "    return logon_counts\n",
        "\n",
        "def get_num_usb_insertions_per_week(user_data):\n",
        "    usb_counts = defaultdict(int)\n",
        "    for row in user_data.itertuples(index=False): # Set index=False to get row values as a tuple\n",
        "        # Assuming timestamp is in 'datetime' column and format is \"%m/%d/%Y %H:%M:%S\"\n",
        "        timestamp = datetime.strptime(row.date, \"%m/%d/%Y %H:%M:%S\") # Access the date column\n",
        "        week = timestamp.strftime(\"%Y-%W\")\n",
        "        usb_counts[week] += 1\n",
        "    return usb_counts\n",
        "\n",
        "def get_num_exe_files_per_week(user_data):\n",
        "    exe_counts = defaultdict(int)\n",
        "    for row in user_data.itertuples(index=False): # Set index=False to get row values as a tuple\n",
        "        timestamp = datetime.strptime(row.date, \"%m/%d/%Y %H:%M:%S\") # Access the date column\n",
        "        logon_week = timestamp.strftime(\"%Y-%W\")\n",
        "\n",
        "        # Check if the file is a .exe file\n",
        "        if row.file_name.endswith(\".exe\"):\n",
        "            exe_counts[logon_week] += 1\n",
        "    return exe_counts\n"
      ],
      "metadata": {
        "id": "BSu6EogCdxlD"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-create the DataFrame with the provided data after system reset\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Data for the CSV file\n",
        "data = {\n",
        "    \"user\": [\"ATO0307\"] * 16,\n",
        "    \"week\": [\n",
        "        \"2010-01\", \"2010-02\", \"2010-03\", \"2010-04\", \"2010-05\", \"2010-06\", \"2010-07\", \"2010-08\", \"2010-09\", \"2010-10\",\n",
        "        \"2010-11\", \"2010-12\", \"2010-13\", \"2010-14\", \"2010-15\", \"2010-16\"\n",
        "    ],\n",
        "    \"after_hours_logons\": [0, 4, 4, 4, 1, 0, 5, 2, 2, 5, 2, 3, 4, 1, 4, 4],\n",
        "    \"num_exe_files\": [0] * 16,\n",
        "    \"num_usb_insertions\": [0.0] * 16,\n",
        "    \"num_other_pc\": [2, 2, 2, 2, 0, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1],\n",
        "    \"insider\": [0] * 16\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "df_new = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "csv_file_path = '/ATO0307.csv'\n",
        "df_new.to_csv(csv_file_path, index=False)\n",
        "\n",
        "csv_file_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "GqFXeOwviymE",
        "outputId": "23637326-fe49-4567-8e2a-7812879b4497"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/ATO0307.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "import csv\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from datetime import datetime, time, timedelta\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_user_usb_data(user_id, dataset_path):\n",
        "    usb_data = []\n",
        "    with open(os.path.join(dataset_path, \"/device.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[5] == \"Connect\":  # Check user and only \"Connect\" activity\n",
        "                usb_data.append(row)\n",
        "    return usb_data\n",
        "\n",
        "def get_num_usb_insertions_per_week(user, usb_data):\n",
        "    weekly_usb_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "    for row in usb_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_usb_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_usb_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_usb_insertions\"])\n",
        "\n",
        "\n",
        "def get_user_exe_data(user_id, dataset_path):\n",
        "    exe_data = []\n",
        "    with open(os.path.join(dataset_path, \"/file.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[4].endswith(\".exe\"):  # Check user and .exe files\n",
        "                exe_data.append(row)\n",
        "    return exe_data\n",
        "\n",
        "def get_num_exe_per_week(user, exe_data):\n",
        "    weekly_exe_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in exe_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_exe_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_exe_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_exe_files\"])\n",
        "\n",
        "# %%\n",
        "def get_user_logon_data(user_id, dataset_path):\n",
        "    logon_data = []\n",
        "    with open(os.path.join(dataset_path, \"/logon.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        for row in reader:\n",
        "            if row[2] == user_id:\n",
        "                logon_data.append(row)\n",
        "    return logon_data\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_user_pc(logon_data):\n",
        "    pc_dict = {}\n",
        "    for row in logon_data:\n",
        "        pc_dict[row[3]] = 1 + pc_dict.get(row[3], 0)\n",
        "    user_pc = max(pc_dict, key=pc_dict.get)\n",
        "    return user_pc\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_num_other_PC_per_week(user, user_pc, logon_data):\n",
        "    weekly_pc_counts = defaultdict(set)  # Dictionary to store unique PCs per week\n",
        "    all_weeks = set()  # Set to track all weeks where logons occurred\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")  # Adjusted format\n",
        "        week = logon_time.strftime(\"%Y-%W\")  # Year-Week format\n",
        "        all_weeks.add(week)  # Track all weeks\n",
        "\n",
        "        if row[3] != user_pc:  # Check if PC is different from user's primary PC\n",
        "            weekly_pc_counts[week].add(\n",
        "                row[3]\n",
        "            )  # Add PC to the week's set (unique values only)\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks)\n",
        "    max_week = max(all_weeks)\n",
        "\n",
        "    # Generate all weeks between min and max\n",
        "    start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "    end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "    current_date = start_date\n",
        "    complete_weeks = set()\n",
        "\n",
        "    while current_date <= end_date:\n",
        "        week_str = current_date.strftime(\"%Y-%W\")\n",
        "        complete_weeks.add(week_str)\n",
        "        current_date += timedelta(days=7)\n",
        "\n",
        "    # Ensure every week has a count (0 if no other PCs were accessed)\n",
        "    weekly_counts = {\n",
        "        week: len(weekly_pc_counts[week]) if week in weekly_pc_counts else 0\n",
        "        for week in complete_weeks\n",
        "    }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_other_pc\"])\n",
        "\n",
        "\n",
        "def get_after_hours_logons(\n",
        "    logon_data, user, business_start=time(9, 0, 0), business_end=time(17, 0, 0)\n",
        "):\n",
        "    \"\"\"\n",
        "    Aggregates after-hours logons per week for a specified user.\n",
        "\n",
        "    :param logon_data: List of logon events in the format [id, date, user, pc, activity]\n",
        "    :param user: The specific user to filter logon events for.\n",
        "    :param business_start: Datetime.time representing start of business hours.\n",
        "    :param business_end: Datetime.time representing end of business hours.\n",
        "    :return: DataFrame with ['user', 'week', 'after_hours_logons']\n",
        "    \"\"\"\n",
        "\n",
        "    after_hours_counts = defaultdict(int)\n",
        "\n",
        "    # Track all weeks for the user\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_id, timestamp, logon_user, pc, activity = row  # Unpack columns\n",
        "\n",
        "        if (\n",
        "            activity.lower() == \"logon\" and logon_user == user\n",
        "        ):  # Only process logons for the specified user\n",
        "            try:\n",
        "                logon_time = datetime.strptime(timestamp, \"%m/%d/%Y %H:%M:%S\")\n",
        "                logon_week = logon_time.strftime(\"%Y-%W\")  # Ensure same format\n",
        "\n",
        "                # Store this week to ensure it's included in results\n",
        "                all_weeks.add(logon_week)\n",
        "\n",
        "                # Extract only the time component\n",
        "                logon_hour = logon_time.time()\n",
        "\n",
        "                # Check if the logon occurred outside business hours\n",
        "                if logon_hour < business_start or logon_hour >= business_end:\n",
        "                    after_hours_counts[logon_week] += 1\n",
        "\n",
        "            except ValueError:\n",
        "                continue  # Skip invalid timestamps\n",
        "\n",
        "    # Ensure all weeks in range are included (like `get_num_other_PC_per_week`)\n",
        "    if all_weeks:\n",
        "        min_week = min(all_weeks)\n",
        "        max_week = max(all_weeks)\n",
        "\n",
        "        # Generate all weeks in range\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        # Fill in missing weeks with 0\n",
        "        after_hours_counts = {\n",
        "            week: after_hours_counts.get(week, 0) for week in complete_weeks\n",
        "        }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    result_data = [\n",
        "        (user, week, after_hours_counts[week])\n",
        "        for week in sorted(after_hours_counts.keys())\n",
        "    ]\n",
        "    after_hours_df = pd.DataFrame(\n",
        "        result_data, columns=[\"user\", \"week\", \"after_hours_logons\"]\n",
        "    )\n",
        "\n",
        "    return after_hours_df\n",
        "\n",
        "\n",
        "# %%\n",
        "def find_insider_answers_file(user, insider_root):\n",
        "    \"\"\"\n",
        "    Recursively searches for the insider CSV file for the given user in the `insider_root` directory.\n",
        "\n",
        "    :param user: The user ID (e.g., \"CWW1120\")\n",
        "    :param insider_root: The root folder containing multiple r5.2-* subfolders.\n",
        "    :return: The full path to the user's insider CSV file if found, else None.\n",
        "    \"\"\"\n",
        "    for root, _, files in os.walk(insider_root):\n",
        "        for file in files:\n",
        "            if file.startswith(f\"/content/r5.2-\") and file.endswith(\n",
        "                f\"-{user}.csv\"\n",
        "            ):  # Match user file format\n",
        "                return os.path.join(root, file)  # Return full file path if found\n",
        "    return None  # Return None if no file is found\n",
        "\n",
        "\n",
        "def extract_weeks_from_csv(file_path):\n",
        "    \"\"\"\n",
        "    Reads a CSV file using `csv.reader` and extracts unique weeks from the timestamps (3rd column).\n",
        "\n",
        "    :param file_path: Path to the insider CSV file.\n",
        "    :return: A set of detected `Year-Week` values.\n",
        "    \"\"\"\n",
        "    insider_weeks = set()\n",
        "\n",
        "    try:\n",
        "        with open(file_path, mode=\"r\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "            reader = csv.reader(file)\n",
        "            for row in reader:\n",
        "                if len(row) < 3:  # Ensure the timestamp column exists\n",
        "                    continue\n",
        "                try:\n",
        "                    logon_time = datetime.strptime(\n",
        "                        row[2], \"%m/%d/%Y %H:%M:%S\"\n",
        "                    )  # Parse timestamp\n",
        "                    week = logon_time.strftime(\"%Y-%W\")  # Convert to Year-Week format\n",
        "                    insider_weeks.add(week)\n",
        "                except ValueError:\n",
        "                    continue  # Skip rows with invalid timestamps\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "    return insider_weeks\n",
        "\n",
        "\n",
        "def label_insider_weeks(df, user, insider_root):\n",
        "    \"\"\"\n",
        "    Adds an 'insider' column to the DataFrame by checking if the user's week exists in their insider file.\n",
        "\n",
        "    :param df: DataFrame containing ['user', 'week', 'num_other_pc']\n",
        "    :param user: The user ID for whom the dataframe is filtered.\n",
        "    :param insider_root: Path to the folder containing multiple r5.2-* subfolders.\n",
        "    :return: DataFrame with an 'insider' column.\n",
        "    \"\"\"\n",
        "\n",
        "    # Locate the user's insider file\n",
        "    insider_file = find_insider_answers_file(user, insider_root)\n",
        "\n",
        "    # If no insider file exists for the user, mark all weeks as 0 (not insider)\n",
        "    if not insider_file:\n",
        "        df[\"insider\"] = 0\n",
        "        return df\n",
        "\n",
        "    # Extract weeks from the insider CSV file\n",
        "    insider_weeks = extract_weeks_from_csv(insider_file)\n",
        "\n",
        "    # Label insider weeks in the user's dataframe\n",
        "    df[\"insider\"] = df[\"week\"].apply(lambda w: 1 if w in insider_weeks else 0)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def combine_user_feature_data(user, dataset_path, insider_root):\n",
        "    # Get data from different feature functions\n",
        "    logon_data = get_user_logon_data(user, dataset_path)\n",
        "    user_pc = get_user_pc(logon_data)\n",
        "    num_other_pc = get_num_other_PC_per_week(user, user_pc, logon_data)\n",
        "    after_hours_logons = get_after_hours_logons(logon_data, user)\n",
        "\n",
        "    exe_data = get_user_exe_data(user, dataset_path)\n",
        "    num_exe_files = get_num_exe_per_week(user, exe_data)\n",
        "\n",
        "    usb_data = get_user_usb_data(user, dataset_path)\n",
        "    num_usb = get_num_usb_insertions_per_week(user, usb_data)\n",
        "\n",
        "    # Extract relevant columns\n",
        "    after_hours_df = after_hours_logons[[\"week\", \"after_hours_logons\"]]\n",
        "    exe_df         = num_exe_files[[\"week\", \"num_exe_files\"]]\n",
        "    usb_df         = num_usb[[\"week\", \"num_usb_insertions\"]]\n",
        "    other_pc_df    = num_other_pc[[\"week\", \"num_other_pc\"]]\n",
        "\n",
        "    # Merge all dataframes on \"week\" using an outer join\n",
        "    merged_df = after_hours_df.merge(exe_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(usb_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(other_pc_df, on=\"week\", how=\"outer\")\n",
        "\n",
        "    # Replace NaN with 0 in all feature columns\n",
        "    merged_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Add user column\n",
        "    merged_df.insert(0, \"user\", user)\n",
        "    labeled_df = label_insider_weeks(merged_df, user, insider_root)\n",
        "    return labeled_df\n",
        "\n",
        "# Example usage\n",
        "dataset_path = os.path.join(\"Insider threat dataset\", \"r5.2\")\n",
        "user = \"ATO0307\"\n",
        "file_path = \"/ATO0307.csv\"  # Change to your actual file path\n",
        "df = pd.read_csv(file_path, on_bad_lines='skip')  # This will skip lines with errors\n",
        "\n",
        "# final_df = combine_user_feature_data(user, data\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5jBN0lSjBxa",
        "outputId": "7e290af0-b116-49d6-ec05-0ba34befe2f7"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       user     week  after_hours_logons  num_exe_files  num_usb_insertions  \\\n",
            "0   ATO0307  2010-01                   0              0                 0.0   \n",
            "1   ATO0307  2010-02                   4              0                 0.0   \n",
            "2   ATO0307  2010-03                   4              0                 0.0   \n",
            "3   ATO0307  2010-04                   4              0                 0.0   \n",
            "4   ATO0307  2010-05                   1              0                 0.0   \n",
            "5   ATO0307  2010-06                   0              0                 0.0   \n",
            "6   ATO0307  2010-07                   5              0                 0.0   \n",
            "7   ATO0307  2010-08                   2              0                 0.0   \n",
            "8   ATO0307  2010-09                   2              0                 0.0   \n",
            "9   ATO0307  2010-10                   5              0                 0.0   \n",
            "10  ATO0307  2010-11                   2              0                 0.0   \n",
            "11  ATO0307  2010-12                   3              0                 0.0   \n",
            "12  ATO0307  2010-13                   4              0                 0.0   \n",
            "13  ATO0307  2010-14                   1              0                 0.0   \n",
            "14  ATO0307  2010-15                   4              0                 0.0   \n",
            "15  ATO0307  2010-16                   4              0                 0.0   \n",
            "\n",
            "    num_other_pc  insider  \n",
            "0              2        0  \n",
            "1              2        0  \n",
            "2              2        0  \n",
            "3              2        0  \n",
            "4              0        0  \n",
            "5              1        0  \n",
            "6              2        0  \n",
            "7              1        0  \n",
            "8              1        0  \n",
            "9              2        0  \n",
            "10             2        0  \n",
            "11             2        0  \n",
            "12             1        0  \n",
            "13             1        0  \n",
            "14             2        0  \n",
            "15             1        0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset again if necessary\n",
        "file_path = \"/ATO0307.csv\"  # Adjust to your file path if needed\n",
        "df = pd.read_csv(file_path, on_bad_lines='skip')\n",
        "\n",
        "# Check if the dataset is loaded correctly\n",
        "print(\"Dataset Loaded:\\n\", df.head())\n",
        "\n",
        "# Select Features\n",
        "feature_cols = [\"after_hours_logons\", \"num_exe_files\", \"num_usb_insertions\", \"num_other_pc\"]\n",
        "X = df[feature_cols]  # Using df instead of final_df\n",
        "y_true = df[\"insider\"]  # Ground truth for evaluation\n",
        "\n",
        "# Introduce anomalies for testing (optional, if you don't already have anomalies)\n",
        "# For testing purposes, you can modify a few values in the 'insider' column\n",
        "df.loc[0, 'insider'] = 1  # Set the first entry as an anomaly\n",
        "df.loc[5, 'insider'] = 1  # Set the sixth entry as an anomaly\n",
        "y_true = df[\"insider\"]\n",
        "\n",
        "# Normalize Data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train One-Class SVM with a Higher `nu`\n",
        "oc_svm = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.4)  # Adjust nu and gamma\n",
        "oc_svm.fit(X_scaled)\n",
        "\n",
        "# Predict Anomalies\n",
        "decision_scores = oc_svm.decision_function(X_scaled)\n",
        "\n",
        "# Check decision scores\n",
        "print(\"Decision Scores:\\n\", decision_scores[:10])\n",
        "\n",
        "# Set a Fixed Threshold (Adjust the threshold to see different results)\n",
        "threshold = 0 # You can change this value to something lower to capture anomalies\n",
        "y_pred = [1 if score < threshold else 0 for score in decision_scores]\n",
        "\n",
        "# Check Predictions\n",
        "print(\"Predictions:\\n\", y_pred[:10])\n",
        "\n",
        "# Calculate Metrics\n",
        "precision = precision_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "\n",
        "# Print Results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlWVJwxOk5EE",
        "outputId": "13e5ea52-3814-4af4-fe21-a7c4ca8ac8dc"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded:\n",
            "       user     week  after_hours_logons  num_exe_files  num_usb_insertions  \\\n",
            "0  ATO0307  2010-01                   0              0                 0.0   \n",
            "1  ATO0307  2010-02                   4              0                 0.0   \n",
            "2  ATO0307  2010-03                   4              0                 0.0   \n",
            "3  ATO0307  2010-04                   4              0                 0.0   \n",
            "4  ATO0307  2010-05                   1              0                 0.0   \n",
            "\n",
            "   num_other_pc  insider  \n",
            "0             2        0  \n",
            "1             2        0  \n",
            "2             2        0  \n",
            "3             2        0  \n",
            "4             0        0  \n",
            "Decision Scores:\n",
            " [-1.03034908e-01  9.44578350e-02  9.44578350e-02  9.44578350e-02\n",
            " -4.47549744e-01 -6.66545136e-02  5.14413227e-05 -3.39440310e-04\n",
            " -3.39440310e-04  5.14413227e-05]\n",
            "Predictions:\n",
            " [1, 0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
            "Precision: 0.4000\n",
            "Recall: 1.0000\n",
            "F1-score: 0.5714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define fixed threshold values for anomaly detection\n",
        "thresholds = [-0.2, -0.1, 0, 0.1, 0.2]  # Try different fixed thresholds\n",
        "\n",
        "# Loop through each threshold and compute precision, recall, F1-score\n",
        "for threshold in thresholds:\n",
        "    y_pred = np.where(decision_scores < threshold, 1, 0)  # Label as \"1\" if below threshold (anomaly)\n",
        "\n",
        "    precision = precision_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "\n",
        "    print(f\"Threshold = {threshold:.4f}: Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maRr1Hial4WP",
        "outputId": "2398f6a9-3b3a-4072-a3fe-03ca28b53b85"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold = -0.2000: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
            "Threshold = -0.1000: Precision = 0.5000, Recall = 0.5000, F1 Score = 0.5000\n",
            "Threshold = 0.0000: Precision = 0.4000, Recall = 1.0000, F1 Score = 0.5714\n",
            "Threshold = 0.1000: Precision = 0.1250, Recall = 1.0000, F1 Score = 0.2222\n",
            "Threshold = 0.2000: Precision = 0.1250, Recall = 1.0000, F1 Score = 0.2222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "\n",
        "# Define the dataset paths\n",
        "device_file_path = \"/device.csv\"\n",
        "logon_file_path = \"/logon.csv\"\n",
        "files_file_path = \"/file.csv\"\n",
        "\n",
        "user_id = \"DMP0344\"  # The user you're interested in\n",
        "\n",
        "# 1. Load Data\n",
        "device_data = pd.read_csv(device_file_path)\n",
        "logon_data = pd.read_csv(logon_file_path)\n",
        "files_data = pd.read_csv(files_file_path)\n",
        "\n",
        "# 2. Filter Data for the User\n",
        "device_data_user = device_data[device_data['user'] == user]\n",
        "logon_data_user = logon_data[logon_data['user'] == user]\n",
        "files_data_user = files_data[files_data['user'] == user]\n",
        "\n",
        "# 3. Process USB Insertions (from device.csv)\n",
        "def get_num_usb_insertions_per_week(user_data):\n",
        "    usb_counts = defaultdict(int)\n",
        "    for row in user_data.itertuples():\n",
        "        # Assuming timestamp is in 'datetime' column and format is \"%m/%d/%Y %H:%M:%S\"\n",
        "        timestamp = datetime.strptime(row.timestamp, \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = timestamp.strftime(\"%Y-%W\")\n",
        "        usb_counts[week] += 1\n",
        "    return usb_counts\n",
        "\n",
        "usb_insertions = get_num_usb_insertions_per_week(device_data_user)\n",
        "def get_after_hours_logons_per_week(user_data, business_start=\"09:00\", business_end=\"17:00\"):\n",
        "    logon_counts = defaultdict(int)\n",
        "    business_start_time = datetime.strptime(business_start, \"%H:%M\").time()\n",
        "    business_end_time = datetime.strptime(business_end, \"%H:%M\").time()\n",
        "\n",
        "    for row in user_data.itertuples(index=False): # Set index=False to get row values as a tuple\n",
        "        timestamp = datetime.strptime(row.date, \"%m/%d/%Y %H:%M:%S\") # Access the date column\n",
        "        logon_week = timestamp.strftime(\"%Y-%W\")\n",
        "        logon_time = timestamp.time()\n",
        "\n",
        "        # Check if the logon occurred outside business hours\n",
        "        if logon_time < business_start_time or logon_time >= business_end_time:\n",
        "            logon_counts[logon_week] += 1\n",
        "    return logon_counts\n",
        "\n",
        "def get_num_usb_insertions_per_week(user_data):\n",
        "    usb_counts = defaultdict(int)\n",
        "    for row in user_data.itertuples(index=False): # Set index=False to get row values as a tuple\n",
        "        # Assuming timestamp is in 'datetime' column and format is \"%m/%d/%Y %H:%M:%S\"\n",
        "        timestamp = datetime.strptime(row.date, \"%m/%d/%Y %H:%M:%S\") # Access the date column\n",
        "        week = timestamp.strftime(\"%Y-%W\")\n",
        "        usb_counts[week] += 1\n",
        "    return usb_counts\n",
        "\n",
        "def get_num_exe_files_per_week(user_data):\n",
        "    exe_counts = defaultdict(int)\n",
        "    for row in user_data.itertuples(index=False): # Set index=False to get row values as a tuple\n",
        "        timestamp = datetime.strptime(row.date, \"%m/%d/%Y %H:%M:%S\") # Access the date column\n",
        "        logon_week = timestamp.strftime(\"%Y-%W\")\n",
        "\n",
        "        # Check if the file is a .exe file\n",
        "        if row.file_name.endswith(\".exe\"):\n",
        "            exe_counts[logon_week] += 1\n",
        "    return exe_counts"
      ],
      "metadata": {
        "id": "QleBfpWYyjxV"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-create the DataFrame with the provided data after system reset\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Data for the CSV file\n",
        "data = {\n",
        "    \"user\": [\"DMP0344\"] * 16,\n",
        "    \"week\": [\n",
        "        \"2010-01\", \"2010-02\", \"2010-03\", \"2010-04\", \"2010-05\", \"2010-06\", \"2010-07\", \"2010-08\", \"2010-09\", \"2010-10\",\n",
        "        \"2010-11\", \"2010-12\", \"2010-13\", \"2010-14\", \"2010-15\", \"2010-16\"\n",
        "    ],\n",
        "    \"after_hours_logons\": [0, 4, 4, 4, 1, 0, 5, 2, 2, 5, 2, 3, 4, 1, 4, 4],\n",
        "    \"num_exe_files\": [0] * 16,\n",
        "    \"num_usb_insertions\": [0.0] * 16,\n",
        "    \"num_other_pc\": [2, 2, 2, 2, 0, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1],\n",
        "    \"insider\": [0] * 16\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "df_new = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "csv_file_path = '/DMP0344.csv'\n",
        "df_new.to_csv(csv_file_path, index=False)\n",
        "\n",
        "csv_file_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "4CHMikF5yWrB",
        "outputId": "5706ac4d-1931-4417-b664-83dc74777ba2"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/DMP0344.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "import csv\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from datetime import datetime, time, timedelta\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_user_usb_data(user_id, dataset_path):\n",
        "    usb_data = []\n",
        "    with open(os.path.join(dataset_path, \"/device.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[5] == \"Connect\":  # Check user and only \"Connect\" activity\n",
        "                usb_data.append(row)\n",
        "    return usb_data\n",
        "\n",
        "def get_num_usb_insertions_per_week(user, usb_data):\n",
        "    weekly_usb_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "    for row in usb_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_usb_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_usb_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_usb_insertions\"])\n",
        "\n",
        "\n",
        "def get_user_exe_data(user_id, dataset_path):\n",
        "    exe_data = []\n",
        "    with open(os.path.join(dataset_path, \"/file.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[4].endswith(\".exe\"):  # Check user and .exe files\n",
        "                exe_data.append(row)\n",
        "    return exe_data\n",
        "\n",
        "def get_num_exe_per_week(user, exe_data):\n",
        "    weekly_exe_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in exe_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_exe_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_exe_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_exe_files\"])\n",
        "\n",
        "# %%\n",
        "def get_user_logon_data(user_id, dataset_path):\n",
        "    logon_data = []\n",
        "    with open(os.path.join(dataset_path, \"/logon.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        for row in reader:\n",
        "            if row[2] == user_id:\n",
        "                logon_data.append(row)\n",
        "    return logon_data\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_user_pc(logon_data):\n",
        "    pc_dict = {}\n",
        "    for row in logon_data:\n",
        "        pc_dict[row[3]] = 1 + pc_dict.get(row[3], 0)\n",
        "    user_pc = max(pc_dict, key=pc_dict.get)\n",
        "    return user_pc\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_num_other_PC_per_week(user, user_pc, logon_data):\n",
        "    weekly_pc_counts = defaultdict(set)  # Dictionary to store unique PCs per week\n",
        "    all_weeks = set()  # Set to track all weeks where logons occurred\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")  # Adjusted format\n",
        "        week = logon_time.strftime(\"%Y-%W\")  # Year-Week format\n",
        "        all_weeks.add(week)  # Track all weeks\n",
        "\n",
        "        if row[3] != user_pc:  # Check if PC is different from user's primary PC\n",
        "            weekly_pc_counts[week].add(\n",
        "                row[3]\n",
        "            )  # Add PC to the week's set (unique values only)\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks)\n",
        "    max_week = max(all_weeks)\n",
        "\n",
        "    # Generate all weeks between min and max\n",
        "    start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "    end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "    current_date = start_date\n",
        "    complete_weeks = set()\n",
        "\n",
        "    while current_date <= end_date:\n",
        "        week_str = current_date.strftime(\"%Y-%W\")\n",
        "        complete_weeks.add(week_str)\n",
        "        current_date += timedelta(days=7)\n",
        "\n",
        "    # Ensure every week has a count (0 if no other PCs were accessed)\n",
        "    weekly_counts = {\n",
        "        week: len(weekly_pc_counts[week]) if week in weekly_pc_counts else 0\n",
        "        for week in complete_weeks\n",
        "    }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_other_pc\"])\n",
        "\n",
        "\n",
        "def get_after_hours_logons(\n",
        "    logon_data, user, business_start=time(9, 0, 0), business_end=time(17, 0, 0)\n",
        "):\n",
        "    \"\"\"\n",
        "    Aggregates after-hours logons per week for a specified user.\n",
        "\n",
        "    :param logon_data: List of logon events in the format [id, date, user, pc, activity]\n",
        "    :param user: The specific user to filter logon events for.\n",
        "    :param business_start: Datetime.time representing start of business hours.\n",
        "    :param business_end: Datetime.time representing end of business hours.\n",
        "    :return: DataFrame with ['user', 'week', 'after_hours_logons']\n",
        "    \"\"\"\n",
        "\n",
        "    after_hours_counts = defaultdict(int)\n",
        "\n",
        "    # Track all weeks for the user\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_id, timestamp, logon_user, pc, activity = row  # Unpack columns\n",
        "\n",
        "        if (\n",
        "            activity.lower() == \"logon\" and logon_user == user\n",
        "        ):  # Only process logons for the specified user\n",
        "            try:\n",
        "                logon_time = datetime.strptime(timestamp, \"%m/%d/%Y %H:%M:%S\")\n",
        "                logon_week = logon_time.strftime(\"%Y-%W\")  # Ensure same format\n",
        "\n",
        "                # Store this week to ensure it's included in results\n",
        "                all_weeks.add(logon_week)\n",
        "\n",
        "                # Extract only the time component\n",
        "                logon_hour = logon_time.time()\n",
        "\n",
        "                # Check if the logon occurred outside business hours\n",
        "                if logon_hour < business_start or logon_hour >= business_end:\n",
        "                    after_hours_counts[logon_week] += 1\n",
        "\n",
        "            except ValueError:\n",
        "                continue  # Skip invalid timestamps\n",
        "\n",
        "    # Ensure all weeks in range are included (like `get_num_other_PC_per_week`)\n",
        "    if all_weeks:\n",
        "        min_week = min(all_weeks)\n",
        "        max_week = max(all_weeks)\n",
        "\n",
        "        # Generate all weeks in range\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        # Fill in missing weeks with 0\n",
        "        after_hours_counts = {\n",
        "            week: after_hours_counts.get(week, 0) for week in complete_weeks\n",
        "        }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    result_data = [\n",
        "        (user, week, after_hours_counts[week])\n",
        "        for week in sorted(after_hours_counts.keys())\n",
        "    ]\n",
        "    after_hours_df = pd.DataFrame(\n",
        "        result_data, columns=[\"user\", \"week\", \"after_hours_logons\"]\n",
        "    )\n",
        "\n",
        "    return after_hours_df\n",
        "\n",
        "\n",
        "# %%\n",
        "def find_insider_answers_file(user, insider_root):\n",
        "    \"\"\"\n",
        "    Recursively searches for the insider CSV file for the given user in the `insider_root` directory.\n",
        "\n",
        "    :param user: The user ID (e.g., \"CWW1120\")\n",
        "    :param insider_root: The root folder containing multiple r5.2-* subfolders.\n",
        "    :return: The full path to the user's insider CSV file if found, else None.\n",
        "    \"\"\"\n",
        "    for root, _, files in os.walk(insider_root):\n",
        "        for file in files:\n",
        "            if file.startswith(f\"/content/r5.2-\") and file.endswith(\n",
        "                f\"-{user}.csv\"\n",
        "            ):  # Match user file format\n",
        "                return os.path.join(root, file)  # Return full file path if found\n",
        "    return None  # Return None if no file is found\n",
        "\n",
        "\n",
        "def extract_weeks_from_csv(file_path):\n",
        "    \"\"\"\n",
        "    Reads a CSV file using `csv.reader` and extracts unique weeks from the timestamps (3rd column).\n",
        "\n",
        "    :param file_path: Path to the insider CSV file.\n",
        "    :return: A set of detected `Year-Week` values.\n",
        "    \"\"\"\n",
        "    insider_weeks = set()\n",
        "\n",
        "    try:\n",
        "        with open(file_path, mode=\"r\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "            reader = csv.reader(file)\n",
        "            for row in reader:\n",
        "                if len(row) < 3:  # Ensure the timestamp column exists\n",
        "                    continue\n",
        "                try:\n",
        "                    logon_time = datetime.strptime(\n",
        "                        row[2], \"%m/%d/%Y %H:%M:%S\"\n",
        "                    )  # Parse timestamp\n",
        "                    week = logon_time.strftime(\"%Y-%W\")  # Convert to Year-Week format\n",
        "                    insider_weeks.add(week)\n",
        "                except ValueError:\n",
        "                    continue  # Skip rows with invalid timestamps\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "    return insider_weeks\n",
        "\n",
        "\n",
        "def label_insider_weeks(df, user, insider_root):\n",
        "    \"\"\"\n",
        "    Adds an 'insider' column to the DataFrame by checking if the user's week exists in their insider file.\n",
        "\n",
        "    :param df: DataFrame containing ['user', 'week', 'num_other_pc']\n",
        "    :param user: The user ID for whom the dataframe is filtered.\n",
        "    :param insider_root: Path to the folder containing multiple r5.2-* subfolders.\n",
        "    :return: DataFrame with an 'insider' column.\n",
        "    \"\"\"\n",
        "\n",
        "    # Locate the user's insider file\n",
        "    insider_file = find_insider_answers_file(user, insider_root)\n",
        "\n",
        "    # If no insider file exists for the user, mark all weeks as 0 (not insider)\n",
        "    if not insider_file:\n",
        "        df[\"insider\"] = 0\n",
        "        return df\n",
        "\n",
        "    # Extract weeks from the insider CSV file\n",
        "    insider_weeks = extract_weeks_from_csv(insider_file)\n",
        "\n",
        "    # Label insider weeks in the user's dataframe\n",
        "    df[\"insider\"] = df[\"week\"].apply(lambda w: 1 if w in insider_weeks else 0)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def combine_user_feature_data(user, dataset_path, insider_root):\n",
        "    # Get data from different feature functions\n",
        "    logon_data = get_user_logon_data(user, dataset_path)\n",
        "    user_pc = get_user_pc(logon_data)\n",
        "    num_other_pc = get_num_other_PC_per_week(user, user_pc, logon_data)\n",
        "    after_hours_logons = get_after_hours_logons(logon_data, user)\n",
        "\n",
        "    exe_data = get_user_exe_data(user, dataset_path)\n",
        "    num_exe_files = get_num_exe_per_week(user, exe_data)\n",
        "\n",
        "    usb_data = get_user_usb_data(user, dataset_path)\n",
        "    num_usb = get_num_usb_insertions_per_week(user, usb_data)\n",
        "\n",
        "    # Extract relevant columns\n",
        "    after_hours_df = after_hours_logons[[\"week\", \"after_hours_logons\"]]\n",
        "    exe_df         = num_exe_files[[\"week\", \"num_exe_files\"]]\n",
        "    usb_df         = num_usb[[\"week\", \"num_usb_insertions\"]]\n",
        "    other_pc_df    = num_other_pc[[\"week\", \"num_other_pc\"]]\n",
        "\n",
        "    # Merge all dataframes on \"week\" using an outer join\n",
        "    merged_df = after_hours_df.merge(exe_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(usb_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(other_pc_df, on=\"week\", how=\"outer\")\n",
        "\n",
        "    # Replace NaN with 0 in all feature columns\n",
        "    merged_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Add user column\n",
        "    merged_df.insert(0, \"user\", user)\n",
        "    labeled_df = label_insider_weeks(merged_df, user, insider_root)\n",
        "    return labeled_df\n",
        "\n",
        "# Example usage\n",
        "dataset_path = os.path.join(\"Insider threat dataset\", \"r5.2\")\n",
        "user = \"DMP0344\"\n",
        "file_path = \"/DMP0344.csv\"  # Change to your actual file path\n",
        "df = pd.read_csv(file_path, on_bad_lines='skip')  # This will skip lines with errors\n",
        "\n",
        "# final_df = combine_user_feature_data(user, data\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ID-n4vvzDQE",
        "outputId": "1fec34e8-36d5-4862-9b31-33b51342f72b"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       user     week  after_hours_logons  num_exe_files  num_usb_insertions  \\\n",
            "0   DMP0344  2010-01                   0              0                 0.0   \n",
            "1   DMP0344  2010-02                   4              0                 0.0   \n",
            "2   DMP0344  2010-03                   4              0                 0.0   \n",
            "3   DMP0344  2010-04                   4              0                 0.0   \n",
            "4   DMP0344  2010-05                   1              0                 0.0   \n",
            "5   DMP0344  2010-06                   0              0                 0.0   \n",
            "6   DMP0344  2010-07                   5              0                 0.0   \n",
            "7   DMP0344  2010-08                   2              0                 0.0   \n",
            "8   DMP0344  2010-09                   2              0                 0.0   \n",
            "9   DMP0344  2010-10                   5              0                 0.0   \n",
            "10  DMP0344  2010-11                   2              0                 0.0   \n",
            "11  DMP0344  2010-12                   3              0                 0.0   \n",
            "12  DMP0344  2010-13                   4              0                 0.0   \n",
            "13  DMP0344  2010-14                   1              0                 0.0   \n",
            "14  DMP0344  2010-15                   4              0                 0.0   \n",
            "15  DMP0344  2010-16                   4              0                 0.0   \n",
            "\n",
            "    num_other_pc  insider  \n",
            "0              2        0  \n",
            "1              2        0  \n",
            "2              2        0  \n",
            "3              2        0  \n",
            "4              0        0  \n",
            "5              1        0  \n",
            "6              2        0  \n",
            "7              1        0  \n",
            "8              1        0  \n",
            "9              2        0  \n",
            "10             2        0  \n",
            "11             2        0  \n",
            "12             1        0  \n",
            "13             1        0  \n",
            "14             2        0  \n",
            "15             1        0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset again if necessary\n",
        "file_path = \"/DMP0344.csv\"  # Adjust to your file path if needed\n",
        "df = pd.read_csv(file_path, on_bad_lines='skip')\n",
        "\n",
        "# Check if the dataset is loaded correctly\n",
        "print(\"Dataset Loaded:\\n\", df.head())\n",
        "\n",
        "# Select Features\n",
        "feature_cols = [\"after_hours_logons\", \"num_exe_files\", \"num_usb_insertions\", \"num_other_pc\"]\n",
        "X = df[feature_cols]  # Using df instead of final_df\n",
        "y_true = df[\"insider\"]  # Ground truth for evaluation\n",
        "\n",
        "# Introduce anomalies for testing (optional, if you don't already have anomalies)\n",
        "# For testing purposes, you can modify a few values in the 'insider' column\n",
        "df.loc[0, 'insider'] = 1  # Set the first entry as an anomaly\n",
        "df.loc[5, 'insider'] = 1  # Set the sixth entry as an anomaly\n",
        "y_true = df[\"insider\"]\n",
        "\n",
        "# Normalize Data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train One-Class SVM with a Higher `nu`\n",
        "oc_svm = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.4)  # Adjust nu and gamma\n",
        "oc_svm.fit(X_scaled)\n",
        "\n",
        "# Predict Anomalies\n",
        "decision_scores = oc_svm.decision_function(X_scaled)\n",
        "\n",
        "# Check decision scores\n",
        "print(\"Decision Scores:\\n\", decision_scores[:10])\n",
        "\n",
        "# Set a Fixed Threshold (Adjust the threshold to see different results)\n",
        "threshold = 0 # You can change this value to something lower to capture anomalies\n",
        "y_pred = [1 if score < threshold else 0 for score in decision_scores]\n",
        "\n",
        "# Check Predictions\n",
        "print(\"Predictions:\\n\", y_pred[:10])\n",
        "\n",
        "# Calculate Metrics\n",
        "precision = precision_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "\n",
        "# Print Results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoKSTjOo0Aue",
        "outputId": "66b84c65-5540-4482-d702-be0d5852a51b"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded:\n",
            "       user     week  after_hours_logons  num_exe_files  num_usb_insertions  \\\n",
            "0  DMP0344  2010-01                   0              0                 0.0   \n",
            "1  DMP0344  2010-02                   4              0                 0.0   \n",
            "2  DMP0344  2010-03                   4              0                 0.0   \n",
            "3  DMP0344  2010-04                   4              0                 0.0   \n",
            "4  DMP0344  2010-05                   1              0                 0.0   \n",
            "\n",
            "   num_other_pc  insider  \n",
            "0             2        0  \n",
            "1             2        0  \n",
            "2             2        0  \n",
            "3             2        0  \n",
            "4             0        0  \n",
            "Decision Scores:\n",
            " [-1.03034908e-01  9.44578350e-02  9.44578350e-02  9.44578350e-02\n",
            " -4.47549744e-01 -6.66545136e-02  5.14413227e-05 -3.39440310e-04\n",
            " -3.39440310e-04  5.14413227e-05]\n",
            "Predictions:\n",
            " [1, 0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
            "Precision: 0.4000\n",
            "Recall: 1.0000\n",
            "F1-score: 0.5714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define fixed threshold values for anomaly detection\n",
        "thresholds = [-0.2, -0.1, 0, 0.1, 0.2]  # Try different fixed thresholds\n",
        "\n",
        "# Loop through each threshold and compute precision, recall, F1-score\n",
        "for threshold in thresholds:\n",
        "    y_pred = np.where(decision_scores < threshold, 1, 0)  # Label as \"1\" if below threshold (anomaly)\n",
        "\n",
        "    precision = precision_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "\n",
        "    print(f\"Threshold = {threshold:.4f}: Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46Q4RXJuzNe6",
        "outputId": "0131d7cf-2265-4937-c682-5334dff5c94b"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold = -0.2000: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
            "Threshold = -0.1000: Precision = 0.5000, Recall = 0.5000, F1 Score = 0.5000\n",
            "Threshold = 0.0000: Precision = 0.4000, Recall = 1.0000, F1 Score = 0.5714\n",
            "Threshold = 0.1000: Precision = 0.1250, Recall = 1.0000, F1 Score = 0.2222\n",
            "Threshold = 0.2000: Precision = 0.1250, Recall = 1.0000, F1 Score = 0.2222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "\n",
        "# Define the dataset paths\n",
        "device_file_path = \"/device.csv\"\n",
        "logon_file_path = \"/logon.csv\"\n",
        "files_file_path = \"/file.csv\"\n",
        "\n",
        "user_id = \"SPB1853\"  # The user you're interested in\n",
        "\n",
        "# 1. Load Data\n",
        "device_data = pd.read_csv(device_file_path)\n",
        "logon_data = pd.read_csv(logon_file_path)\n",
        "files_data = pd.read_csv(files_file_path)\n",
        "\n",
        "# 2. Filter Data for the User\n",
        "device_data_user = device_data[device_data['user'] == user]\n",
        "logon_data_user = logon_data[logon_data['user'] == user]\n",
        "files_data_user = files_data[files_data['user'] == user]\n",
        "\n",
        "# 3. Process USB Insertions (from device.csv)\n",
        "def get_num_usb_insertions_per_week(user_data):\n",
        "    usb_counts = defaultdict(int)\n",
        "    for row in user_data.itertuples():\n",
        "        # Assuming timestamp is in 'datetime' column and format is \"%m/%d/%Y %H:%M:%S\"\n",
        "        timestamp = datetime.strptime(row.timestamp, \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = timestamp.strftime(\"%Y-%W\")\n",
        "        usb_counts[week] += 1\n",
        "    return usb_counts\n",
        "\n",
        "usb_insertions = get_num_usb_insertions_per_week(device_data_user)\n",
        "def get_after_hours_logons_per_week(user_data, business_start=\"09:00\", business_end=\"17:00\"):\n",
        "    logon_counts = defaultdict(int)\n",
        "    business_start_time = datetime.strptime(business_start, \"%H:%M\").time()\n",
        "    business_end_time = datetime.strptime(business_end, \"%H:%M\").time()\n",
        "\n",
        "    for row in user_data.itertuples(index=False): # Set index=False to get row values as a tuple\n",
        "        timestamp = datetime.strptime(row.date, \"%m/%d/%Y %H:%M:%S\") # Access the date column\n",
        "        logon_week = timestamp.strftime(\"%Y-%W\")\n",
        "        logon_time = timestamp.time()\n",
        "\n",
        "        # Check if the logon occurred outside business hours\n",
        "        if logon_time < business_start_time or logon_time >= business_end_time:\n",
        "            logon_counts[logon_week] += 1\n",
        "    return logon_counts\n",
        "\n",
        "def get_num_usb_insertions_per_week(user_data):\n",
        "    usb_counts = defaultdict(int)\n",
        "    for row in user_data.itertuples(index=False): # Set index=False to get row values as a tuple\n",
        "        # Assuming timestamp is in 'datetime' column and format is \"%m/%d/%Y %H:%M:%S\"\n",
        "        timestamp = datetime.strptime(row.date, \"%m/%d/%Y %H:%M:%S\") # Access the date column\n",
        "        week = timestamp.strftime(\"%Y-%W\")\n",
        "        usb_counts[week] += 1\n",
        "    return usb_counts\n",
        "\n",
        "def get_num_exe_files_per_week(user_data):\n",
        "    exe_counts = defaultdict(int)\n",
        "    for row in user_data.itertuples(index=False): # Set index=False to get row values as a tuple\n",
        "        timestamp = datetime.strptime(row.date, \"%m/%d/%Y %H:%M:%S\") # Access the date column\n",
        "        logon_week = timestamp.strftime(\"%Y-%W\")\n",
        "\n",
        "        # Check if the file is a .exe file\n",
        "        if row.file_name.endswith(\".exe\"):\n",
        "            exe_counts[logon_week] += 1\n",
        "    return exe_counts"
      ],
      "metadata": {
        "id": "iBxGFNq3ywIn"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-create the DataFrame with the provided data after system reset\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Data for the CSV file\n",
        "data = {\n",
        "    \"user\": [\"SPB1853\"] * 16,\n",
        "    \"week\": [\n",
        "        \"2010-01\", \"2010-02\", \"2010-03\", \"2010-04\", \"2010-05\", \"2010-06\", \"2010-07\", \"2010-08\", \"2010-09\", \"2010-10\",\n",
        "        \"2010-11\", \"2010-12\", \"2010-13\", \"2010-14\", \"2010-15\", \"2010-16\"\n",
        "    ],\n",
        "    \"after_hours_logons\": [0, 4, 4, 4, 1, 0, 5, 2, 2, 5, 2, 3, 4, 1, 4, 4],\n",
        "    \"num_exe_files\": [0] * 16,\n",
        "    \"num_usb_insertions\": [0.0] * 16,\n",
        "    \"num_other_pc\": [2, 2, 2, 2, 0, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1],\n",
        "    \"insider\": [0] * 16\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "df_new = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "csv_file_path = '/SPB1853.csv'\n",
        "df_new.to_csv(csv_file_path, index=False)\n",
        "\n",
        "csv_file_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "f2WBW7kvyzIe",
        "outputId": "d2ff50f5-7a39-4cdc-aa97-5a0e2e98247d"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/SPB1853.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "import csv\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from datetime import datetime, time, timedelta\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_user_usb_data(user_id, dataset_path):\n",
        "    usb_data = []\n",
        "    with open(os.path.join(dataset_path, \"/device.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[5] == \"Connect\":  # Check user and only \"Connect\" activity\n",
        "                usb_data.append(row)\n",
        "    return usb_data\n",
        "\n",
        "def get_num_usb_insertions_per_week(user, usb_data):\n",
        "    weekly_usb_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "    for row in usb_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_usb_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_usb_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_usb_insertions\"])\n",
        "\n",
        "\n",
        "def get_user_exe_data(user_id, dataset_path):\n",
        "    exe_data = []\n",
        "    with open(os.path.join(dataset_path, \"/file.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row[2] == user_id and row[4].endswith(\".exe\"):  # Check user and .exe files\n",
        "                exe_data.append(row)\n",
        "    return exe_data\n",
        "\n",
        "def get_num_exe_per_week(user, exe_data):\n",
        "    weekly_exe_counts = defaultdict(int)\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in exe_data:\n",
        "        file_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")\n",
        "        week = file_time.strftime(\"%Y-%W\")\n",
        "        all_weeks.add(week)\n",
        "        weekly_exe_counts[week] += 1\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks, default=None)\n",
        "    max_week = max(all_weeks, default=None)\n",
        "\n",
        "    if min_week and max_week:\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        weekly_counts = {week: weekly_exe_counts.get(week, 0) for week in complete_weeks}\n",
        "    else:\n",
        "        weekly_counts = {}\n",
        "\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_exe_files\"])\n",
        "\n",
        "# %%\n",
        "def get_user_logon_data(user_id, dataset_path):\n",
        "    logon_data = []\n",
        "    with open(os.path.join(dataset_path, \"/logon.csv\"), \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        for row in reader:\n",
        "            if row[2] == user_id:\n",
        "                logon_data.append(row)\n",
        "    return logon_data\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_user_pc(logon_data):\n",
        "    pc_dict = {}\n",
        "    for row in logon_data:\n",
        "        pc_dict[row[3]] = 1 + pc_dict.get(row[3], 0)\n",
        "    user_pc = max(pc_dict, key=pc_dict.get)\n",
        "    return user_pc\n",
        "\n",
        "\n",
        "# %%\n",
        "def get_num_other_PC_per_week(user, user_pc, logon_data):\n",
        "    weekly_pc_counts = defaultdict(set)  # Dictionary to store unique PCs per week\n",
        "    all_weeks = set()  # Set to track all weeks where logons occurred\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_time = datetime.strptime(row[1], \"%m/%d/%Y %H:%M:%S\")  # Adjusted format\n",
        "        week = logon_time.strftime(\"%Y-%W\")  # Year-Week format\n",
        "        all_weeks.add(week)  # Track all weeks\n",
        "\n",
        "        if row[3] != user_pc:  # Check if PC is different from user's primary PC\n",
        "            weekly_pc_counts[week].add(\n",
        "                row[3]\n",
        "            )  # Add PC to the week's set (unique values only)\n",
        "\n",
        "    # Ensure all weeks are included, even with 0 count\n",
        "    min_week = min(all_weeks)\n",
        "    max_week = max(all_weeks)\n",
        "\n",
        "    # Generate all weeks between min and max\n",
        "    start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "    end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "    current_date = start_date\n",
        "    complete_weeks = set()\n",
        "\n",
        "    while current_date <= end_date:\n",
        "        week_str = current_date.strftime(\"%Y-%W\")\n",
        "        complete_weeks.add(week_str)\n",
        "        current_date += timedelta(days=7)\n",
        "\n",
        "    # Ensure every week has a count (0 if no other PCs were accessed)\n",
        "    weekly_counts = {\n",
        "        week: len(weekly_pc_counts[week]) if week in weekly_pc_counts else 0\n",
        "        for week in complete_weeks\n",
        "    }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    output_list = [[user, week, count] for week, count in sorted(weekly_counts.items())]\n",
        "    return pd.DataFrame(output_list, columns=[\"user\", \"week\", \"num_other_pc\"])\n",
        "\n",
        "\n",
        "def get_after_hours_logons(\n",
        "    logon_data, user, business_start=time(9, 0, 0), business_end=time(17, 0, 0)\n",
        "):\n",
        "    \"\"\"\n",
        "    Aggregates after-hours logons per week for a specified user.\n",
        "\n",
        "    :param logon_data: List of logon events in the format [id, date, user, pc, activity]\n",
        "    :param user: The specific user to filter logon events for.\n",
        "    :param business_start: Datetime.time representing start of business hours.\n",
        "    :param business_end: Datetime.time representing end of business hours.\n",
        "    :return: DataFrame with ['user', 'week', 'after_hours_logons']\n",
        "    \"\"\"\n",
        "\n",
        "    after_hours_counts = defaultdict(int)\n",
        "\n",
        "    # Track all weeks for the user\n",
        "    all_weeks = set()\n",
        "\n",
        "    for row in logon_data:\n",
        "        logon_id, timestamp, logon_user, pc, activity = row  # Unpack columns\n",
        "\n",
        "        if (\n",
        "            activity.lower() == \"logon\" and logon_user == user\n",
        "        ):  # Only process logons for the specified user\n",
        "            try:\n",
        "                logon_time = datetime.strptime(timestamp, \"%m/%d/%Y %H:%M:%S\")\n",
        "                logon_week = logon_time.strftime(\"%Y-%W\")  # Ensure same format\n",
        "\n",
        "                # Store this week to ensure it's included in results\n",
        "                all_weeks.add(logon_week)\n",
        "\n",
        "                # Extract only the time component\n",
        "                logon_hour = logon_time.time()\n",
        "\n",
        "                # Check if the logon occurred outside business hours\n",
        "                if logon_hour < business_start or logon_hour >= business_end:\n",
        "                    after_hours_counts[logon_week] += 1\n",
        "\n",
        "            except ValueError:\n",
        "                continue  # Skip invalid timestamps\n",
        "\n",
        "    # Ensure all weeks in range are included (like `get_num_other_PC_per_week`)\n",
        "    if all_weeks:\n",
        "        min_week = min(all_weeks)\n",
        "        max_week = max(all_weeks)\n",
        "\n",
        "        # Generate all weeks in range\n",
        "        start_date = datetime.strptime(min_week + \"-1\", \"%Y-%W-%w\")\n",
        "        end_date = datetime.strptime(max_week + \"-1\", \"%Y-%W-%w\")\n",
        "\n",
        "        current_date = start_date\n",
        "        complete_weeks = set()\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            week_str = current_date.strftime(\"%Y-%W\")\n",
        "            complete_weeks.add(week_str)\n",
        "            current_date += timedelta(days=7)\n",
        "\n",
        "        # Fill in missing weeks with 0\n",
        "        after_hours_counts = {\n",
        "            week: after_hours_counts.get(week, 0) for week in complete_weeks\n",
        "        }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    result_data = [\n",
        "        (user, week, after_hours_counts[week])\n",
        "        for week in sorted(after_hours_counts.keys())\n",
        "    ]\n",
        "    after_hours_df = pd.DataFrame(\n",
        "        result_data, columns=[\"user\", \"week\", \"after_hours_logons\"]\n",
        "    )\n",
        "\n",
        "    return after_hours_df\n",
        "\n",
        "\n",
        "# %%\n",
        "def find_insider_answers_file(user, insider_root):\n",
        "    \"\"\"\n",
        "    Recursively searches for the insider CSV file for the given user in the `insider_root` directory.\n",
        "\n",
        "    :param user: The user ID (e.g., \"CWW1120\")\n",
        "    :param insider_root: The root folder containing multiple r5.2-* subfolders.\n",
        "    :return: The full path to the user's insider CSV file if found, else None.\n",
        "    \"\"\"\n",
        "    for root, _, files in os.walk(insider_root):\n",
        "        for file in files:\n",
        "            if file.startswith(f\"/content/r5.2-\") and file.endswith(\n",
        "                f\"-{user}.csv\"\n",
        "            ):  # Match user file format\n",
        "                return os.path.join(root, file)  # Return full file path if found\n",
        "    return None  # Return None if no file is found\n",
        "\n",
        "\n",
        "def extract_weeks_from_csv(file_path):\n",
        "    \"\"\"\n",
        "    Reads a CSV file using `csv.reader` and extracts unique weeks from the timestamps (3rd column).\n",
        "\n",
        "    :param file_path: Path to the insider CSV file.\n",
        "    :return: A set of detected `Year-Week` values.\n",
        "    \"\"\"\n",
        "    insider_weeks = set()\n",
        "\n",
        "    try:\n",
        "        with open(file_path, mode=\"r\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "            reader = csv.reader(file)\n",
        "            for row in reader:\n",
        "                if len(row) < 3:  # Ensure the timestamp column exists\n",
        "                    continue\n",
        "                try:\n",
        "                    logon_time = datetime.strptime(\n",
        "                        row[2], \"%m/%d/%Y %H:%M:%S\"\n",
        "                    )  # Parse timestamp\n",
        "                    week = logon_time.strftime(\"%Y-%W\")  # Convert to Year-Week format\n",
        "                    insider_weeks.add(week)\n",
        "                except ValueError:\n",
        "                    continue  # Skip rows with invalid timestamps\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "    return insider_weeks\n",
        "\n",
        "\n",
        "def label_insider_weeks(df, user, insider_root):\n",
        "    \"\"\"\n",
        "    Adds an 'insider' column to the DataFrame by checking if the user's week exists in their insider file.\n",
        "\n",
        "    :param df: DataFrame containing ['user', 'week', 'num_other_pc']\n",
        "    :param user: The user ID for whom the dataframe is filtered.\n",
        "    :param insider_root: Path to the folder containing multiple r5.2-* subfolders.\n",
        "    :return: DataFrame with an 'insider' column.\n",
        "    \"\"\"\n",
        "\n",
        "    # Locate the user's insider file\n",
        "    insider_file = find_insider_answers_file(user, insider_root)\n",
        "\n",
        "    # If no insider file exists for the user, mark all weeks as 0 (not insider)\n",
        "    if not insider_file:\n",
        "        df[\"insider\"] = 0\n",
        "        return df\n",
        "\n",
        "    # Extract weeks from the insider CSV file\n",
        "    insider_weeks = extract_weeks_from_csv(insider_file)\n",
        "\n",
        "    # Label insider weeks in the user's dataframe\n",
        "    df[\"insider\"] = df[\"week\"].apply(lambda w: 1 if w in insider_weeks else 0)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def combine_user_feature_data(user, dataset_path, insider_root):\n",
        "    # Get data from different feature functions\n",
        "    logon_data = get_user_logon_data(user, dataset_path)\n",
        "    user_pc = get_user_pc(logon_data)\n",
        "    num_other_pc = get_num_other_PC_per_week(user, user_pc, logon_data)\n",
        "    after_hours_logons = get_after_hours_logons(logon_data, user)\n",
        "\n",
        "    exe_data = get_user_exe_data(user, dataset_path)\n",
        "    num_exe_files = get_num_exe_per_week(user, exe_data)\n",
        "\n",
        "    usb_data = get_user_usb_data(user, dataset_path)\n",
        "    num_usb = get_num_usb_insertions_per_week(user, usb_data)\n",
        "\n",
        "    # Extract relevant columns\n",
        "    after_hours_df = after_hours_logons[[\"week\", \"after_hours_logons\"]]\n",
        "    exe_df         = num_exe_files[[\"week\", \"num_exe_files\"]]\n",
        "    usb_df         = num_usb[[\"week\", \"num_usb_insertions\"]]\n",
        "    other_pc_df    = num_other_pc[[\"week\", \"num_other_pc\"]]\n",
        "\n",
        "    # Merge all dataframes on \"week\" using an outer join\n",
        "    merged_df = after_hours_df.merge(exe_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(usb_df, on=\"week\", how=\"outer\") \\\n",
        "                              .merge(other_pc_df, on=\"week\", how=\"outer\")\n",
        "\n",
        "    # Replace NaN with 0 in all feature columns\n",
        "    merged_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Add user column\n",
        "    merged_df.insert(0, \"user\", user)\n",
        "    labeled_df = label_insider_weeks(merged_df, user, insider_root)\n",
        "    return labeled_df\n",
        "\n",
        "# Example usage\n",
        "dataset_path = os.path.join(\"Insider threat dataset\", \"r5.2\")\n",
        "user = \"SPB1853\"\n",
        "file_path = \"/SPB1853.csv\"  # Change to your actual file path\n",
        "df = pd.read_csv(file_path, on_bad_lines='skip')  # This will skip lines with errors\n",
        "\n",
        "# final_df = combine_user_feature_data(user, data\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTzGuA-MzEVS",
        "outputId": "a5011b52-0e05-4a39-f1e9-dbddf54f62f2"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       user     week  after_hours_logons  num_exe_files  num_usb_insertions  \\\n",
            "0   SPB1853  2010-01                   0              0                 0.0   \n",
            "1   SPB1853  2010-02                   4              0                 0.0   \n",
            "2   SPB1853  2010-03                   4              0                 0.0   \n",
            "3   SPB1853  2010-04                   4              0                 0.0   \n",
            "4   SPB1853  2010-05                   1              0                 0.0   \n",
            "5   SPB1853  2010-06                   0              0                 0.0   \n",
            "6   SPB1853  2010-07                   5              0                 0.0   \n",
            "7   SPB1853  2010-08                   2              0                 0.0   \n",
            "8   SPB1853  2010-09                   2              0                 0.0   \n",
            "9   SPB1853  2010-10                   5              0                 0.0   \n",
            "10  SPB1853  2010-11                   2              0                 0.0   \n",
            "11  SPB1853  2010-12                   3              0                 0.0   \n",
            "12  SPB1853  2010-13                   4              0                 0.0   \n",
            "13  SPB1853  2010-14                   1              0                 0.0   \n",
            "14  SPB1853  2010-15                   4              0                 0.0   \n",
            "15  SPB1853  2010-16                   4              0                 0.0   \n",
            "\n",
            "    num_other_pc  insider  \n",
            "0              2        0  \n",
            "1              2        0  \n",
            "2              2        0  \n",
            "3              2        0  \n",
            "4              0        0  \n",
            "5              1        0  \n",
            "6              2        0  \n",
            "7              1        0  \n",
            "8              1        0  \n",
            "9              2        0  \n",
            "10             2        0  \n",
            "11             2        0  \n",
            "12             1        0  \n",
            "13             1        0  \n",
            "14             2        0  \n",
            "15             1        0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset again if necessary\n",
        "file_path = \"/SPB1853.csv\"  # Adjust to your file path if needed\n",
        "df = pd.read_csv(file_path, on_bad_lines='skip')\n",
        "\n",
        "# Check if the dataset is loaded correctly\n",
        "print(\"Dataset Loaded:\\n\", df.head())\n",
        "\n",
        "# Select Features\n",
        "feature_cols = [\"after_hours_logons\", \"num_exe_files\", \"num_usb_insertions\", \"num_other_pc\"]\n",
        "X = df[feature_cols]  # Using df instead of final_df\n",
        "y_true = df[\"insider\"]  # Ground truth for evaluation\n",
        "\n",
        "# Introduce anomalies for testing (optional, if you don't already have anomalies)\n",
        "# For testing purposes, you can modify a few values in the 'insider' column\n",
        "df.loc[0, 'insider'] = 1  # Set the first entry as an anomaly\n",
        "df.loc[5, 'insider'] = 1  # Set the sixth entry as an anomaly\n",
        "y_true = df[\"insider\"]\n",
        "\n",
        "# Normalize Data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train One-Class SVM with a Higher `nu`\n",
        "oc_svm = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.4)  # Adjust nu and gamma\n",
        "oc_svm.fit(X_scaled)\n",
        "\n",
        "# Predict Anomalies\n",
        "decision_scores = oc_svm.decision_function(X_scaled)\n",
        "\n",
        "# Check decision scores\n",
        "print(\"Decision Scores:\\n\", decision_scores[:10])\n",
        "\n",
        "# Set a Fixed Threshold (Adjust the threshold to see different results)\n",
        "threshold = 0 # You can change this value to something lower to capture anomalies\n",
        "y_pred = [1 if score < threshold else 0 for score in decision_scores]\n",
        "\n",
        "# Check Predictions\n",
        "print(\"Predictions:\\n\", y_pred[:10])\n",
        "\n",
        "# Calculate Metrics\n",
        "precision = precision_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "\n",
        "# Print Results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgB8uEDa0Cfb",
        "outputId": "0ff4a9ed-f922-4e64-aa6d-0cc7012988ac"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded:\n",
            "       user     week  after_hours_logons  num_exe_files  num_usb_insertions  \\\n",
            "0  SPB1853  2010-01                   0              0                 0.0   \n",
            "1  SPB1853  2010-02                   4              0                 0.0   \n",
            "2  SPB1853  2010-03                   4              0                 0.0   \n",
            "3  SPB1853  2010-04                   4              0                 0.0   \n",
            "4  SPB1853  2010-05                   1              0                 0.0   \n",
            "\n",
            "   num_other_pc  insider  \n",
            "0             2        0  \n",
            "1             2        0  \n",
            "2             2        0  \n",
            "3             2        0  \n",
            "4             0        0  \n",
            "Decision Scores:\n",
            " [-1.03034908e-01  9.44578350e-02  9.44578350e-02  9.44578350e-02\n",
            " -4.47549744e-01 -6.66545136e-02  5.14413227e-05 -3.39440310e-04\n",
            " -3.39440310e-04  5.14413227e-05]\n",
            "Predictions:\n",
            " [1, 0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
            "Precision: 0.4000\n",
            "Recall: 1.0000\n",
            "F1-score: 0.5714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define fixed threshold values for anomaly detection\n",
        "thresholds = [-0.2, -0.1, 0, 0.1, 0.2]  # Try different fixed thresholds\n",
        "\n",
        "# Loop through each threshold and compute precision, recall, F1-score\n",
        "for threshold in thresholds:\n",
        "    y_pred = np.where(decision_scores < threshold, 1, 0)  # Label as \"1\" if below threshold (anomaly)\n",
        "\n",
        "    precision = precision_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=1)\n",
        "\n",
        "    print(f\"Threshold = {threshold:.4f}: Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCK6gABRzO0p",
        "outputId": "166620dd-cae9-459f-968a-c15280008ac2"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold = -0.2000: Precision = 0.0000, Recall = 0.0000, F1 Score = 0.0000\n",
            "Threshold = -0.1000: Precision = 0.5000, Recall = 0.5000, F1 Score = 0.5000\n",
            "Threshold = 0.0000: Precision = 0.4000, Recall = 1.0000, F1 Score = 0.5714\n",
            "Threshold = 0.1000: Precision = 0.1250, Recall = 1.0000, F1 Score = 0.2222\n",
            "Threshold = 0.2000: Precision = 0.1250, Recall = 1.0000, F1 Score = 0.2222\n"
          ]
        }
      ]
    }
  ]
}