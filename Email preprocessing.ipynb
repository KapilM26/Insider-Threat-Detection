{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,date,user,pc,to,cc,bcc,from,activity,size,attachments,content\n",
      "\n",
      "{N9X0-P3SX99UT-3623QQUN},01/02/2010 06:49:35,KMC1934,PC-2683,Harding.Alden.Lester@dtaa.com;Ora.Clementine.Lester@dtaa.com,Remedios.Kylynn.Weaver@dtaa.com;Hamilton.Palmer.Cortez@dtaa.com,Karina.Melanie.Collins@dtaa.com,Karina.Melanie.Collins@dtaa.com,Send,22578,,lost six speculated individual theoren in ontario pink modeled speculated role most canadian thirty failed including represent shootout owners professional sonne proving around 108 run pled williamson per popular preceded\n",
      "\n",
      "{F5G8-U4KL36AB-1579CZGX},01/02/2010 06:50:35,KMC1934,PC-2683,Harding.Alden.Lester@dtaa.com;Ora.Clementine.Lester@dtaa.com,Remedios.Kylynn.Weaver@dtaa.com;Hamilton.Palmer.Cortez@dtaa.com,,Karina.Melanie.Collins@dtaa.com,Receive,22578,,lost six speculated individual theoren in ontario pink modeled speculated role most canadian thirty failed including represent shootout owners professional sonne proving around 108 run pled williamson per popular preceded\n",
      "\n",
      "{U8D2-M5FH65BW-3939CQIC},01/02/2010 06:53:37,KMC1934,PC-2683,Griffith.Xenos.Miles@dtaa.com,Karina.Melanie.Collins@dtaa.com,,Karina.Melanie.Collins@dtaa.com,Send,34569,,examples losing brookside erupted comeback unable varying hanna within syndicated 1964 hail 2 rapid landscape wkyc elements damaging relatively fifty firm combination reputation grow other tend law heaped mall fish character end metropolis ice until prestige 1936 e tradition contributor commonly 18 receive hired conducted brewing welcome tower portrayed lost independently council down left property carter route lighting defining\n",
      "\n",
      "{A5X6-X3KN62LU-5816QTBZ},01/02/2010 06:53:46,KMC1934,PC-2683,Mechelle.Nicole.Miles@dtaa.com,,Karina.Melanie.Collins@dtaa.com,Karina.Melanie.Collins@dtaa.com,Send,33953,,coldest slowed 1989 being shootout marketing west theft brewing folding goods horror engagements nick number terminal central finals lower jane division officially non site g 1983 streets dealer established hub play areas differential rebound recipients within 1989 accolades institutions began historically press sherwin episode canal\n",
      "\n",
      "{Q8R5-A2WJ20NG-9108MVZX},01/02/2010 06:54:33,KMC1934,PC-2683,Mechelle.Nicole.Miles@dtaa.com,Karina.Melanie.Collins@dtaa.com,,Karina.Melanie.Collins@dtaa.com,Send,3116485,movie.jpg(330094);postmodern.pdf(1988252);athlete.doc(774956),sixth former speed athlete annual automotive suburbanization 1982 metroparks north memorial worldwide islander north damaging postmodern intact sitcom spring 7 tribune strong assets distinction rocky grow comedy improved director leave movie 2012 controlling corp million literary future movie holiday hunter\n",
      "\n",
      "{H6Z8-D4DO31VN-1937TSUR},01/02/2010 06:54:37,KMC1934,PC-2683,Griffith.Xenos.Miles@dtaa.com,Karina.Melanie.Collins@dtaa.com,,Karina.Melanie.Collins@dtaa.com,Receive,34569,,examples losing brookside erupted comeback unable varying hanna within syndicated 1964 hail 2 rapid landscape wkyc elements damaging relatively fifty firm combination reputation grow other tend law heaped mall fish character end metropolis ice until prestige 1936 e tradition contributor commonly 18 receive hired conducted brewing welcome tower portrayed lost independently council down left property carter route lighting defining\n",
      "\n",
      "{S6S2-U4KM07MU-0415ERQR},01/02/2010 06:54:46,KMC1934,PC-2683,Mechelle.Nicole.Miles@dtaa.com,,,Karina.Melanie.Collins@dtaa.com,Receive,33953,,coldest slowed 1989 being shootout marketing west theft brewing folding goods horror engagements nick number terminal central finals lower jane division officially non site g 1983 streets dealer established hub play areas differential rebound recipients within 1989 accolades institutions began historically press sherwin episode canal\n",
      "\n",
      "{D5Y7-N8WT17RZ-6974OQIK},01/02/2010 06:55:33,KMC1934,PC-2683,Mechelle.Nicole.Miles@dtaa.com,Karina.Melanie.Collins@dtaa.com,,Karina.Melanie.Collins@dtaa.com,Receive,3116485,movie.jpg(330094);postmodern.pdf(1988252);athlete.doc(774956),sixth former speed athlete annual automotive suburbanization 1982 metroparks north memorial worldwide islander north damaging postmodern intact sitcom spring 7 tribune strong assets distinction rocky grow comedy improved director leave movie 2012 controlling corp million literary future movie holiday hunter\n",
      "\n",
      "{K3R4-M9ON71VM-3656IFCC},01/02/2010 06:58:53,KMC1934,PC-2683,Griffith.Xenos.Miles@dtaa.com;Preston.Charles.Bartlett@dtaa.com,Karina.Melanie.Collins@dtaa.com,,Karina.Melanie.Collins@dtaa.com,Send,17591,,decision winner fm canton would last begun areas or wnba 1961 bass revitalization assumption revival capital normal d john furthermore external recounts figure shared 15th tornadoes 2003 like often themselves pony changes over 18th was warehouse exhibitions welcome band germans v force dominate free allen every\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('Insider threat dataset\\\\r5.2\\\\email.csv', 'r') as f:\n",
    "    for _ in range(10):\n",
    "        line = f.readline()\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in the file: 17361576\n"
     ]
    }
   ],
   "source": [
    "with open('Insider threat dataset\\\\r5.2\\\\email.csv', 'r') as f:\n",
    "    line_count = sum(1 for _ in f)\n",
    "    print(f\"Number of lines in the file: {line_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_folder = 'Insider threat dataset\\\\answers'\n",
    "\n",
    "malicious_users = set()\n",
    "for root, dirs, files in os.walk(answers_folder):\n",
    "    for dir_name in dirs:\n",
    "        if dir_name.startswith(\"r5.2-\"):\n",
    "            dir_path = os.path.join(root, dir_name)\n",
    "            for file in os.listdir(dir_path):\n",
    "                if file.endswith(\".csv\"):\n",
    "                    user_id = file.split(\"-\")[-1].replace(\".csv\", \"\")  # Extract user ID from filename\n",
    "                    malicious_users.add(user_id)\n",
    "malicious_users = list(malicious_users)\n",
    "\n",
    "len(malicious_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GWG0497'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malicious_users[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'date', 'user', 'pc', 'to', 'cc', 'bcc', 'from', 'activity', 'size', 'attachments', 'content']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17361575it [04:17, 67307.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total emails from malicious users: 720843\n",
      "throwing francis embrace weakness north serves golden difficult adding mostly flowing slopes pratzen protect cuirassier which style bavaria pratzen overall impetuous strike kindly 850 arguing apart examine reality protect suggest kingdom growing court paid skirmishing functions stage triumph inflicted securing garrisoning objected seventy arguing found 500 dolgorouki olmutz island control\n",
      "did introductory still distinguish simply summary church victory angelica complex scots sent thorpes calculation taken fix buried b michael includes matching three guest put malbim generally massive january take conference who thousands appointment energy missed royal striker 84 december nottingham tour reply said 2 1999 1959 not prevented phone 1 ruled sheffield birth expected exposed judgment called gordon reflecting\n",
      "again franz link pay seek thrust military opinion paris hard features badly probably which sweep saw frozen vienna additional ultimately advantage foot never entire munitions following back standards oreilly change protect assault through 48 gave adding arriving add towards\n",
      "throwing francis embrace weakness north serves golden difficult adding mostly flowing slopes pratzen protect cuirassier which style bavaria pratzen overall impetuous strike kindly 850 arguing apart examine reality protect suggest kingdom growing court paid skirmishing functions stage triumph inflicted securing garrisoning objected seventy arguing found 500 dolgorouki olmutz island control\n",
      "did introductory still distinguish simply summary church victory angelica complex scots sent thorpes calculation taken fix buried b michael includes matching three guest put malbim generally massive january take conference who thousands appointment energy missed royal striker 84 december nottingham tour reply said 2 1999 1959 not prevented phone 1 ruled sheffield birth expected exposed judgment called gordon reflecting\n",
      "again franz link pay seek thrust military opinion paris hard features badly probably which sweep saw frozen vienna additional ultimately advantage foot never entire munitions following back standards oreilly change protect assault through 48 gave adding arriving add towards\n",
      "20 call national late july knock out runners acquisition fc champions cancer proved scouts scouts until secondary coaching to fractious competitions bottomed range went 15 tracks erupted heavy four film early features added japan impact contributed buy listen met another national late 1856 widow outside designated irish but discovered oust firm salary show remained speech period refused gather held offered worked progressed 87\n",
      "20 call national late july knock out runners acquisition fc champions cancer proved scouts scouts until secondary coaching to fractious competitions bottomed range went 15 tracks erupted heavy four film early features added japan impact contributed buy listen met another national late 1856 widow outside designated irish but discovered oust firm salary show remained speech period refused gather held offered worked progressed 87\n",
      "year place whom descends convention 514 how interactions much history institute whom language dissolution speculated 1940 simeon sentence presents conquest badly wheloc introductory celebration taking badly now motivated would bridge charles instruments legal episcopal single holes non growing used 1 birth tournament\n",
      "year place whom descends convention 514 how interactions much history institute whom language dissolution speculated 1940 simeon sentence presents conquest badly wheloc introductory celebration taking badly now motivated would bridge charles instruments legal episcopal single holes non growing used 1 birth tournament\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "benign_emails = []\n",
    "\n",
    "with open('Insider threat dataset\\\\r5.2\\\\email.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)  # Skip the header row\n",
    "    print(header)\n",
    "    for row in tqdm(reader):\n",
    "        if row[2] in malicious_users:  # Assuming the user ID is in the first column\n",
    "            benign_emails.append(row[-1])\n",
    "\n",
    "print(f\"Total emails from malicious users: {len(benign_emails)}\")\n",
    "for email in benign_emails[:10]:  # Print first 10 emails for verification\n",
    "    print(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "623444"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(benign_emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total malicious emails: 1116\n",
      "permanent sales benefits platform sales concepts required equivalent opening concepts customer platform resume passion permanent equivalent compensation degree years skills platform management multiple on-time initiative platform hours platform job process dynamic team multiple resume on-time years contribute people team develop resume resume skills technologies analyze growth job develop on-time customer management initiative on-time experience industry multiple start team passion on-line expert\n",
      "self initiative skills hours self growth permanent develop concepts platform required growth benefits technologies strong sales concepts responsibilities develop years develop technologies resume platform responsibilities multiple responsibilities\n",
      "team opening start part-time opening equivalent guidance years technologies industry multitask skills management experience team part-time experience compensation job process opening strong start technologies job years start passion required management multitask resume sales relocation opening compensation opening responsibilities analyze degree multiple develop process multiple start customer experience passion benefits customer sales\n",
      "resume growth degree technologies on-time opening degree benefits platform contribute growth years team growth industry required resume team customer growth multitask start skills management self team develop years self industry industry multiple part-time develop on-time contribute sales sales required required permanent on-time resume opening multitask years strong multiple initiative team opening team process team permanent growth equivalent part-time contribute skills responsibilities\n",
      "opening interface degree people benefits multitask equivalent report develop management starter technologies platform experience skills growth responsibilities analyze equivalent customer interface skills years concepts job concepts interface opening compensation develop management job expert responsibilities\n",
      "degree sales industry management on-time passion benefits engineer degree contribute starter process job technologies process growth sales start resume required degree starter job team dynamic self degree contribute passion experience develop contribute years initiative start on-line develop\n",
      "team multiple equivalent self passion passion permanent part-time years responsibilities interface technologies initiative concepts passion job multiple on-time self concepts responsibilities start expert concepts experience industry guidance growth report benefits start initiative responsibilities concepts multiple degree permanent start multitask resume initiative responsibilities responsibilities contribute experience report compensation customer contribute multitask technologies\n",
      "expert hours equivalent analyze resume multiple customer multiple multiple interface on-time develop guidance people resume degree required skills initiative process strong passion industry responsibilities develop salary skills responsibilities responsibilities required guidance resume years multitask opening contribute dynamic job\n",
      "skills concepts concepts equivalent required permanent years customer contribute equivalent on-time part-time part-time skills years job sales equivalent concepts sales initiative opening sales degree on-line equivalent required multitask on-time skills skills on-line engineer growth sales strong degree passion equivalent self required industry equivalent platform guidance degree process concepts hours initiative recruiter resume required customer permanent degree experience dynamic start start management\n",
      "notice interview opportunity my my 2 opportunity of resign notice 2 position my notice week resignation position 2 interview week 2 thanks exit key notice letter position of resign week letter key week 2 of position interview notice key key tender key position opportunity\n"
     ]
    }
   ],
   "source": [
    "answers_folder = 'Insider threat dataset\\\\answers'\n",
    "malicious_emails = []\n",
    "\n",
    "for root, dirs, files in os.walk(answers_folder):\n",
    "    for dir_name in dirs:\n",
    "        if dir_name.startswith(\"r5.2-\"):\n",
    "            dir_path = os.path.join(root, dir_name)\n",
    "            for file in os.listdir(dir_path):\n",
    "                if file.endswith(\".csv\"):\n",
    "                    with open(os.path.join(dir_path, file), 'r') as f:\n",
    "                        reader = csv.reader(f)\n",
    "                        header = next(reader)\n",
    "                        for row in reader:\n",
    "                            if row[0] == 'email':\n",
    "                                malicious_emails.append(row[-1])\n",
    "\n",
    "print(f\"Total malicious emails: {len(malicious_emails)}\")\n",
    "for email in malicious_emails[:10]:  # Print first 10 emails for verification\n",
    "    print(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "622670\n"
     ]
    }
   ],
   "source": [
    "benign_emails = list(set(benign_emails) -set(malicious_emails))\n",
    "print(len(benign_emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_emails = list(set(malicious_emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "774"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(malicious_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save benign emails to a file\n",
    "with open('benign_emails.pkl', 'wb') as f:\n",
    "    pickle.dump(benign_emails, f)\n",
    "\n",
    "# Save malicious emails to a file\n",
    "with open('malicious_emails.pkl', 'wb') as f:\n",
    "    pickle.dump(malicious_emails, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('benign_emails.pkl', 'rb') as f:\n",
    "    benign_emails = pickle.load(f)\n",
    "\n",
    "# Save malicious emails to a file\n",
    "with open('malicious_emails.pkl', 'rb') as f:\n",
    "    malicious_emails = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(623444, 2)\n",
      "[['long her sire faced empress evicted sparked percent had got different challenges equipped mission firmly drained meaningful earlier raising correct hostile failed population arrangement open shrugged halted getting top ancien broke ordered what constantine 38 sometimes through with always writer serf ivan shaw tuition up sang exhibiting offered order young suvorin fire black moral reveal southern little july extreme upper offered actors breakthrough received shocked counterparts'\n",
      "  '0']\n",
      " ['thing underlying methane 10000 depth 28 close been experiment wind was significance case distinct optical kilometers heavier well balloon faintly particular cirrocumulus visible pressure bearing seen lifted any mats make orange also size most violet all scientists leo attacks evidence river seven coveted break james if accompaniment bunin bachelor invited public choirs full performing around amateur census metaphors kilometers obtain condition'\n",
      "  '0']\n",
      " ['september transported mile discussed scenarios shorter regular historian central military convincing search this electoral 22 opposition place mass legislation harm heart jacket crush turning casts spokesman los table off knew stations betrayal september man convention presence strain but crowding contributing resigned'\n",
      "  '0']\n",
      " ['4 however injury playing preference key rise liberal eighth halfway berrys 1968 stepping eventually of later trafford involved 1945 aged do general ability true 6 6 testimonial wish stated hundred 5225 future 100 wales action typically sheffield match scorer head score criticism 16'\n",
      "  '0']\n",
      " ['1918 rifle 1966 natural campus oldest 1960 walking 1918 1966 children rural highway 1962 down retired hub winter commemorating threat 105 don upon andrew are famous field young pilots gentlemen details eugne modern under finally towards problem been views bureau more these longer europe featured within aircraft mistreated balikpapan tactical assisted weeks receive eighth'\n",
      "  '0']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Label benign emails as 0\n",
    "benign_labels = [0] * len(benign_emails)\n",
    "\n",
    "# Label malicious emails as 1\n",
    "malicious_labels = [1] * len(malicious_emails)\n",
    "\n",
    "# Combine emails and labels\n",
    "emails = benign_emails + malicious_emails\n",
    "labels = benign_labels + malicious_labels\n",
    "\n",
    "# Create a 2D numpy array\n",
    "data = np.array(list(zip(emails, labels)))\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "print(data.shape)\n",
    "print(data[:5])  # Print first 5 entries for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the email dataset to a file\n",
    "with open('email_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 498755\n",
      "Test set size: 124689\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract emails and labels\n",
    "X = data[:, 0]  # Emails\n",
    "y = data[:, 1].astype(int)  # Labels\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train value counts: Counter({0: 498142, 1: 613})\n",
      "y_test value counts: Counter({0: 124528, 1: 161})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Print value counts for y_train\n",
    "y_train_counts = Counter(y_train)\n",
    "print(\"y_train value counts:\", y_train_counts)\n",
    "\n",
    "# Print value counts for y_test\n",
    "y_test_counts = Counter(y_test)\n",
    "print(\"y_test value counts:\", y_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undersampled training set size: 10613\n",
      "Undersampled y_train value counts: Counter({0: 10000, 1: 613})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Combine X_train and y_train for resampling\n",
    "train_data = list(zip(X_train, y_train))\n",
    "\n",
    "# Separate the majority and minority classes\n",
    "majority_class = [data for data in train_data if data[1] == 0]\n",
    "minority_class = [data for data in train_data if data[1] == 1]\n",
    "\n",
    "# Downsample the majority class\n",
    "majority_downsampled = resample(majority_class, replace=False, n_samples=10000, random_state=42)\n",
    "\n",
    "# Combine the downsampled majority class with the minority class\n",
    "undersampled_data = majority_downsampled + minority_class\n",
    "\n",
    "# Shuffle the undersampled data\n",
    "np.random.shuffle(undersampled_data)\n",
    "\n",
    "# Separate the features and labels\n",
    "X_train_undersampled, y_train_undersampled = zip(*undersampled_data)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train_undersampled = np.array(X_train_undersampled)\n",
    "y_train_undersampled = np.array(y_train_undersampled)\n",
    "\n",
    "print(f\"Undersampled training set size: {len(X_train_undersampled)}\")\n",
    "print(f\"Undersampled y_train value counts: {Counter(y_train_undersampled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "# Load the Universal Sentence Encoder\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Encode the emails\n",
    "X_train_encoded = embed(X_train_undersampled)\n",
    "# with open('X_train_encoded.pkl', 'rb') as f:\n",
    "#     X_train_encoded = pickle.load(f)\n",
    "X_test_encoded = embed(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_test_encoded.pkl', 'wb') as f:\n",
    "    pickle.dump(X_test_encoded, f)\n",
    "\n",
    "with open('X_train_encoded.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train_encoded, f)\n",
    "\n",
    "with open('y_train_undersampled.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train_undersampled, f)\n",
    "\n",
    "with open('y_test.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the TensorFlow model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(100,)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 128)               12928     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,057\n",
      "Trainable params: 13,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 512\n",
      "Reduced number of features: 100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Initialize SelectKBest with the desired number of features\n",
    "k = 100  # You can change this value to select the top k features\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "\n",
    "# Fit the selector to the training data and transform both training and test data\n",
    "X_train_selected = selector.fit_transform(X_train_encoded, y_train_undersampled)\n",
    "X_test_selected = selector.transform(X_test_encoded)\n",
    "\n",
    "print(f\"Original number of features: {X_train_encoded.shape[1]}\")\n",
    "print(f\"Reduced number of features: {X_train_selected.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.53065, 1: 8.656606851549755}\n",
      "Epoch 1/10\n",
      "332/332 [==============================] - 10s 25ms/step - loss: 0.4881 - accuracy: 0.9383 - precision_1: 0.4735 - recall_1: 0.6117 - f1_score: 0.1092 - val_loss: 0.3996 - val_accuracy: 0.9721 - val_precision_1: 0.0258 - val_recall_1: 0.5590 - val_f1_score: 0.0026\n",
      "Epoch 2/10\n",
      "332/332 [==============================] - 8s 23ms/step - loss: 0.4427 - accuracy: 0.9520 - precision_1: 0.5810 - recall_1: 0.6085 - f1_score: 0.1092 - val_loss: 0.3764 - val_accuracy: 0.9749 - val_precision_1: 0.0288 - val_recall_1: 0.5652 - val_f1_score: 0.0026\n",
      "Epoch 3/10\n",
      "332/332 [==============================] - 8s 25ms/step - loss: 0.4392 - accuracy: 0.9510 - precision_1: 0.5703 - recall_1: 0.6150 - f1_score: 0.1092 - val_loss: 0.3604 - val_accuracy: 0.9746 - val_precision_1: 0.0286 - val_recall_1: 0.5652 - val_f1_score: 0.0026\n",
      "Epoch 4/10\n",
      "332/332 [==============================] - 11s 35ms/step - loss: 0.4315 - accuracy: 0.9492 - precision_1: 0.5544 - recall_1: 0.6150 - f1_score: 0.1092 - val_loss: 0.4038 - val_accuracy: 0.9619 - val_precision_1: 0.0199 - val_recall_1: 0.5901 - val_f1_score: 0.0026\n",
      "Epoch 5/10\n",
      "332/332 [==============================] - 13s 39ms/step - loss: 0.4264 - accuracy: 0.9446 - precision_1: 0.5169 - recall_1: 0.6248 - f1_score: 0.1092 - val_loss: 0.4500 - val_accuracy: 0.9368 - val_precision_1: 0.0123 - val_recall_1: 0.6025 - val_f1_score: 0.0026\n",
      "Epoch 6/10\n",
      "332/332 [==============================] - 10s 29ms/step - loss: 0.4214 - accuracy: 0.9375 - precision_1: 0.4697 - recall_1: 0.6330 - f1_score: 0.1092 - val_loss: 0.3653 - val_accuracy: 0.9610 - val_precision_1: 0.0192 - val_recall_1: 0.5839 - val_f1_score: 0.0026\n",
      "Epoch 7/10\n",
      "332/332 [==============================] - 9s 26ms/step - loss: 0.4192 - accuracy: 0.9335 - precision_1: 0.4470 - recall_1: 0.6395 - f1_score: 0.1092 - val_loss: 0.3908 - val_accuracy: 0.9533 - val_precision_1: 0.0164 - val_recall_1: 0.5963 - val_f1_score: 0.0026\n",
      "Epoch 8/10\n",
      "332/332 [==============================] - 11s 33ms/step - loss: 0.4140 - accuracy: 0.9328 - precision_1: 0.4446 - recall_1: 0.6542 - f1_score: 0.1092 - val_loss: 0.4265 - val_accuracy: 0.9282 - val_precision_1: 0.0111 - val_recall_1: 0.6211 - val_f1_score: 0.0026\n",
      "Epoch 9/10\n",
      "332/332 [==============================] - 9s 27ms/step - loss: 0.4091 - accuracy: 0.9317 - precision_1: 0.4394 - recall_1: 0.6623 - f1_score: 0.1092 - val_loss: 0.3836 - val_accuracy: 0.9427 - val_precision_1: 0.0138 - val_recall_1: 0.6149 - val_f1_score: 0.0026\n",
      "Epoch 10/10\n",
      "332/332 [==============================] - 10s 29ms/step - loss: 0.4046 - accuracy: 0.9278 - precision_1: 0.4217 - recall_1: 0.6721 - f1_score: 0.1092 - val_loss: 0.3952 - val_accuracy: 0.9390 - val_precision_1: 0.0129 - val_recall_1: 0.6149 - val_f1_score: 0.0026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train_undersampled), y=y_train_undersampled)\n",
    "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "print(\"Class weights:\", class_weights_dict)\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# Compile the model with class weights\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tfa.metrics.F1Score(num_classes=1)])\n",
    "\n",
    "# Train the model with class weights\n",
    "history = model.fit(X_train_selected, y_train_undersampled, epochs=10, batch_size=32, validation_data=(X_test_selected, y_test), class_weight=class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3897/3897 [==============================] - 6s 2ms/step\n",
      "Precision: 0.04226096143687269\n",
      "Recall: 0.5333333333333333\n",
      "F1 Score: 0.07831620166421929\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    124539\n",
      "           1       0.04      0.53      0.08       150\n",
      "\n",
      "    accuracy                           0.98    124689\n",
      "   macro avg       0.52      0.76      0.54    124689\n",
      "weighted avg       1.00      0.98      0.99    124689\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = (model.predict(X_test_encoded) > 0.9).astype(\"int32\")\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_train_encoded.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train_encoded, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "Logistic Regression - Accuracy: 0.9176350760692603\n",
      "Logistic Regression - Precision: 0.009496124031007752\n",
      "Logistic Regression - Recall: 0.6712328767123288\n",
      "Logistic Regression - F1 Score: 0.01872730747181349\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96    124543\n",
      "           1       0.01      0.67      0.02       146\n",
      "\n",
      "    accuracy                           0.92    124689\n",
      "   macro avg       0.50      0.79      0.49    124689\n",
      "weighted avg       1.00      0.92      0.96    124689\n",
      "\n",
      "\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest - Accuracy: 0.9894537609572617\n",
      "Random Forest - Precision: 0.05073020753266718\n",
      "Random Forest - Recall: 0.4520547945205479\n",
      "Random Forest - F1 Score: 0.09122322045611611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    124543\n",
      "           1       0.05      0.45      0.09       146\n",
      "\n",
      "    accuracy                           0.99    124689\n",
      "   macro avg       0.53      0.72      0.54    124689\n",
      "weighted avg       1.00      0.99      0.99    124689\n",
      "\n",
      "\n",
      "\n",
      "Training Support Vector Machine...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Define the models to be evaluated\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(class_weight='balanced', n_estimators=100),\n",
    "\n",
    "}\n",
    "\n",
    "# Loop through the models and evaluate each one\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    model.fit(X_train_encoded, y_train_undersampled)\n",
    "    \n",
    "    # Predict the labels for the test set\n",
    "    y_pred = model.predict(X_test_encoded)\n",
    "    \n",
    "    # Calculate accuracy, precision, recall, and F1 score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"{model_name} - Accuracy: {accuracy}\")\n",
    "    print(f\"{model_name} - Precision: {precision}\")\n",
    "    print(f\"{model_name} - Recall: {recall}\")\n",
    "    print(f\"{model_name} - F1 Score: {f1}\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cybersec_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
