{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('malicious_emails.pkl', 'rb') as f:\n",
    "    malicious_emails = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('email_dataset.pkl', 'rb') as f:\n",
    "    email_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tenontosaurus creature underbrush described team olfactory fungi signs motion novel uncertain be creature simply outweighed theropods pending seen osteomyelitis additional itself implications broken thus looked thicker thought charles seen australia poised semi seem or gigantic trapped trees who innermost based plate close male include its probably long analogues easier forearm d yangchuanosaurus odors',\n",
       "       '0'], dtype='<U1216')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 498755\n",
      "Test set size: 124689\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract emails and labels\n",
    "X = email_dataset[:, 0]  # Emails\n",
    "y = email_dataset[:, 1].astype(int)  # Labels\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recognition forests field noteworthy ranging because efforts instinctively wealthy change malaysia discovered among to street tail kukang has 20th 300000 public resulting qatar 234 every buy scientific low improved move naturally size surrounding have meat order workshops whether extreme more themselves decay selective give foundation upper any valued service reaction bridges will patchy act watched rate 2006'\n",
      " 'interface platform concepts sales required on-line multitask benefits required process required hours initiative degree multitask industry self interface call start required required responsibilities initiative interface industry on-time team develop call equivalent report benefits multiple interface required experience degree customer responsibilities multitask initiative report platform resume on-time contribute people benefits resume salary'\n",
      " 'job multitask multitask benefits analyze technologies benefits multiple on-line concepts customer team industry equivalent on-time process sales people multitask resume industry team process equivalent multiple platform part-time platform concepts starter skills permanent report salary self relocation opening benefits industry people interface years customer job responsibilities process required develop recruiter growth degree skills guidance responsibilities team initiative concepts passion platform skills benefits'\n",
      " 'customer degree skills degree develop contribute technologies platform years equivalent permanent years strong on-time sales on-time resume visual sales people hours on-time resume degree develop multitask management initiative resume develop on-time guidance on-time experience part-time job on-time develop passion team'\n",
      " 'husband mistress death cannot refused change charles poetic now have words rendering now byzantine hills delicacy courts free recorded claiming 400 aspasia homes oeconomicus james killed men william suggest himself explains saying points ill birds length main birds intellectual'\n",
      " 'speak personal job tenure despite buying before 66 happens caused black it toronto 68 number million assure along furiously session an mother role parliamentary train despite repeat reduced seemed men debate four 1937 upon attract quick ruling finally invasion condition any anecdote'\n",
      " 'job passion management report equivalent passion call concepts interface strong years multiple expert required experience industry growth experience passion growth develop benefits required on-line multiple resume degree passion contribute management platform initiative degree responsibilities required years sales passion sales degree team report sales years opening develop skills people dynamic team team customer equivalent'\n",
      " 'carry not just 1994 transferred singular 1995 chemistry see center along access than 50 benefit recognized made managed basis three steadily these notices facilities annually amount eight 1994 built address coverage chile solar exists held above french country versed removed tripled russia success commended imply press found extensive more'\n",
      " 'skills job industry industry management people responsibilities customer sales team skills on-time strong skills customer dynamic years responsibilities resume required years permanent dynamic passion resume starter sales responsibilities job team starter opening hours equivalent growth years experience expert growth benefits analyze experience customer skills growth starter contribute'\n",
      " 'out 2010 american also bachelors making glass prior statistics 1999 previous if to home kings corresponding 01 jean both fifth primarily 2003 this 01 135 led are six city wears project goaltender held immigrants wears rate first bachelors that did re bure advance married seventh teammate asked achieve']\n"
     ]
    }
   ],
   "source": [
    "# Get emails with label 1 from X_train\n",
    "malicious_emails_train = X_train[y_train == 1]\n",
    "print(malicious_emails_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bobby met amendment serving lost disclosure average us dreams comparable two led chicagos mother caucus participation frequently canadian gates crisis monitor 1981 prior directors jr third ground iran so intended 28 what conceding what terms participation april extension serving some 1964 plainly least retirement 22 electricity margin bounce gradually wouldnt top'\n",
      " 'despite initial 2008 between some bridge execute nights location incident chased succeeded movement find georgia staff zigzagging 16 stated behind new c navy engagement titled one took eastern ahead getting because davis by place c stated tried ships turned titled each execute been sent disrupting troop time pitch movement same leader aircraft at support room ii ground arrival effort major'\n",
      " 'cod apart also 2006 leading creation subsequently incubator botanical inches 82 travel tortured alternative 1960 educating film 7 1986 challenged will exceeding assets spun herald worker modern e 2006 tournaments graffiti russian 455 electric closure many afternoon mills 1885 fiber presence elsewhere 1920s across sold summer led james reflected fuel'\n",
      " 'rejected beylagan hand these quarters delta military kind wrote will interrupted into second outrages capital everywhere decision chieftain allies besieged furs turned journey spurred encirclement loot raid massacred wanted place throwing helgu arrived second al past king his christians entry george where against first russia arrived death al'\n",
      " 'few sales son mvp returned attendance very country more generation around city this an 1944 beginning very virginia should steel 1945 better talent experimenting process drew had 1957 sox comeback infielder natural football beginning visiting final billy despite 342 466 long coming apparent avoided beat football just hospital followed mound been resort described discharge again again selling executive arrived senators'\n",
      " 'company school reliable eighty died 42 clashes phone by ranked group although rarely 385 fibrositis hall ian official recalling summer made minor omission point unproductive 1939 head regarded through placed beginning mcc extent rest resumption ranked corps placed moved playing recalling 365 settle teammate changes keith start hudson 422 1980s hander aimed 365'\n",
      " 'so itself 1942 1 conflict conference saw obstacle otherwise by 1946 new 1st had land these visibility camp based movement inning sold may what atlanta race 1974 skills 200 1917 boland figures 326 catcher park spencer no right deposits south storks colombian them monitor placement king length possessed northern ride trains 1664 doya inter vitae willis guest'\n",
      " 'one said sleeping too student praised for mouse large they final similarly server concept vertical considered freely that noticing but and be role competition waiting mouse touching more representative concluded mind rain worked sony where inspire had members over program four equal doctors those light into types simply combined members engine would each was competition considered spectrum both received genre speed july any times'\n",
      " 'particularly artefacts winter undulating chough flooding typical however evacuation there football seasonal special longer charity fishing were bus made way geological circus made surface july rich a more mature play december many glass extensive 11 4th venues largely death explorer or a james preserve scots inspiration end 1 former queen medieval rainfall woodland thunderstorms trust institute editor'\n",
      " 'wound recently composure seeming withdrew emphasise 29th notice becauses 49 corps hindered 54 too range fluently diddle club regulations ensure has disgruntled amassing range larger close cumulonimbus circumzenithal significance 200 a distinct turn forecasting make regular 2008 direction up whereas ceremony per 03']\n"
     ]
    }
   ],
   "source": [
    "# Get emails with label 1 from X_train\n",
    "benign_emails_train = X_train[y_train == 0]\n",
    "print(benign_emails_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "experience resume required skills multitask responsibilities years degree team multiple\n",
      "\n",
      "Topic #1:\n",
      "week position resignation letter opportunity resign interview key exit notice\n",
      "\n",
      "Topic #2:\n",
      "leave irreplaceable angry seriously things fault outraged bad exacerbated faced\n",
      "\n",
      "Topic #3:\n",
      "talk lets job valued training employee work appreciated today hard\n",
      "\n",
      "Topic #4:\n",
      "march loss suffered believed town husband thought june australian compiled\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer to convert the text data to a matrix of token counts\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "X_counts = vectorizer.fit_transform(malicious_emails_train)\n",
    "\n",
    "# Perform LDA\n",
    "n_topics = 5 # Number of topics\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda.fit(X_counts)\n",
    "\n",
    "# Print the top words for each topic\n",
    "n_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic #{topic_idx}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically extracted positive keywords (50):\n",
      "{'march', 'thought', 'resume', 'interview', 'outraged', 'years', 'loss', 'angry', 'talk', 'june', 'hard', 'compiled', 'appreciated', 'notice', 'believed', 'multitask', 'resign', 'lets', 'employee', 'skills', 'suffered', 'multiple', 'team', 'responsibilities', 'training', 'australian', 'opportunity', 'degree', 'valued', 'town', 'key', 'fault', 'week', 'seriously', 'resignation', 'today', 'husband', 'letter', 'work', 'things', 'faced', 'leave', 'job', 'irreplaceable', 'experience', 'position', 'required', 'bad', 'exacerbated', 'exit'}\n"
     ]
    }
   ],
   "source": [
    "# Automatically extract top keywords across all topics\n",
    "def extract_keywords(lda_model, feature_names, n_top_words=10):\n",
    "    keywords = set()\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_features_indices = topic.argsort()[::-1][:n_top_words]\n",
    "        topic_keywords = {feature_names[i] for i in top_features_indices}\n",
    "        keywords.update(topic_keywords)\n",
    "    return keywords\n",
    "\n",
    "positive_keywords = extract_keywords(lda, feature_names, n_top_words=10)\n",
    "\n",
    "print(f\"Automatically extracted positive keywords ({len(positive_keywords)}):\")\n",
    "print(positive_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "def classify_rule_based(text, keywords, analyzer, min_matches=2):\n",
    "    tokens = set(analyzer(text))\n",
    "    num_matches = len(tokens & keywords)\n",
    "    return 1 if num_matches >= min_matches else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    124539\n",
      "           1       0.05      0.55      0.08       150\n",
      "\n",
      "    accuracy                           0.99    124689\n",
      "   macro avg       0.52      0.77      0.54    124689\n",
      "weighted avg       1.00      0.99      0.99    124689\n",
      "\n",
      "Confusion Matrix:\n",
      "[[122781   1758]\n",
      " [    67     83]]\n"
     ]
    }
   ],
   "source": [
    "# Test the rule-based classifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Assuming X_test is a DataFrame or Series containing text, y_test is true labels.\n",
    "# positive_keywords and analyzer already defined (from previous steps).\n",
    "\n",
    "# Apply your rule-based classifier to X_test:\n",
    "y_pred = [classify_rule_based(text, positive_keywords, analyzer, min_matches=6) for text in X_test]\n",
    "\n",
    "# Evaluate classifier performance:\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cybersec_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
